{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob,os,re\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from unicodedata import category, normalize\n",
    "from tf.fabric import Fabric\n",
    "from tf.timestamp import Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode tricks\n",
    "\n",
    "We show how the unicode library works, and use it to define a function that splits punctuation from words, and\n",
    "a function that converts greek accented characters to plain uppercase characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lu', 'Ll', 'Ll', 'Ll', 'Ll', 'Ll', 'Po', 'Zs']\n",
      "['Α', 'υ', '̓', 'τ', 'ο', 'υ', '́', 'ς', '·', ' ']\n",
      "['Lu', 'Ll', 'Mn', 'Ll', 'Ll', 'Ll', 'Mn', 'Ll', 'Po', 'Zs']\n"
     ]
    }
   ],
   "source": [
    "word = 'Αὐτούς· '\n",
    "print([category(x) for x in word])\n",
    "print([x for x in normalize('NFD', word)])\n",
    "print([category(x) for x in normalize('NFD', word)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letter = {'L'}\n",
    "dia = {'M'}\n",
    "NFD = 'NFD'\n",
    "\n",
    "def splitPunc(w):\n",
    "    afterWord = len(w)\n",
    "    for i in range(len(w) - 1, -1, -1):\n",
    "        if category(w[i])[0] not in letter:\n",
    "            afterWord = i\n",
    "        else:\n",
    "            break\n",
    "    return (w[0:afterWord], w[afterWord:]+' ')\n",
    "\n",
    "def plainCaps(w):\n",
    "    return ''.join(x.upper() for x in normalize(NFD, w) if category(x)[0] not in dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αὐτούς\n",
      "ΑΥΤΟΥΣ\n",
      "·  \n"
     ]
    }
   ],
   "source": [
    "(wordBare, punct) = splitPunc(word)\n",
    "wordPlain = plainCaps(wordBare)\n",
    "print(wordBare)\n",
    "print(wordPlain)\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.6\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otype\" not found in\n",
      "\n",
      "  0.01s Grid feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.01s Grid feature \"otext\" not found. Working without Text-API\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm = Timestamp()\n",
    "TF = Fabric('~/github/text-fabric-data/greek/sblgnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR_PATH = '~/github/greek-new-testament/syntax-trees/sblgnt/xml/'.replace(\n",
    "    '~', os.path.expanduser('~').replace('\\\\', '/'),\n",
    ")\n",
    "\n",
    "otypeFromCat = dict(\n",
    "    np='phrase',\n",
    "    CL='clause',\n",
    "    vp='phrase',\n",
    "    noun='word',\n",
    "    verb='word',\n",
    "    V='clause_atom',\n",
    "    det='word',\n",
    "    ADV='clause_atom',\n",
    "    S='clause_atom',\n",
    "    conj='conjunction',\n",
    "    pron='word',\n",
    "    pp='phrase',\n",
    "    prep='word',\n",
    "    O='clause_atom',\n",
    "    adjp='phrase',\n",
    "    adj='word',\n",
    "    advp='phrase',\n",
    "    adv='word',\n",
    "    P='clause_atom',\n",
    "    IO='clause_atom',\n",
    "    VC='clause_atom',\n",
    "    ptcl='word',\n",
    "    nump='phrase',\n",
    "    num='word',\n",
    "    intj='word',\n",
    "    O2='clause_atom',\n",
    ")\n",
    "\n",
    "numberFeatures = set('''\n",
    "    chapter\n",
    "    End\n",
    "    Head\n",
    "    Start\n",
    "    verse\n",
    "    chapter\n",
    "    booknum\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langHead = re.compile('\\[([^\\]]+)\\]')\n",
    "bookNames = collections.defaultdict(lambda: [])\n",
    "bookLangs = {}\n",
    "with open('blang.txt') as fb:\n",
    "    specsDone = False\n",
    "    for line in fb:\n",
    "        line = line.rstrip('\\n')\n",
    "        if line == '---':\n",
    "            specsDone = True\n",
    "            curLang = None\n",
    "            continue\n",
    "        if specsDone:\n",
    "            match = langHead.findall(line)\n",
    "            if match:\n",
    "                curLang = match[0]\n",
    "            else:\n",
    "                bookNames[curLang].append(line)\n",
    "        else:\n",
    "            (acro, langEn, lang) = line.split('=', 2)\n",
    "            bookLangs[acro] = (langEn, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.tfFromXml = {}\n",
    "        self.xmlFromTf = {}\n",
    "        self.nodeNum = 1\n",
    "        self.maxSlot = 0\n",
    "        self.maxNode = 0\n",
    "        self.paths = {}\n",
    "        self.nodeFeatures = collections.defaultdict(dict)\n",
    "        self.edgeFeatures = collections.defaultdict(dict)\n",
    "\n",
    "ignoreAtts = {'Cat'}\n",
    "\n",
    "books = {}\n",
    "chapters = {}\n",
    "verses = {}\n",
    "\n",
    "def walkNode(node, path):\n",
    "    if node.tag == 'Node':\n",
    "        n = data.nodeNum\n",
    "        xmlId = node.attrib['nodeId']\n",
    "        data.tfFromXml[xmlId] = n\n",
    "        data.xmlFromTf[n] = xmlId\n",
    "        cat = node.attrib['Cat']\n",
    "        if len(node) == 0:\n",
    "            data.nodeFeatures['otype'][n] = 'word'\n",
    "            data.nodeFeatures['psp'][n] = cat\n",
    "            for parent in path:\n",
    "                data.edgeFeatures['oslots'].setdefault(parent, []).append(n)\n",
    "            book = int(xmlId[0:2])\n",
    "            chapter = int(xmlId[3:5])\n",
    "            verse = int(xmlId[5:8])\n",
    "            books.setdefault(book, []).append(n)\n",
    "            chapters.setdefault((book, chapter), []).append(n)\n",
    "            verses.setdefault((book, chapter, verse), []).append(n)\n",
    "        else:\n",
    "            if len(path) == 0:\n",
    "                otype = 'sentence'\n",
    "            else:\n",
    "                otype = otypeFromCat[cat]\n",
    "            if otype == 'word':\n",
    "                otype = 'wordx'\n",
    "            data.nodeFeatures['otype'][n] = otype\n",
    "            if otype not in {'clause', 'sentence'}:\n",
    "                data.nodeFeatures['function'][n] = cat\n",
    "        for (att, val) in node.attrib.items():\n",
    "            data.nodeFeatures[att][n] = val\n",
    "        \n",
    "        if len(path) != 0:\n",
    "            parent = path[-1]\n",
    "            data.edgeFeatures['child'].setdefault(parent, []).append(n)\n",
    "        data.paths[n] = path\n",
    "        newPath = path+(n,)\n",
    "\n",
    "        data.nodeNum += 1\n",
    "    else:\n",
    "        newPath = path\n",
    "    for child in node:\n",
    "        walkNode(child, newPath)\n",
    "\n",
    "def getNode(root):\n",
    "    walkNode(root, ())\n",
    "    data.maxNode = data.nodeNum - 1\n",
    "        \n",
    "def reorder():\n",
    "    otypeValues = set(data.nodeFeatures['otype'].values())\n",
    "    otypeRank = dict(((val, ' ' if val == 'word' else val) for val in otypeValues))\n",
    "    newIds = sorted(range(1, data.maxNode + 1), key=lambda n: (otypeRank[data.nodeFeatures['otype'][n]], n))\n",
    "    mapping = dict(((v, i+1) for (i, v) in enumerate(newIds)))\n",
    "    \n",
    "    orderedFeatures = {}\n",
    "    for (name, dat) in data.nodeFeatures.items():\n",
    "        orderedFeatures[name] = dict(((mapping[n], v) for (n, v) in dat.items()))\n",
    "    data.nodeFeatures = orderedFeatures\n",
    "\n",
    "    orderedFeatures = {}\n",
    "    for (name, dat) in data.edgeFeatures.items():\n",
    "        orderedFeatures[name] = dict(((mapping[n], [mapping[m] for m in v]) for (n, v) in dat.items()))\n",
    "    data.edgeFeatures = orderedFeatures\n",
    "\n",
    "def sections():\n",
    "    n = data.maxNode\n",
    "    data.nodeFeatures['book'] = {}\n",
    "    for (i, book) in enumerate(books):\n",
    "        n += 1\n",
    "        data.nodeFeatures['otype'][n] = 'book'\n",
    "        data.nodeFeatures['book'][n] = bookNamesOrig[book]\n",
    "        for ll in bookNames:\n",
    "            data.nodeFeatures['book@{}'.format(ll)][n] = bookNames[ll][i]\n",
    "        data.nodeFeatures['booknum'][n] = str(book - HEBREW_BOOKS)\n",
    "        data.edgeFeatures['oslots'][n] = books[book]\n",
    "    for (book, chapter) in chapters:\n",
    "        n += 1\n",
    "        data.nodeFeatures['otype'][n] = 'chapter'\n",
    "        data.nodeFeatures['chapter'][n] = str(chapter)\n",
    "        data.edgeFeatures['oslots'][n] = chapters[(book, chapter)]\n",
    "    for (book, chapter, verse) in verses:\n",
    "        n += 1\n",
    "        data.nodeFeatures['otype'][n] = 'verse'\n",
    "        data.nodeFeatures['verse'][n] = str(verse)\n",
    "        data.edgeFeatures['oslots'][n] = verses[(book, chapter, verse)]\n",
    "\n",
    "\n",
    "    data.maxNode = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Scanning XML sources of all books\n",
      "   |     1.02s matthew\n",
      "   |     0.63s mark\n",
      "   |     1.14s luke\n",
      "   |     1.04s john\n",
      "   |     1.11s acts\n",
      "   |     0.36s romans\n",
      "   |     0.49s 1corinthians\n",
      "   |     0.20s 2corinthians\n",
      "   |     0.32s galatians\n",
      "   |     0.12s ephesians\n",
      "   |     0.15s philippians\n",
      "   |     0.22s colossians\n",
      "   |     0.14s 1thessalonians\n",
      "   |     0.16s 2thessalonians\n",
      "   |     0.17s 1timothy\n",
      "   |     0.60s 2timothy\n",
      "   |     0.05s titus\n",
      "   |     0.04s philemon\n",
      "   |     0.26s hebrews\n",
      "   |     0.10s james\n",
      "   |     0.09s 1peter\n",
      "   |     0.07s 2peter\n",
      "   |     0.37s 1john\n",
      "   |     0.05s 2john\n",
      "   |     0.04s 3john\n",
      "   |     0.08s jude\n",
      "   |     0.98s revelation\n",
      "    10s Processing data ...\n",
      "    13s Done\n"
     ]
    }
   ],
   "source": [
    "HEBREW_BOOKS = 39\n",
    "\n",
    "bookNamesOrig = {}\n",
    "\n",
    "nodes_with_ID = collections.OrderedDict()\n",
    "nodes_without_ID = []\n",
    "\n",
    "filenamepat = re.compile('^([0-9]{2})-(.*)$')\n",
    "\n",
    "data = Data()\n",
    "\n",
    "tm.indent(reset=True)\n",
    "tm.info('Scanning XML sources of all books')\n",
    "for xmlfile in glob.glob(DIR_PATH+'*.xml'):\n",
    "    tm.indent(level=1, reset=True)\n",
    "    (dirName, baseName) = os.path.split(xmlfile)\n",
    "    (fileName, extension) = os.path.splitext(baseName)\n",
    "    match = filenamepat.findall(fileName)\n",
    "    if len(match) == 0: continue\n",
    "    (numeral, bookName) = match[0]\n",
    "    numeral = int(numeral) + HEBREW_BOOKS\n",
    "    bookNamesOrig[numeral] = bookName\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    getNode(root)\n",
    "    tm.info(bookName)\n",
    "tm.indent(level=0)\n",
    "tm.info('Processing data ...')\n",
    "sections()\n",
    "reorder()\n",
    "tm.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metaData = {\n",
    "    '': dict(\n",
    "        createdBy='Cody Kingham and Dirk Roorda',\n",
    "    ),\n",
    "    'otext': {\n",
    "        'sectionFeatures': 'book,chapter,verse',\n",
    "        'sectionTypes': 'book,chapter,verse',\n",
    "        'fmt:text-orig-full': '{g_word}{trailer}',\n",
    "        'fmt:text-orig-plain': '{g_plain}{trailer}',\n",
    "        'fmt:lex-orig-full': '{UnicodeLemma} ',\n",
    "    },\n",
    "    'book@en': {\n",
    "        'valueType': 'str',\n",
    "        'language': 'English',\n",
    "        'languageCode': 'en',\n",
    "        'languageEnglish': 'english',\n",
    "    },\n",
    "}\n",
    "for ll in bookNames:\n",
    "    metaData['book@{}'.format(ll)] = {\n",
    "        'valueType': 'str',\n",
    "        'language': bookLangs[ll][1],\n",
    "        'languageCode': ll,\n",
    "        'languageEnglish': bookLangs[ll][0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features: text representation and statistical\n",
    "We add some features for text representation.\n",
    "The feature `Unicode` contains punctuation. We split that off, and create new features\n",
    "`g_word`, `g_plain` and `trailer`, where `g_` stands for *graphical*.\n",
    "We add some statistical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    20s Computing statistics\n",
      "    20s Done\n",
      "    20s Adding statistics as features\n",
      "    22s Done\n"
     ]
    }
   ],
   "source": [
    "tm.info('Computing statistics')\n",
    "wstats = {\n",
    "    'freq': {\n",
    "        'lex': collections.Counter(),\n",
    "        'occ': collections.Counter(),\n",
    "    },\n",
    "    'rank': {\n",
    "        'lex': {},\n",
    "        'occ': {},\n",
    "    },\n",
    "}\n",
    "g_word = {}\n",
    "g_plain = {}\n",
    "trailer = {}\n",
    "\n",
    "nodeFeatures = data.nodeFeatures\n",
    "\n",
    "words = [n[0] for n in nodeFeatures['otype'].items() if n[1] == 'word']\n",
    "\n",
    "for w in words:\n",
    "    occ = nodeFeatures['Unicode'][w]\n",
    "    lex = nodeFeatures['UnicodeLemma'][w]\n",
    "    wstats['freq']['lex'][lex] += 1\n",
    "    wstats['freq']['occ'][occ] += 1\n",
    "for tp in ['lex', 'occ']:\n",
    "    rank = -1\n",
    "    prev_n = -1\n",
    "    amount = 1\n",
    "    for (x, n) in sorted(wstats['freq'][tp].items(), key=lambda y: (-y[1], y[0])):\n",
    "        if n == prev_n:\n",
    "            amount += 1\n",
    "        else:\n",
    "            rank += amount\n",
    "            amount = 1\n",
    "        prev_n = n\n",
    "        wstats['rank'][tp][x] = rank\n",
    "tm.info('Done')\n",
    "\n",
    "tm.info('Adding statistics as features')\n",
    "occFeatures = {}\n",
    "for tp in ['occ', 'lex']:\n",
    "    for ft in ('freq_{}'.format(tp), 'rank_{}'.format(tp)):\n",
    "        occFeatures[ft] = {}\n",
    "        metaData.setdefault(ft, {})['valueType'] = 'int'\n",
    "for ft in ['g_word', 'g_plain', 'trailer']:\n",
    "    metaData.setdefault(ft, {})['valueType'] = 'str'\n",
    "\n",
    "for w in words:\n",
    "    occ = nodeFeatures['Unicode'][w]\n",
    "    lex = nodeFeatures['UnicodeLemma'][w]\n",
    "    (gw, tr) = splitPunc(occ)\n",
    "    gp = plainCaps(gw)\n",
    "    g_word[w] = gw\n",
    "    g_plain[w] = gp\n",
    "    trailer[w] = tr\n",
    "\n",
    "    for tp in ['occ', 'lex']:\n",
    "        ref = occ if tp == 'occ' else lex\n",
    "        for kn in ['freq', 'rank']:\n",
    "            ft = '{}_{}'.format(kn, tp)\n",
    "            occFeatures[ft][w] = str(wstats[kn][tp][ref])\n",
    "\n",
    "nodeFeatures.update(occFeatures)\n",
    "nodeFeatures.update(dict(g_word=g_word, g_plain=g_plain, trailer=trailer))\n",
    "\n",
    "tm.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otypeValues = set(data.nodeFeatures['otype'].values())\n",
    "otypeRank = dict(((val, ' ' if val == 'word' else val) for val in otypeValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 60 node and 2 edge and 1 config features to /Users/dirk/github/text-fabric-data/greek/sblgnt:\n",
      "   |     0.19s T Case                 to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.74s T Cat                  to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.02s T ClType               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.00s T Degree               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.75s T End                  to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.15s T Gender               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.11s T HasDet               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.50s T Head                 to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.09s T Mood                 to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.38s T Number               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.07s T Person               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.51s T Rule                 to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.85s T Start                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.10s T Tense                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.11s T Type                 to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.27s T Unicode              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.28s T UnicodeLemma         to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.08s T Voice                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book                 to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@am              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@ar              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@bn              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@da              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@de              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@el              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@en              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@es              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@fa              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@fr              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@he              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@hi              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@id              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@ja              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@ko              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.02s T book@la              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@nl              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@pa              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@pt              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@ru              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@sw              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@syc             to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@tr              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@ur              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@yo              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T book@zh              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T booknum              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s T chapter              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.53s T freq_lex             to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.28s T freq_occ             to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.51s T function             to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.44s T g_plain              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.33s T g_word               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.40s T morphId              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.85s T nodeId               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.37s T otype                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.35s T psp                  to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.25s T rank_lex             to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.30s T rank_occ             to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.34s T trailer              to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.04s T verse                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     1.09s T child                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     1.23s T oslots               to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "   |     0.01s M otext                to /Users/dirk/github/text-fabric-data/greek/sblgnt\n",
      "    13s Exported 60 node features and 2 edge features and 1 config features to /Users/dirk/github/text-fabric-data/greek/sblgnt\n"
     ]
    }
   ],
   "source": [
    "for nf in data.nodeFeatures:\n",
    "    metaData.setdefault(nf, {})['valueType'] = 'int' if nf in numberFeatures else 'str'\n",
    "for ef in data.edgeFeatures:\n",
    "    metaData.setdefault(ef, {})['valueType'] = 'int' if ef in numberFeatures else 'str'\n",
    "\n",
    "TF.save(nodeFeatures=data.nodeFeatures, edgeFeatures=data.edgeFeatures, metaData=metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
