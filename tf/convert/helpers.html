<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.1" />
<title>tf.convert.helpers API documentation</title>
<meta name="description" content="" />
<!-- integrity SRI from https://cdnjs.com/libraries/10up-sanitize.css/11.0.1 -->
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css"
integrity="sha512-kcbluZFacWN57NgWZ4aH6eUMBEaTyErFhIFD3y5qYZbKuuyImH0K/AKsBbfXlivh2z5C+3IDTIhI11YmKomzmA=="
crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css"
integrity="sha512-uVeAgzAmieLUTGba0qr9vXQgVD7fko2kcbYIKIraXUIDg9iJLxveTFUrg3DJhqn3cAf3HFDbgmhq0eGko5wEAA=="
crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tf.convert.helpers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/2550bbf9865ccdc92c0924ffbc71e2c2377f77e6/tf/convert/helpers.py#L1-L838" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import re
from textwrap import dedent

from ..core.helpers import console
from ..core.generic import AttrDict


PRE = &#34;pre&#34;
ZWSP = &#34;\u200b&#34;  # zero-width space

NODE = &#34;node&#34;
FOLDER = &#34;folder&#34;
FILE = &#34;file&#34;
PAGE = &#34;page&#34;
LINE = &#34;line&#34;
LN = &#34;ln&#34;
REGION = &#34;region&#34;
DOC = &#34;doc&#34;
CHAPTER = &#34;chapter&#34;
CHUNK = &#34;chunk&#34;

XNEST = &#34;xnest&#34;
TNEST = &#34;tnest&#34;
TSIB = &#34;tsiblings&#34;
SLOT = &#34;slot&#34;
WORD = &#34;word&#34;
CHAR = &#34;char&#34;
TOKEN = &#34;token&#34;
T = &#34;t&#34;


LINE_MODELS = dict(
    I=dict(),
    II=dict(
        element=(str, &#34;p&#34;),
        nodeType=(str, LN),
    ),
)


LINE_MODEL_DEFAULT = &#34;I&#34;

PAGE_MODELS = dict(
    I=dict(),
    II=dict(
        keepPb=(bool, False),
        element=(str, &#34;div&#34;),
        attributes=(dict, {}),
        pbAtTop=(bool, True),
        nodeType=(str, PAGE),
    ),
)


PAGE_MODEL_DEFAULT = &#34;I&#34;

SECTION_MODELS = dict(
    I=dict(
        levels=(list, [FOLDER, FILE, CHUNK]),
        drillDownDivs=(bool, True),
        backMatter=(str, &#34;backmatter&#34;),
    ),
    II=dict(
        levels=(list, [CHAPTER, CHUNK]),
        element=(str, &#34;head&#34;),
        attributes=(dict, {}),
    ),
    III=dict(
        levels=(list, [FILE, CHAPTER, CHUNK]),
        element=(str, &#34;head&#34;),
        attributes=(dict, {}),
    ),
)
&#34;&#34;&#34;Models for sections.

A section is a part of the corpus that is defined by a set of files,
or by elements within a single TEI source file.

A model
&#34;&#34;&#34;


SECTION_MODEL_DEFAULT = &#34;I&#34;
&#34;&#34;&#34;Default model for sections.
&#34;&#34;&#34;

CM_LIT = &#34;literal&#34;
&#34;&#34;&#34;The value is taken literally from a TEI attribute.

Code `tei`, since there is a 1-1 correspondence with the TEI source.
&#34;&#34;&#34;

CM_LITP = &#34;literal-processed&#34;
&#34;&#34;&#34;The value results from straightforward processing of material in the TEI.

Code `tei`, since there is a direct correspondence with the TEI source.

*Straightforward* means: by taking into account the semantics of XML.

Examples:

*   Generated white-space based on whether elements are pure or mixed;
*   Edges between parent and child elements, or sibling elements.
&#34;&#34;&#34;

CM_LITC = &#34;literal-composed&#34;
&#34;&#34;&#34;The value is results from more intricate processing of material in the TEI.

*More intricate means*: we derive data that goes beyond pure XML syntax.

Examples:

*   The values of the `rend` attributes are translated into `rend_`*value* features;
*   Adding features `is_meta` (being inside the TEI-header) and `is_note`
    (being inside a note);
*   The feature that gives the content of a (character) slot;
*   Decomposing strings into words material and after-word material.

Code `tf`, since this is for the benefit of the resulting TF dataset.
&#34;&#34;&#34;

CM_PROV = &#34;provided&#34;
&#34;&#34;&#34;The value is added by the conversion to TF w.r.t. the material in the TEI.

Examples:

*   Slots in empty elements, in order to anchor the element to the text sequence;
*   Section levels, based on the folder and file that the TEI source is in;
*   A section level within the TEI, defined from several elements and the way they
    are nested;

Code `tf`, since this is for the benefit of the resulting TF dataset.
&#34;&#34;&#34;

CM_NLP = &#34;nlp-generated&#34;
&#34;&#34;&#34;The value is added by an NLP pipeline w.r.t. the material in the TEI.

Code `nlp`, since this comes from third party software.

Examples:

*   The feature `nsent` which gives the sentence number in the corpus.
    Sentences are not encoded in the TEI, but detected by an NLP program such as Spacy.
&#34;&#34;&#34;

CONVERSION_METHODS = {
    CM_LIT: &#34;tei&#34;,
    CM_LITP: &#34;tei&#34;,
    CM_LITC: &#34;tf&#34;,
    CM_PROV: &#34;tf&#34;,
    CM_NLP: &#34;nlp&#34;,
}
&#34;&#34;&#34;Information about the conversion.

When we produce TF features, we specify a bit of information in the feature
metadata as how we arrived at the specific value.

That information ends up in two keys:

*   `conversionMethod`: with values any of:
    *   `CM_LIT`
    *   `CM_LITP`
    *   `CM_LITC`
    *   `CM_PROV`
    *   `CM_NLP`
*   `conversionCode`: the value is derived from `conversionMethod` by looking it
    up in this table. These values can be used to qualify the name of the attribute
    for further processing.

    For example, if you have a feature `n` that originates literally from the TEI,
    you could pass it on as `tei:n`.

    But if you have a feature `chapter` that is provided by the conversion,
    you could pass it on as `tf:chapter`.

    This passing on is a matter of other software, that takes the generated TF as
    input and processes it further, e.g. as annotations.

!!! note &#34;More methods and codes&#34;

The TEI conversion is customizable by providing your own methods to several hooks
in the program. These hooks may generate extra features, which you can give metadata
in the `tei.yaml` file next to the `tei.py` file where you define the custom functions.

It is advised to state appropriate values for the `conversionMethod` and
`conversionCode` fields of these features.

Examples:

*   A feature `country` is derived from specific elements in the TEI Header, and
    defined for nodes of type `letter`.
    This happens in order to support the software of Team Text that shows the
    text on a webpage.

    In such a case you could define

    *   `conversionMethod=&#34;derived&#34;
    *   `conversionCode=&#34;tt&#34;
&#34;&#34;&#34;


TOKEN_RE = re.compile(r&#34;&#34;&#34;\w+|\W&#34;&#34;&#34;)
NUMBER_RE = re.compile(
    r&#34;&#34;&#34;
    ^
    [0-9]+
    (?:
        [.,]
        [0-9]+
    )*
    $
&#34;&#34;&#34;,
    re.X,
)

W_BEFORE = re.compile(r&#34;&#34;&#34;^\s+&#34;&#34;&#34;)
W_AFTER = re.compile(r&#34;&#34;&#34;\s+$&#34;&#34;&#34;)


def getWhites(text):
    match = W_BEFORE.match(text)
    if match:
        before = match.group(0)
        rest = text[len(before) :]
    else:
        before = &#34;&#34;
        rest = text
    match = W_AFTER.search(rest)
    if match:
        after = match.group(0)
        material = rest[0 : -len(after)]
    else:
        after = &#34;&#34;
        material = rest
    return (&#34; &#34; if before else &#34;&#34;, material, &#34; &#34; if after else &#34;&#34;)


def tokenize(line):
    tokens = []

    for word in line.split():
        ts = (
            [[word, &#34;&#34;]]
            if NUMBER_RE.match(word)
            else [[t, &#34;&#34;] for t in TOKEN_RE.findall(word)]
        )
        if len(ts):
            ts[-1][-1] = &#34; &#34;
        tokens.extend(ts)

    if len(tokens):
        tokens[-1][-1] = &#34;&#34;
    return tuple(tokens)


def repTokens(tokens):
    text = []
    for t, space in tokens:
        text.append(f&#34;‹{t}›{space}&#34;)
    return &#34;&#34;.join(text)


def checkModel(kind, thisModel, verbose):
    modelDefault = (
        LINE_MODEL_DEFAULT
        if kind == LINE
        else PAGE_MODEL_DEFAULT if kind == PAGE else SECTION_MODEL_DEFAULT
    )
    modelSpecs = (
        LINE_MODELS if kind == LINE else PAGE_MODELS if kind == PAGE else SECTION_MODELS
    )

    if thisModel is None:
        model = modelDefault
        if verbose == 1:
            console(f&#34;WARNING: No {kind} model specified. Assuming model {model}.&#34;)
        properties = {k: v[1] for (k, v) in modelSpecs[model].items()}
        return dict(model=model, properties=properties)

    if type(thisModel) is str:
        if thisModel in modelSpecs:
            thisModel = dict(model=thisModel)
        else:
            console(f&#34;ERROR: unknown {kind} model: {thisModel}&#34;)
            return False

    elif type(thisModel) is not dict:
        console(f&#34;ERROR: {kind} model must be a dict. You passed a {type(thisModel)}&#34;)
        return False

    model = thisModel.get(&#34;model&#34;, None)

    if model is None:
        model = modelDefault
        if verbose == 1:
            console(f&#34;WARNING: No {kind} model specified. Assuming model {model}.&#34;)
        thisModel[&#34;model&#34;] = model

    if model not in modelSpecs:
        console(f&#34;WARNING: unknown {kind} model: {thisModel}&#34;)
        return False

    if verbose &gt;= 0:
        console(f&#34;{kind} model is {model}&#34;)

    properties = {k: v for (k, v) in thisModel.items() if k != &#34;model&#34;}
    modelProperties = modelSpecs[model]

    good = True
    delKeys = []

    for k, v in properties.items():
        if k not in modelProperties:
            console(f&#34;WARNING: ignoring unknown {kind} model property {k}={v}&#34;)
            delKeys.append(k)
        elif type(v) is not modelProperties[k][0]:
            console(
                f&#34;ERROR: {kind} property {k} should have type {modelProperties[k][0]}&#34;
                f&#34; but {v} has type {type(v)}&#34;
            )
            good = False
    if good:
        for k in delKeys:
            del properties[k]

    for k, v in modelProperties.items():
        if k not in properties:
            if verbose == 1:
                console(
                    f&#34;WARNING: {kind} model property {k} not specified, &#34;
                    f&#34;taking default {v[1]}&#34;
                )
            properties[k] = v[1]

    if not good:
        return False

    return dict(model=model, properties=properties)


def matchModel(properties, tag, atts):
    if tag == properties[&#34;element&#34;]:
        criticalAtts = properties[&#34;attributes&#34;]
        match = True

        for k, cVal in criticalAtts.items():
            aVal = atts.get(k, None)

            thisNoMatch = (
                all(aVal != cV for cV in cVal)
                if type(cVal) in {list, tuple, set}
                else aVal != cVal
            )
            if thisNoMatch:
                match = False
                break
        return match


def setUp(kind):
    helpText = f&#34;&#34;&#34;
    Convert {kind} to TF.

    There are also commands to check the {kind} and to load the resulting TF.&#34;&#34;&#34;

    taskSpec = dict(
        check=&#34;reports on the elements in the source&#34;,
        convert=f&#34;converts {kind} to TF&#34;,
        load=&#34;loads the generated TF&#34;,
        app=&#34;configures the TF app for the result&#34;,
        apptoken=&#34;modifies the TF app to make it token- instead of character-based&#34;,
        browse=&#34;starts the TF browser on the result&#34;,
    )

    paramSpec = {
        &#34;tf&#34;: (
            (
                &#34;0 or latest: update latest version;\n\t\t&#34;
                &#34;1 2 3: increase major, intermediate, minor TF version;\n\t\t&#34;
                &#34;rest: explicit version.&#34;
            ),
            &#34;latest&#34;,
        ),
        &#34;sourceBase&#34;: (
            (&#34;empty: refDir/{kind.lower()};\n\t\t&#34; &#34;any directory of choice.&#34;),
            &#34;&#34;,
        ),
        &#34;reportDir&#34;: (
            (&#34;empty: refDir/report;\n\t\t&#34; &#34;any directory of choice.&#34;),
            &#34;&#34;,
        ),
        kind.lower(): (
            (
                &#34;0 or latest: latest version;\n\t\t&#34;
                &#34;-1 -2 etc: previous version, before previous, ...;\n\t\t&#34;
                &#34;1 2 etc: first version, second version, ...;\n\t\t&#34;
                &#34;rest: explicit version.&#34;
            ),
            &#34;latest&#34;,
        ),
        &#34;validate&#34;: (
            &#34;Whether to validate the XML input (use 1 for validation per file)&#34;,
            True,
        ),
    }

    flagSpec = dict(
        verbose=(&#34;Produce less or more progress and reporting messages&#34;, -1, 3),
        doc=(&#34;Do only this document&#34;, None, 0),
    )
    return (helpText, taskSpec, paramSpec, flagSpec)


def tweakTrans(
    template,
    procins,
    wordAsSlot,
    tokenAsSlot,
    charAsSlot,
    parentEdges,
    siblingEdges,
    tokenBased,
    sectionModel,
    sectionProperties,
    rendDesc,
    extra,
):
    if wordAsSlot:
        slot = WORD
        slotc = &#34;Word&#34;
        slotf = &#34;words&#34;
        xslot = &#34;`word`&#34;
    elif charAsSlot:
        slotc = &#34;Char&#34;
        slot = CHAR
        slotf = &#34;characters&#34;
        xslot = &#34;`char` and `word`&#34;
    elif tokenAsSlot or True:
        slotc = &#34;Token&#34;
        slot = T
        slotf = &#34;tokens&#34;
        xslot = &#34;`t` and `word`&#34;

    if parentEdges:
        hasParent = &#34;Yes&#34;
    else:
        hasParent = &#34;No&#34;

    if siblingEdges:
        hasSibling = &#34;Yes&#34;
    else:
        hasSibling = &#34;No&#34;

    if tokenBased:
        slot = TOKEN
        slotc = &#34;Token&#34;
        slotf = &#34;tokens&#34;
        xslot = &#34;`token`&#34;
        tokenGen = dedent(
            &#34;&#34;&#34;
            Tokens and sentence boundaries have been generated by a Natural Language
            Pipeline, such as Spacy.
            &#34;&#34;&#34;
        )
        tokenWord = &#34;token&#34;
        hasToken = &#34;Yes&#34;
    else:
        tokenGen = &#34;&#34;
        tokenWord = &#34;word&#34;
        hasToken = &#34;No&#34;

    if extra:
        hasExtra = &#34;Yes&#34;
    else:
        hasExtra = &#34;No&#34;

    if procins:
        doProcins = &#34;Yes&#34;
    else:
        doProcins = &#34;No&#34;

    levelNames = sectionProperties[&#34;levels&#34;]

    if sectionModel == &#34;II&#34;:
        nLevels = &#34;2&#34;
        chapterSection = levelNames[0]
        chunkSection = levelNames[1]
        head = sectionProperties[&#34;element&#34;]
        attributes = sectionProperties[&#34;attributes&#34;]
        propertiesRaw = repr(sectionProperties)
        properties = (
            &#34;&#34;.join(
                f&#34;\t*\t`{att}` = `{val}`\n&#34; for (att, val) in sorted(attributes.items())
            )
            if attributes
            else &#34;\t*\t*no attribute properties*\n&#34;
        )
    elif sectionModel == &#34;III&#34;:
        nLevels = &#34;3&#34;
        fileSection = levelNames[0]
        chapterSection = levelNames[1]
        chunkSection = levelNames[2]
        head = sectionProperties[&#34;element&#34;]
        attributes = sectionProperties[&#34;attributes&#34;]
        propertiesRaw = repr(sectionProperties)
        properties = (
            &#34;&#34;.join(
                f&#34;\t*\t`{att}` = `{val}`\n&#34; for (att, val) in sorted(attributes.items())
            )
            if attributes
            else &#34;\t*\t*no attribute properties*\n&#34;
        )
    else:
        nLevels = &#34;3&#34;
        folderSection = levelNames[0]
        fileSection = levelNames[1]
        chunkSection = levelNames[2]

    rendDescStr = &#34;\n&#34;.join(
        f&#34;`{val}` | {desc}&#34; for (val, desc) in sorted(rendDesc.items())
    )
    modelKeepRe = re.compile(rf&#34;«(?:begin|end)Model{sectionModel}»&#34;)
    modelRemoveRe = re.compile(r&#34;«beginModel([^»]+)».*?«endModel\1»&#34;, re.S)
    slotKeepRe = re.compile(rf&#34;«(?:begin|end)Slot{slot}»&#34;)
    slotRemoveRe = re.compile(r&#34;«beginSlot([^»]+)».*?«endSlot\1»&#34;, re.S)
    tokenKeepRe = re.compile(rf&#34;«(?:begin|end)Token{hasToken}»&#34;)
    tokenRemoveRe = re.compile(r&#34;«beginToken([^»]+)».*?«endToken\1»&#34;, re.S)
    parentKeepRe = re.compile(rf&#34;«(?:begin|end)Parent{hasParent}»&#34;)
    parentRemoveRe = re.compile(r&#34;«beginParent([^»]+)».*?«endParent\1»&#34;, re.S)
    siblingKeepRe = re.compile(rf&#34;«(?:begin|end)Sibling{hasSibling}»&#34;)
    siblingRemoveRe = re.compile(r&#34;«beginSibling([^»]+)».*?«endSibling\1»&#34;, re.S)
    extraKeepRe = re.compile(rf&#34;«(?:begin|end)Extra{hasExtra}»&#34;)
    extraRemoveRe = re.compile(r&#34;«beginExtra([^»]+)».*?«endToken\1»&#34;, re.S)
    procinsKeepRe = re.compile(rf&#34;«(?:begin|end)Procins{doProcins}»&#34;)
    procinsRemoveRe = re.compile(r&#34;«beginProcins([^»]+)».*?«endToken\1»&#34;, re.S)

    skipVars = re.compile(r&#34;«[^»]+»&#34;)

    text = (
        template.replace(&#34;«slot»&#34;, slot)
        .replace(&#34;«Slot»&#34;, slotc)
        .replace(&#34;«slotf»&#34;, slotf)
        .replace(&#34;«char and word»&#34;, xslot)
        .replace(&#34;«tokenWord»&#34;, tokenWord)
        .replace(&#34;«token generation»&#34;, tokenGen)
        .replace(&#34;«nLevels»&#34;, nLevels)
        .replace(&#34;«sectionModel»&#34;, sectionModel)
        .replace(&#34;«rendDesc»&#34;, rendDescStr)
        .replace(&#34;«extraFeatures»&#34;, extra)
    )
    if sectionModel == &#34;II&#34;:
        text = (
            text.replace(&#34;«head»&#34;, head)
            .replace(&#34;«properties»&#34;, properties)
            .replace(&#34;«propertiesRaw»&#34;, propertiesRaw)
            .replace(&#34;«chapter»&#34;, chapterSection)
            .replace(&#34;«chunk»&#34;, chunkSection)
        )
    elif sectionModel == &#34;III&#34;:
        text = (
            text.replace(&#34;«file»&#34;, fileSection)
            .replace(&#34;«head»&#34;, head)
            .replace(&#34;«properties»&#34;, properties)
            .replace(&#34;«propertiesRaw»&#34;, propertiesRaw)
            .replace(&#34;«chapter»&#34;, chapterSection)
            .replace(&#34;«chunk»&#34;, chunkSection)
        )
    else:
        text = (
            text.replace(&#34;«folder»&#34;, folderSection)
            .replace(&#34;«file»&#34;, fileSection)
            .replace(&#34;«chunk»&#34;, chunkSection)
        )

    text = parentKeepRe.sub(&#34;&#34;, text)
    text = parentRemoveRe.sub(&#34;&#34;, text)
    text = siblingKeepRe.sub(&#34;&#34;, text)
    text = siblingRemoveRe.sub(&#34;&#34;, text)
    text = tokenKeepRe.sub(&#34;&#34;, text)
    text = tokenRemoveRe.sub(&#34;&#34;, text)
    text = modelKeepRe.sub(&#34;&#34;, text)
    text = modelRemoveRe.sub(&#34;&#34;, text)
    text = slotKeepRe.sub(&#34;&#34;, text)
    text = slotRemoveRe.sub(&#34;&#34;, text)
    text = extraKeepRe.sub(&#34;&#34;, text)
    text = extraRemoveRe.sub(&#34;&#34;, text)
    text = procinsKeepRe.sub(&#34;&#34;, text)
    text = procinsRemoveRe.sub(&#34;&#34;, text)

    text = skipVars.sub(&#34;&#34;, text)

    if extra:
        text += dedent(
            f&#34;&#34;&#34;
            # Additional features

            {extra}
            &#34;&#34;&#34;
        )

    return text


def lookupSource(cv, cur, tokenAsSlot, specs):
    &#34;&#34;&#34;Looks up information from the current XML stack.

    The current XML stack contains the ancestry of the current node, including
    the current node itself.

    It is a list of components, corresponding to the path from the root node to the
    current node.
    Each component is a tuple, consisting of the tag name and the attributes of
    an XML node.

    Against this stack a sequence of instructions, given in `specs`, is executed.
    These instructions collect information from the stack, under certain conditions,
    and put that information into a feature, as value for a certain node.

    Here is an example of a single instruction:

    Parameters
    ----------
    cv: object
        The converter object, needed to issue actions.
    cur: dict
        Various pieces of data collected during walking
        and relevant for some next steps in the walk.
    specs: tuple
        A sequence of instructions what to look for.
        Each instruction has the following parts:

        *   `pathSpec`
        *   `nodeType`
        *   `featureName`

        The effect is:

        The `pathSpec` is compared to the current XML stack.
        If it matches the current node, the text content of the current node or one of
        its attributes will be collected and put in a feature with name
        `featureName`, for the current TF node of type `nodeType`.

        The `pathSpec` is a list of components.
        The first component should match the top of the XML stack, the second
        component the element that is below the top, etc.
        Each component is a tuple of

        *   a tag name;
        *   a dictionary of attribute values;

        The first component may have a tag name that has `@` plus an attribute name
        appended to it. That means that the information will be extracted from
        that attribute, not from the content of the element.
    &#34;&#34;&#34;
    nest = cur[XNEST]
    nNest = len(nest)

    for path, nodeType, feature in specs:
        nPath = len(path)

        if nPath &gt; nNest:
            continue

        ok = True
        extractAttr = None

        for p, (lookForTag, lookForAtts) in enumerate(path):
            (compareToTag, compareToAtts) = nest[-(p + 1)]

            if p == 0:
                pieces = lookForTag.split(&#34;@&#34;, 1)
                if len(pieces) == 2:
                    (lookForTag, extractAttr) = pieces
                else:
                    extractAttr = None
            ok = compareToTag == lookForTag

            if not ok:
                break

            if lookForAtts is not None:
                for att, val in lookForAtts.items():
                    if att not in compareToAtts or compareToAtts[att] != val:
                        ok = False
                        break

            if not ok:
                break

        if not ok:
            continue

        targetNode = cur[NODE].get(nodeType, None)

        if targetNode is None:
            return

        sourceNode = cur[TNEST][-1]
        slots = cv.linked(sourceNode)
        sourceText = (
            (
                &#34;&#34;.join(
                    cv.get(&#34;str&#34;, (T, slot)) + cv.get(&#34;after&#34;, (T, slot))
                    for slot in slots
                )
                if tokenAsSlot
                else &#34;&#34;.join(cv.get(&#34;ch&#34;, (CHAR, slot)) for slot in slots)
            )
            if extractAttr is None
            else cv.get(extractAttr, sourceNode)
        )
        sourceText = (sourceText or &#34;&#34;).strip()
        source = {feature: sourceText}
        cv.feature(targetNode, **source)


def parseIIIF(settings, prod, selector):
    &#34;&#34;&#34;Parse the iiif yaml file.

    We fill in the parameters.
    &#34;&#34;&#34;

    def applySwitches(prod, constants, switches):
        if len(switches):
            for k, v in switches[&#34;prod&#34; if prod else &#34;dev&#34;].items():
                constants[k] = v

        return constants

    def substituteConstants(data, macros, constants):
        tpd = type(data)

        if tpd is str:
            for k, v in macros.items():
                pattern = f&#34;&lt;{k}&gt;&#34;
                data = data.replace(pattern, str(v))

            for k, v in constants.items():
                pattern = f&#34;«{k}»&#34;

                if type(v) is int and data == pattern:
                    data = v
                    break
                else:
                    data = data.replace(pattern, str(v))

            return data

        if tpd is list:
            return [substituteConstants(item, macros, constants) for item in data]

        if tpd is dict:
            return {
                k: substituteConstants(v, macros, constants) for (k, v) in data.items()
            }

        return data

    constants = applySwitches(
        prod, settings.get(&#34;constants&#34;, {}), settings.get(&#34;switches&#34;, {})
    )
    macros = applySwitches(
        prod, settings.get(&#34;macros&#34;, {}), settings.get(&#34;switches&#34;, {})
    )

    return AttrDict(
        {
            x: substituteConstants(xText, macros, constants)
            for (x, xText) in settings[selector].items()
        }
    )


def operationalize(data):
    scanInfo = {}

    for extraFeat, featInfo in data.items():
        for nodeType, info in featInfo.items():
            variables = info[&#34;vars&#34;]
            value = info[&#34;value&#34;]

            newVars = {}

            for name, val in variables.items():
                if val.endswith(&#34;-1&#34;):
                    newVal = val[0:-2]
                    shift = -1
                elif val.endswith(&#34;+1&#34;):
                    newVal = val[0:-2]
                    shift = 1
                else:
                    newVal = val
                    shift = 0

                feat = tuple(newVal.split(&#34;.&#34;, 1))

                if len(feat) == 1:
                    parent = None
                    child = None
                    feat = feat[0]
                else:
                    parent, feat = feat

                    if parent.startswith(&#34;-&#34;):
                        child = parent[1:]
                        parent = None
                    else:
                        child = None

                newVars[name] = (parent, child, feat, shift)

            scanInfo.setdefault(nodeType, []).append((extraFeat, value, newVars))

    return scanInfo


def fillinIIIF(data, **kwargs):
    tpd = type(data)

    if tpd is str:
        for k, v in kwargs.items():
            pattern = &#34;{&#34; + k + &#34;}&#34;

            if type(v) is int and data == pattern:
                data = v
                break
            else:
                data = data.replace(pattern, str(v))

        return data

    if tpd is list:
        return [fillinIIIF(item, **kwargs) for item in data]

    if tpd is dict:
        return {k: fillinIIIF(v, **kwargs) for (k, v) in data.items()}

    return data</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="tf.convert.helpers.CM_LIT"><code class="name">var <span class="ident">CM_LIT</span></code></dt>
<dd>
<div class="desc"><p>The value is taken literally from a TEI attribute.</p>
<p>Code <code>tei</code>, since there is a 1-1 correspondence with the TEI source.</p></div>
</dd>
<dt id="tf.convert.helpers.CM_LITC"><code class="name">var <span class="ident">CM_LITC</span></code></dt>
<dd>
<div class="desc"><p>The value is results from more intricate processing of material in the TEI.</p>
<p><em>More intricate means</em>: we derive data that goes beyond pure XML syntax.</p>
<p>Examples:</p>
<ul>
<li>The values of the <code>rend</code> attributes are translated into <code>rend_</code><em>value</em> features;</li>
<li>Adding features <code>is_meta</code> (being inside the TEI-header) and <code>is_note</code>
(being inside a note);</li>
<li>The feature that gives the content of a (character) slot;</li>
<li>Decomposing strings into words material and after-word material.</li>
</ul>
<p>Code <code><a title="tf" href="../index.html">tf</a></code>, since this is for the benefit of the resulting TF dataset.</p></div>
</dd>
<dt id="tf.convert.helpers.CM_LITP"><code class="name">var <span class="ident">CM_LITP</span></code></dt>
<dd>
<div class="desc"><p>The value results from straightforward processing of material in the TEI.</p>
<p>Code <code>tei</code>, since there is a direct correspondence with the TEI source.</p>
<p><em>Straightforward</em> means: by taking into account the semantics of XML.</p>
<p>Examples:</p>
<ul>
<li>Generated white-space based on whether elements are pure or mixed;</li>
<li>Edges between parent and child elements, or sibling elements.</li>
</ul></div>
</dd>
<dt id="tf.convert.helpers.CM_NLP"><code class="name">var <span class="ident">CM_NLP</span></code></dt>
<dd>
<div class="desc"><p>The value is added by an NLP pipeline w.r.t. the material in the TEI.</p>
<p>Code <code>nlp</code>, since this comes from third party software.</p>
<p>Examples:</p>
<ul>
<li>The feature <code>nsent</code> which gives the sentence number in the corpus.
Sentences are not encoded in the TEI, but detected by an NLP program such as Spacy.</li>
</ul></div>
</dd>
<dt id="tf.convert.helpers.CM_PROV"><code class="name">var <span class="ident">CM_PROV</span></code></dt>
<dd>
<div class="desc"><p>The value is added by the conversion to TF w.r.t. the material in the TEI.</p>
<p>Examples:</p>
<ul>
<li>Slots in empty elements, in order to anchor the element to the text sequence;</li>
<li>Section levels, based on the folder and file that the TEI source is in;</li>
<li>A section level within the TEI, defined from several elements and the way they
are nested;</li>
</ul>
<p>Code <code><a title="tf" href="../index.html">tf</a></code>, since this is for the benefit of the resulting TF dataset.</p></div>
</dd>
<dt id="tf.convert.helpers.CONVERSION_METHODS"><code class="name">var <span class="ident">CONVERSION_METHODS</span></code></dt>
<dd>
<div class="desc"><p>Information about the conversion.</p>
<p>When we produce TF features, we specify a bit of information in the feature
metadata as how we arrived at the specific value.</p>
<p>That information ends up in two keys:</p>
<ul>
<li><code>conversionMethod</code>: with values any of:<ul>
<li><code><a title="tf.convert.helpers.CM_LIT" href="#tf.convert.helpers.CM_LIT">CM_LIT</a></code></li>
<li><code><a title="tf.convert.helpers.CM_LITP" href="#tf.convert.helpers.CM_LITP">CM_LITP</a></code></li>
<li><code><a title="tf.convert.helpers.CM_LITC" href="#tf.convert.helpers.CM_LITC">CM_LITC</a></code></li>
<li><code><a title="tf.convert.helpers.CM_PROV" href="#tf.convert.helpers.CM_PROV">CM_PROV</a></code></li>
<li><code><a title="tf.convert.helpers.CM_NLP" href="#tf.convert.helpers.CM_NLP">CM_NLP</a></code></li>
</ul>
</li>
<li>
<p><code>conversionCode</code>: the value is derived from <code>conversionMethod</code> by looking it
up in this table. These values can be used to qualify the name of the attribute
for further processing.</p>
<p>For example, if you have a feature <code>n</code> that originates literally from the TEI,
you could pass it on as <code>tei:n</code>.</p>
<p>But if you have a feature <code>chapter</code> that is provided by the conversion,
you could pass it on as <code>tf:chapter</code>.</p>
<p>This passing on is a matter of other software, that takes the generated TF as
input and processes it further, e.g. as annotations.</p>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">More methods and codes</p>
</div>
<p>The TEI conversion is customizable by providing your own methods to several hooks
in the program. These hooks may generate extra features, which you can give metadata
in the <code>tei.yaml</code> file next to the <code>tei.py</code> file where you define the custom functions.</p>
<p>It is advised to state appropriate values for the <code>conversionMethod</code> and
<code>conversionCode</code> fields of these features.</p>
<p>Examples:</p>
<ul>
<li>
<p>A feature <code>country</code> is derived from specific elements in the TEI Header, and
defined for nodes of type <code>letter</code>.
This happens in order to support the software of Team Text that shows the
text on a webpage.</p>
<p>In such a case you could define</p>
<ul>
<li>`conversionMethod="derived"</li>
<li>`conversionCode="tt"</li>
</ul>
</li>
</ul></div>
</dd>
<dt id="tf.convert.helpers.SECTION_MODELS"><code class="name">var <span class="ident">SECTION_MODELS</span></code></dt>
<dd>
<div class="desc"><p>Models for sections.</p>
<p>A section is a part of the corpus that is defined by a set of files,
or by elements within a single TEI source file.</p>
<p>A model</p></div>
</dd>
<dt id="tf.convert.helpers.SECTION_MODEL_DEFAULT"><code class="name">var <span class="ident">SECTION_MODEL_DEFAULT</span></code></dt>
<dd>
<div class="desc"><p>Default model for sections.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tf.convert.helpers.checkModel"><code class="name flex">
<span>def <span class="ident">checkModel</span></span>(<span>kind, thisModel, verbose)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.fillinIIIF"><code class="name flex">
<span>def <span class="ident">fillinIIIF</span></span>(<span>data, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.getWhites"><code class="name flex">
<span>def <span class="ident">getWhites</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.lookupSource"><code class="name flex">
<span>def <span class="ident">lookupSource</span></span>(<span>cv, cur, tokenAsSlot, specs)</span>
</code></dt>
<dd>
<div class="desc"><p>Looks up information from the current XML stack.</p>
<p>The current XML stack contains the ancestry of the current node, including
the current node itself.</p>
<p>It is a list of components, corresponding to the path from the root node to the
current node.
Each component is a tuple, consisting of the tag name and the attributes of
an XML node.</p>
<p>Against this stack a sequence of instructions, given in <code>specs</code>, is executed.
These instructions collect information from the stack, under certain conditions,
and put that information into a feature, as value for a certain node.</p>
<p>Here is an example of a single instruction:</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cv</code></strong> :&ensp;<code>object</code></dt>
<dd>The converter object, needed to issue actions.</dd>
<dt><strong><code>cur</code></strong> :&ensp;<code>dict</code></dt>
<dd>Various pieces of data collected during walking
and relevant for some next steps in the walk.</dd>
<dt><strong><code>specs</code></strong> :&ensp;<code>tuple</code></dt>
<dd>
<p>A sequence of instructions what to look for.
Each instruction has the following parts:</p>
<ul>
<li><code>pathSpec</code></li>
<li><code>nodeType</code></li>
<li><code>featureName</code></li>
</ul>
<p>The effect is:</p>
<p>The <code>pathSpec</code> is compared to the current XML stack.
If it matches the current node, the text content of the current node or one of
its attributes will be collected and put in a feature with name
<code>featureName</code>, for the current TF node of type <code>nodeType</code>.</p>
<p>The <code>pathSpec</code> is a list of components.
The first component should match the top of the XML stack, the second
component the element that is below the top, etc.
Each component is a tuple of</p>
<ul>
<li>a tag name;</li>
<li>a dictionary of attribute values;</li>
</ul>
<p>The first component may have a tag name that has <code>@</code> plus an attribute name
appended to it. That means that the information will be extracted from
that attribute, not from the content of the element.</p>
</dd>
</dl></div>
</dd>
<dt id="tf.convert.helpers.matchModel"><code class="name flex">
<span>def <span class="ident">matchModel</span></span>(<span>properties, tag, atts)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.operationalize"><code class="name flex">
<span>def <span class="ident">operationalize</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.parseIIIF"><code class="name flex">
<span>def <span class="ident">parseIIIF</span></span>(<span>settings, prod, selector)</span>
</code></dt>
<dd>
<div class="desc"><p>Parse the iiif yaml file.</p>
<p>We fill in the parameters.</p></div>
</dd>
<dt id="tf.convert.helpers.repTokens"><code class="name flex">
<span>def <span class="ident">repTokens</span></span>(<span>tokens)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.setUp"><code class="name flex">
<span>def <span class="ident">setUp</span></span>(<span>kind)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>line)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="tf.convert.helpers.tweakTrans"><code class="name flex">
<span>def <span class="ident">tweakTrans</span></span>(<span>template, procins, wordAsSlot, tokenAsSlot, charAsSlot, parentEdges, siblingEdges, tokenBased, sectionModel, sectionProperties, rendDesc, extra)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<p><a href="https://github.com/annotation" title="annotation on GitHub"><img src="../../tf/images/tf-small.png" alt="annotation"></a></p>
<p><a href="../../tf/index.html">tf home</a> -
<a href="../../tf/cheatsheet.html">cheat sheet</a> -
<a href="https://github.com/annotation/text-fabric" title="GitHub repo"><img src="../../tf/images/GitHub_Logo.png" alt="GitHub" width="50"></a></p>
</p>
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tf.convert" href="index.html">tf.convert</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="tf.convert.helpers.CM_LIT" href="#tf.convert.helpers.CM_LIT">CM_LIT</a></code></li>
<li><code><a title="tf.convert.helpers.CM_LITC" href="#tf.convert.helpers.CM_LITC">CM_LITC</a></code></li>
<li><code><a title="tf.convert.helpers.CM_LITP" href="#tf.convert.helpers.CM_LITP">CM_LITP</a></code></li>
<li><code><a title="tf.convert.helpers.CM_NLP" href="#tf.convert.helpers.CM_NLP">CM_NLP</a></code></li>
<li><code><a title="tf.convert.helpers.CM_PROV" href="#tf.convert.helpers.CM_PROV">CM_PROV</a></code></li>
<li><code><a title="tf.convert.helpers.CONVERSION_METHODS" href="#tf.convert.helpers.CONVERSION_METHODS">CONVERSION_METHODS</a></code></li>
<li><code><a title="tf.convert.helpers.SECTION_MODELS" href="#tf.convert.helpers.SECTION_MODELS">SECTION_MODELS</a></code></li>
<li><code><a title="tf.convert.helpers.SECTION_MODEL_DEFAULT" href="#tf.convert.helpers.SECTION_MODEL_DEFAULT">SECTION_MODEL_DEFAULT</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="tf.convert.helpers.checkModel" href="#tf.convert.helpers.checkModel">checkModel</a></code></li>
<li><code><a title="tf.convert.helpers.fillinIIIF" href="#tf.convert.helpers.fillinIIIF">fillinIIIF</a></code></li>
<li><code><a title="tf.convert.helpers.getWhites" href="#tf.convert.helpers.getWhites">getWhites</a></code></li>
<li><code><a title="tf.convert.helpers.lookupSource" href="#tf.convert.helpers.lookupSource">lookupSource</a></code></li>
<li><code><a title="tf.convert.helpers.matchModel" href="#tf.convert.helpers.matchModel">matchModel</a></code></li>
<li><code><a title="tf.convert.helpers.operationalize" href="#tf.convert.helpers.operationalize">operationalize</a></code></li>
<li><code><a title="tf.convert.helpers.parseIIIF" href="#tf.convert.helpers.parseIIIF">parseIIIF</a></code></li>
<li><code><a title="tf.convert.helpers.repTokens" href="#tf.convert.helpers.repTokens">repTokens</a></code></li>
<li><code><a title="tf.convert.helpers.setUp" href="#tf.convert.helpers.setUp">setUp</a></code></li>
<li><code><a title="tf.convert.helpers.tokenize" href="#tf.convert.helpers.tokenize">tokenize</a></code></li>
<li><code><a title="tf.convert.helpers.tweakTrans" href="#tf.convert.helpers.tweakTrans">tweakTrans</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://pure.knaw.nl/portal/en/persons/dirk-roorda">Dirk Roorda</a>
<a href="https://huc.knaw.nl"><img alt="HuC" src="../../tf/images/huc.png" width="200" alt="Humanities Cluster"></a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
