<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tf.convert.addnlp API documentation</title>
<meta name="description" content="Add data from an NLP pipeline …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tf.convert.addnlp</code></h1>
</header>
<section id="section-intro">
<p>Add data from an NLP pipeline.</p>
<p>When you have used <code><a title="tf.convert.tei" href="tei.html">tf.convert.tei</a></code> to convert a TEI data source into a TF dataset,
the situation with words and sentences is usually not satisfactory.
In most TEI sources, words and sentences are not explicitly marked up, and it is
really hard to build token detection and sentence boundary detection into the
conversion program.</p>
<p>There is a better way.
You can use this module to have tokens and sentences detected by NLP pipelines
(currently only Spacy is supported).
These tokens and sentences will then be transformed to nodes and attributes
and inserted in the TF dataset as a new version.</p>
<p>The original slots in the TF dataset (characters) will be discarded, because the
new tokens will be used as slots.</p>
<p><strong>This is work in progress. Details of the workflow may change rather often!</strong></p>
<h2 id="requirements">Requirements</h2>
<ul>
<li>The initial data set should be one that is generated by <code><a title="tf.convert.tei" href="tei.html">tf.convert.tei</a></code>
with the option <code>wordAsSlot=False</code>. So it is a <em>character</em>-based dataset.</li>
<li>The version of the initial data should end with the string <code>pre</code>, e.g.
<code>0.8pre</code>.</li>
</ul>
<h2 id="effect">Effect</h2>
<ul>
<li>
<p>A new version of the data (whose label is the old version minus the <code>pre</code>)
will be produced:</p>
<ul>
<li>with new node types <code>sentence</code> and <code>token</code></li>
<li>with <code>token</code> as slot type</li>
<li>with the old slot type (char) removed</li>
<li>with the feature <code>ch</code> on characters removed</li>
<li>with other node features on <code>char</code> translated to equally named
features on <code>token</code></li>
<li>with other node and edge features translated faithfully to the new situation.</li>
</ul>
</li>
</ul>
<h2 id="homework">Homework</h2>
<ul>
<li>
<p>The new data needs a slightly different TF app than the original version.
You can generate that with the program that created the TF from the TEI,
typically</p>
<p><code>sh
python tfFromTei.py apptoken</code></p>
</li>
</ul>
<h2 id="examples">Examples</h2>
<ul>
<li><a href="https://nbviewer.org/github/annotation/mobydick/blob/main/programs/creation.ipynb">Moby Dick</a>.</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L1-L1091" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;Add data from an NLP pipeline.

When you have used `tf.convert.tei` to convert a TEI data source into a TF dataset,
the situation with words and sentences is usually not satisfactory.
In most TEI sources, words and sentences are not explicitly marked up, and it is
really hard to build token detection and sentence boundary detection into the
conversion program.

There is a better way.
You can use this module to have tokens and sentences detected by NLP pipelines
(currently only Spacy is supported).
These tokens and sentences will then be transformed to nodes and attributes
and inserted in the TF dataset as a new version.

The original slots in the TF dataset (characters) will be discarded, because the
new tokens will be used as slots.

**This is work in progress. Details of the workflow may change rather often!**

## Requirements

*   The initial data set should be one that is generated by `tf.convert.tei`
    with the option `wordAsSlot=False`. So it is a *character*-based dataset.
*   The version of the initial data should end with the string `pre`, e.g.
    `0.8pre`.

## Effect

*   A new version of the data (whose label is the old version minus the `pre`)
    will be produced:

    *   with new node types `sentence` and `token`
    *   with `token` as slot type
    *   with the old slot type (char) removed
    *   with the feature `ch` on characters removed
    *   with other node features on `char` translated to equally named
        features on `token`
    *   with other node and edge features translated faithfully to the new situation.

## Homework

*   The new data needs a slightly different TF app than the original version.
    You can generate that with the program that created the TF from the TEI,
    typically

    ``` sh
    python tfFromTei.py apptoken
    ```

## Examples

*   [Moby Dick](https://nbviewer.org/github/annotation/mobydick/blob/main/programs/creation.ipynb).

&#34;&#34;&#34;

import sys
import re

from .recorder import Recorder
from ..tools.xmlschema import Analysis
from ..tools.myspacy import tokensAndSentences
from ..dataset import modify
from ..core.helpers import console
from ..core.files import initTree, getLocation, dirMake, dirExists
from ..core.timestamp import DEEP, TERSE
from ..app import use
from ..lib import writeList, readList


class NLPipeline:
    def __init__(self, app=None):
        &#34;&#34;&#34;Enrich a TF dataset with annotations generated by an NLP pipeline.&#34;&#34;&#34;
        self.good = True
        self.verbose = -1

    def loadApp(self, app=None, verbose=None):
        &#34;&#34;&#34;Loads a given TF app or loads the TF app based on the working directory.

        Parameters
        ----------
        app: object, optional None
            The handle to the original TF dataset, already loaded.
            We assume that the original data resides in the current
            version, which has the string `pre` appended to it,
            e.g. in version `1.3pre`.
            We create a new version of the dataset, with the same number,
            but without the `pre`.

            If not given, we load the TF app that is nearby in the file system.
        verbose: boolean, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        if app is None:
            (backend, org, repo, relative) = getLocation()
            if any(s is None for s in (backend, org, repo, relative)):
                console(
                    &#34;Not working in a repo: &#34;
                    f&#34;backend={backend} org={org} repo={repo} relative={relative}&#34;
                )
                self.good = False
                return

            app = use(
                f&#34;{org}/{repo}{relative}:clone&#34;,
                checkout=&#34;clone&#34;,
                backend=backend,
                silent=DEEP,
            )

        self.app = app
        repoDir = app.repoLocation
        txtDir = f&#34;{repoDir}/_temp/txt&#34;
        self.txtDir = txtDir
        self.tokenFile = f&#34;{txtDir}/tokens.tsv&#34;
        self.sentenceFile = f&#34;{txtDir}/sentences.tsv&#34;
        self.textPath = f&#34;{repoDir}/_temp/txt/plain.txt&#34;

    @staticmethod
    def help():
        &#34;&#34;&#34;Print a help text to the console.

        In order to give help on the command line, here is a pre-baked help text.
        Only the name of the conversion script needs to be merged in.
        &#34;&#34;&#34;

        console(
            &#34;&#34;&#34;

        Add NLP-generated features to a TF dataset.

        There are also commands to perform this operation step by step.

        addnlp [tasks/flags] [--help]

        --help: show this text and exit

        tasks:
            a sequence of tasks:
            plaintext:
                make a plain text for the NLP tools
            lingo:
                run the NLP tool on the plain text
            ingest:
                ingest the results of the NLP tool in the dataset
            all:
                all tasks

        flags:
            -write, +write:
                whether to write the generated files with plain text and node
                positions to disk
            +verbose, ++verbose:
                Produce more progress and reporting messages
        &#34;&#34;&#34;
        )

    def getElementInfo(self, verbose=None):
        &#34;&#34;&#34;Analyse the schema.

        The XML schema has useful information about the XML elements that
        occur in the source. Here we extract that information and make it
        fast-accessible.

        Parameters
        ----------
        verbose: boolean, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.

        Returns
        -------
        dict
            Keyed by element name (without namespaces), where the value
            for each name is a tuple of booleans: whether the element is simple
            or complex; whether the element allows mixed content or only pure content.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        self.elementDefs = {}

        A = Analysis(verbose=verbose)
        A.configure()
        A.interpret()
        if not A.good:
            console(&#34;Could not get TEI element definitions&#34;)
            return

        elementDefs = {name: (typ, mixed) for (name, typ, mixed) in A.getDefs()}
        self.mixedTypes = {x for (x, (typ, mixed)) in elementDefs.items() if mixed}

    def generatePlain(self, takeAways={&#34;note&#34;, &#34;orig&#34;, &#34;del&#34;}, empty=&#34;empty&#34;):
        &#34;&#34;&#34;Generates a plain text out of a data source.

        The text is generatad in such a way that out of flow elements are collected
        and put at the end. Examples of such elements are notes.
        Leaving them at their original positions will interfere with sentence detection.

        We separate the flows clearly in the output, so that they are discernible
        in the output of the NLP pipeline.

        Parameters
        ----------
        takeAways: set, optional `{&#34;note&#34;, &#34;reg&#34;}`
            A set of node types whose content will be put in separate text flows at
            the end of the document.
        empty: string, optional empty
            Name of feature that identifies the empty slots.

        Returns
        -------
        tuple
            The result is a tuple consisting of

            *   *text*: the generated text
            *   *positions*: a list of nodes such that list item *i* contains
                the original slot that corresponds to the character *i* in the
                generated text (counting from zero).
        &#34;&#34;&#34;
        verbose = self.verbose
        write = self.write
        app = self.app
        info = app.info
        indent = app.indent
        api = app.api
        F = api.F
        Fs = api.Fs
        N = api.N

        sectionTypes = {&#34;file&#34;, &#34;folder&#34;, &#34;chapter&#34;, &#34;chunk&#34;, &#34;word&#34;}
        ignoreTypes = {&#34;word&#34;}
        sentenceBreakRe = re.compile(r&#34;[.!?]&#34;)

        info(&#34;Generating a plain text with positions ...&#34;, force=verbose &gt;= 0)
        self.getElementInfo()
        mixedTypes = self.mixedTypes

        flows = {elem: [] for elem in takeAways}
        flows[&#34;&#34;] = []
        flowStack = [&#34;&#34;]

        nTypeStack = []

        def finishSentence(flowContent):
            nContent = len(flowContent)
            lnw = None  # last non white position
            for i in range(nContent - 1, -1, -1):
                item = flowContent[i]
                if type(item) is not str or item.strip() == &#34;&#34;:
                    continue
                else:
                    lnw = i
                    break

            if lnw is None:
                return

            # note that every slot appears in the sequence preceded by a neg int
            # and followed by a pos int
            # Material outside slots may be followed and preceded by other strings
            # We have to make sure that what we add, falls outside any slot.
            # We do that by inspecting the following item:
            # if that is a positive int, we are in a slot so we have to insert material
            # after that int
            # If the following item is a string or a negative int,
            # so we can insert right after the point where we are.
            if not sentenceBreakRe.match(flowContent[lnw]):
                offset = 1
                if lnw &lt; nContent - 1:
                    following = flowContent[lnw + 1]
                    if type(following) is int and following &gt; 0:
                        offset = 2
                flowContent.insert(lnw + offset, &#34;.&#34;)
                lnw += 1

            if not any(ch == &#34;\n&#34; for ch in flowContent[lnw + 1 :]):
                flowContent.append(&#34;\n&#34;)

        emptySlots = 0

        for (n, kind) in N.walk(events=True):
            nType = F.otype.v(n)

            if nType in ignoreTypes:
                continue

            isSeparateType = nType in takeAways

            if kind is None:  # slot type
                if Fs(empty).v(n):
                    emptySlots += 1
                    ch = &#34;￮&#34;
                else:
                    ch = F.ch.v(n)
                flows[flowStack[-1]].extend([-n, ch, n])

            elif kind:  # end node
                if isSeparateType:
                    flow = flowStack.pop()
                else:
                    flow = flowStack[-1]
                flowContent = flows[flow]

                if flow:
                    finishSentence(flowContent)
                else:
                    if nType == &#34;teiHeader&#34;:
                        finishSentence(flowContent)
                        flowContent.append(&#34; xxx.\nEND META.\n\n&#34;)
                    elif nType in sectionTypes:
                        flowContent.append(f&#34; xxx.\nEND {nType}.\n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            finishSentence(flowContent)
                nTypeStack.pop()

            else:  # start node
                nTypeStack.append(nType)

                if isSeparateType:
                    flowStack.append(nType)
                flow = flowStack[-1]
                flowContent = flows[flow]

                if isSeparateType:
                    flowContent.append(f&#34;\nITEM {flow}.\n&#34;)
                else:
                    if nType == &#34;teiHeader&#34;:
                        flowContent.append(&#34;BEGIN META.\n\n&#34;)
                    elif nType in sectionTypes:
                        flowContent.append(f&#34;BEGIN {nType}.\n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            flowContent.append(f&#34;{nType}. &#34;)

        indent(level=True)
        info(f&#34;Found {emptySlots} empty slots&#34;, tm=False, force=verbose &gt;= 0)

        rec = Recorder(app.api)

        for flow in sorted(flows):
            items = flows[flow]

            if len(items) == 0:
                continue

            rec.add(f&#34;BEGIN FLOW {flow if flow else &#39;MAIN&#39;}.\n\n&#34;)

            for item in items:
                if type(item) is int:
                    if item &lt; 0:
                        rec.start(-item)
                    else:
                        rec.end(item)
                else:
                    rec.add(item)

            rec.add(f&#34; xxx.\nEND FLOW {flow if flow else &#39;MAIN&#39;}.\n\n&#34;)

            info(
                (
                    f&#34;recorded flow {flow if flow else &#39;MAIN&#39;:&lt;10} &#34;
                    f&#34;with {len(items):&gt;6} items&#34;
                ),
                tm=False,
                force=verbose &gt;= 0,
            )

        indent(level=False)

        if write:
            textPath = self.textPath
            rec.write(textPath)
            info(
                f&#34;Done. Generated text and positions written to {textPath}&#34;,
                force=verbose &gt;= 0,
            )
        else:
            info(&#34;Done&#34;, force=verbose &gt;= 0)

        return (rec.text(), rec.positions(simple=True))

    @staticmethod
    def lingo(*args, **kwargs):
        return tokensAndSentences(*args, **kwargs)

    def ingest(
        self,
        isToken,
        positions,
        stream,
        slotFeature,
        tp,
        features,
        nFeature=None,
        skipBlanks=False,
        skipFlows=None,
        empty=None,
    ):
        &#34;&#34;&#34;Ingests a stream of NLP data and transforms it into nodes and features.

        The data is a stream of values associated with a spans of text.

        For each span a node will be created of the given type, and a feature
        of the given name will assign a value to that span.
        The value assigned is by default the value that is present in the data stream,
        but it is possible to specify a method to change the value.

        !!! caution
            The plain text on which the NLP pipeline has run may not correspond
            exactly with the text as defined by the corpus.
            When the plain text was generated, some extra convenience material
            may have been inserted.
            Items in the stream that refer to these pieces of text will be ignored.

            When items refer partly to proper corpus text and partly to convenience text,
            they will be narrowed down to the proper text.

        !!! caution
            The plain text may exhibit another order of material than the proper corpus
            text. For example, notes may have been collected and moved out of the
            main text flow to the end of the text.

            That means that if an item specifies a span in the plain text, it may
            not refer to a single span in the proper text, but to various spans.

            We take care to map all spans in the generated plain text back to *sets*
            of slots in the proper text.

        Parameters
        ----------
        isToken: boolean
            Whether the data specifies tokens or something else.
            Tokens are special because they are intended to become the new slot type.
        positions: list
            which slot node corresponds to which position in the plain text.

        stream: list of tuple
            The tuples should consist of

            *   *start*: a start number (char pos in the plain text, starting at `0`)
            *   *end*: an end number (char pos in tghe plain text plus one)
            *   *value*: a value for feature assignment

        tp: string
            The type of the nodes that will be generated.

        features: tuple
            The names of the features that will be generated.

        nFeature: string, optional None
            If not None, the name of a feature that will hold the sequence number of
            the element in the data stream, starting at 1.

        slotFeature: string, optional None
            The feature by which we can retrieve the character value of a slot

        skipBlanks: boolean, optional False
            If True, rows whose text component is only white space will be skipped.

        skipFlows: set
            set of elements whose resulting data in the stream should be ignored

        empty: string, optional empty
            Name of feature that identifies the empty slots.

        Returns
        -------
        tuple
            We deliver the following pieces of information in a tuple:

            * the last node
            * the mapping of the new nodes to the slots they occupy;
            * the data of the new feature.
        &#34;&#34;&#34;
        verbose = self.verbose
        app = self.app
        info = app.info
        indent = app.indent
        F = app.api.F
        Fs = app.api.Fs
        Fotypev = F.otype.v
        slotType = F.otype.slotType
        Fslotv = Fs(slotFeature).v
        if empty is not None:
            Femptyv = Fs(empty).v

        doN = nFeature is not None
        slotLinks = {}
        featuresData = {feat: {} for feat in features}
        if nFeature is not None:
            featuresData[nFeature] = {}
        if empty is not None:
            featuresData[empty] = {}

        if isToken:
            featToken = featuresData[features[0]]
            featAfter = featuresData[features[1]]

        whiteMultipleRe = re.compile(r&#34;^[ \n]{2,}$&#34;, re.S)

        node = 0
        itemsOutside = []
        itemsEmpty = []

        info(
            f&#34;generating {tp}-nodes with features {&#39;, &#39;.join(featuresData)}&#34;,
            force=verbose &gt;= 0,
        )
        indent(level=True)

        def addToken():
            nonlocal node
            nonlocal curSlots
            nonlocal curValue

            node += 1
            slotLinks[node] = curSlots
            featToken[node] = curValue
            for (feat, val) in zip(features[2:], vals[2:]):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

            curSlots = []
            curValue = &#34;&#34;

        def addSlot(slot):
            nonlocal node

            node += 1
            slotLinks[node] = [slot]
            featToken[node] = Fslotv(slot)
            if Femptyv(slot):
                featuresData[empty][node] = 1

        def addItem():
            nonlocal node

            node += 1
            slotLinks[node] = mySlots
            for (feat, val) in zip(features, vals):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

        # First add all empty slots, provided we are doing tokens

        if isToken:
            emptySlots = (
                {s for s in Fs(empty).s(1) if Fotypev(s) == slotType}
                if empty
                else set()
            )
            emptyWithinToken = 0
            spaceWithinToken = 0

            for slot in sorted(emptySlots):
                addSlot(slot)

        # now the data from the NLP pipeline

        flowBeginRe = re.compile(r&#34;BEGIN FLOW (\w+)&#34;)
        flowEndRe = re.compile(r&#34; xxx.\nEND FLOW (\w+)&#34;)

        skipping = False
        flow = None

        for (i, (b, e, *vals)) in enumerate(stream):
            if skipFlows is not None:
                text = vals[0]
                if skipping:
                    match = flowEndRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = False
                        flow = None
                        continue
                else:
                    match = flowBeginRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = flow in skipFlows
                        continue

            if skipping:
                continue

            mySlots = set()

            for j in range(b, e):
                s = positions[j]
                if s is not None:
                    mySlots.add(s)

            if len(mySlots) == 0:
                if doN:
                    vals.append(i + 1)
                itemsOutside.append((i, b, e, *vals))
                continue

            if skipBlanks and len(vals):
                slotsOrdered = sorted(mySlots)
                nSlots = len(slotsOrdered)

                start = min(
                    (
                        i
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=nSlots,
                )
                end = max(
                    (
                        i + 1
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=0,
                )

                if end &lt;= start:
                    itemsEmpty.append((i, b, e, *vals))
                    continue

                mySlots = slotsOrdered[start:end]
            else:
                mySlots = sorted(mySlots)

            curValue = &#34;&#34;
            curSlots = []

            nMySlots = len(mySlots)

            if isToken:
                # we might need to split tokens:
                # at points that correspond to empty slots
                # at spaces or newlines within the token
                # decompose it into individual characters

                tokenText = &#34;&#34;.join(Fslotv(s) for s in mySlots)

                if whiteMultipleRe.match(tokenText):
                    spaceWithinToken += 1
                    for slot in mySlots:
                        addSlot(slot)
                        spaceWithinToken += 1

                else:
                    for (i, slot) in enumerate(mySlots):
                        last = i == nMySlots - 1
                        if slot in emptySlots:
                            emptyWithinToken += 1
                            if curValue:
                                addToken()
                            if last:
                                featAfter[node] = vals[1]
                        else:
                            curValue += Fslotv(slot)
                            curSlots.append(slot)
                    if curValue:
                        addToken()
                        featAfter[node] = vals[1]
            else:
                addItem()

        repFeatures = &#34;, &#34;.join(features + ((nFeature,) if doN else ()))
        info(
            f&#34;{node} {tp} nodes have values assigned for {repFeatures}&#34;,
            force=verbose &gt;= 0,
        )
        if isToken:
            info(
                f&#34;{emptyWithinToken} empty slots have split surrounding tokens&#34;,
                force=verbose &gt;= 0,
            )
            info(
                f&#34;{spaceWithinToken} space slots have split into {slotType}s&#34;,
                force=verbose &gt;= 0,
            )

        tasks = [(&#34;Items contained in extra generated text&#34;, itemsOutside)]
        if skipBlanks:
            tasks.append((&#34;Items with empty final text&#34;, itemsEmpty))

        for (label, items) in tasks:
            nItems = len(items)
            info(f&#34;{nItems:&gt;5}x {label}&#34;, force=verbose &gt;= 0)
            indent(level=True)
            for (i, b, e, *vals) in items[0:5]:
                info(
                    f&#34;\t{i} span {b}-{e}: {&#39;, &#39;.join(str(v) for v in vals)}&#34;,
                    force=verbose == 1,
                )
            indent(level=False)

        indent(level=False)
        return (node, slotLinks, featuresData)

    def ingestTokensAndSentences(
        self,
        positions,
        tokenStream,
        sentenceStream,
        empty=&#34;empty&#34;,
        tokenType=&#34;token&#34;,
        tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None),
        removeSlotFeatures=(&#34;ch&#34;,),
        sentenceType=&#34;sentence&#34;,
        sentenceFeatures=(&#34;nsent&#34;,),
        sentenceSkipFlows={&#34;orig&#34;, &#34;del&#34;},
    ):
        &#34;&#34;&#34;Ingests a tokens and sentences in a dataset and turn the tokens into slots.

        By default:

        * tokens become nodes of a new type `token`;
        * the texts of a token ends up in the feature `str`;
        * if there is a space after a token, it ends up in the feature `after`;
        * sentences become nodes of a new type `sentence`;
        * the sentence number ends up in the feature `nsent`.
        * tokens become the new slots.

        But this function can also be adapted to token and sentence streams that
        have additional names and values, see below.

        The streams of tokens and sentences may contain more fields.
        In the parameters `tokenFeatures` and `sentenceFeatures` you may pass the
        feature names for the data in those fields.

        When the streams are read, for each feature name in the `tokenFeatures`
        (resp. `sentenceFeatures`) the corresponding field in the stream will be
        read, and the value found there will be assigned to that feature.

        If there are more fields in the stream than there are declared in the
        `tokenFeatures` (resp. `sentenceFeatures`) parameter, these extra fields will
        be ignored.

        The last feature name in these parameters is special.
        If it is None, it will be ignored.
        Otherwise, an extra feature with that name will be created, and it will be
        filled with the node numbers of the newly generated nodes.

        !!! hint &#34;Look at the defaults&#34;
            The default `tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None)` specifies that two
            fields from the tokenstream will be read, and those values will be assigned
            to features `str` and `after`.
            There will be no field with the node itself in it.

            The default `sentenceFeatures=(&#34;nsent&#34;,)` specifies that no field from the
            tokenstream will be read, but that there will be a feature `nsent` that
            has the node of each sentence as value.

        We have to ignore the sentence boundaries in some flows,
        e.g. the material coming from `&lt;orig&gt;` and `&lt;del&gt;` elements.
        However, in the flow coming from the `&lt;note&gt;` elements, we want to retain the
        sentence boundaries.

        Parameters
        ----------
        tokenStream: list
            The list of tokens as delivered by the NLP pipe.
        sentenceStream: list
            The list of sentences as delivered by the NLP pipe.
        empty: string, optional empty
            Names of features that identify the empty slots.
            Name of feature that identifies the empty slots.
        tokenType: string, optional str
            The node type for the tokens
        tokenFeatures: tuple, optional (&#34;str&#34;, &#34;after&#34;, &#34;&#34;)
            The names of the features that the token stream contains.
            There must be at least two features:
            the first one should give the token content, the second one the non-token
            material until the next token.
            The rest are additional features that the
            pipeline might supply.
        removeSlotFeatures: tuple, optional (&#34;ch&#34;,)
            The names of features defined on original slots that do not have to be
            carried over to the new slots of type token.
            There should be at least one feature: the character content of the slot.
        sentenceType: string, optional str
            The node type for the sentences
        sentenceFeatures: tuple, optional (&#34;nsent&#34;,)
            The names of the features that the sentence stream contains.
        sentenceSkipFlows: set
            The elements whose flows in the sentence stream should be ignored

        Returns
        -------
        string
            The new version number of the data that contains the tokens and sentences.
        &#34;&#34;&#34;
        app = self.app
        info = app.info
        indent = app.indent
        verbose = self.verbose
        silent = &#34;auto&#34; if verbose == 1 else TERSE if verbose == 0 else DEEP

        info(&#34;Ingesting tokens and sentences into the dataset ...&#34;, force=verbose &gt;= 0)
        indent(level=True)
        info(&#34;Mapping NLP data to nodes and features ...&#34;, force=verbose &gt;= 0)
        indent(level=True)

        slotFeature = removeSlotFeatures[0]

        slotLinks = {tokenType: {}, sentenceType: {}}
        features = {}
        for feat in tokenFeatures:
            if feat is not None:
                features[feat] = {}
        for feat in sentenceFeatures:
            if feat is not None:
                features[feat] = {}
        lastNode = {tokenType: 0, sentenceType: 0}

        i = 0
        for (isToken, data, skipFlows, tp, feats, skipBlanks, thisEmpty) in (
            (True, tokenStream, None, tokenType, tokenFeatures, False, empty),
            (
                False,
                sentenceStream,
                sentenceSkipFlows,
                sentenceType,
                sentenceFeatures,
                True,
                None,
            ),
        ):
            i += 1
            realFeats = feats[0:-1]
            nFeat = feats[-1]
            (node, theseSlotLinks, featuresData) = self.ingest(
                isToken,
                positions,
                data,
                slotFeature,
                tp,
                realFeats,
                nFeature=nFeat,
                skipBlanks=skipBlanks,
                skipFlows=skipFlows,
                empty=thisEmpty,
            )
            lastNode[tp] = node
            slotLinks[tp] = theseSlotLinks
            for (feat, featData) in featuresData.items():
                features[feat] = featData
            info(f&#34;{lastNode[tp]} {tp}s&#34;, force=verbose &gt;= 0)

        indent(level=False)

        info(&#34;Make a modified dataset ...&#34;, force=verbose &gt;= 0)

        repoDir = app.repoLocation
        versionPre = app.version
        version = versionPre.removesuffix(&#34;pre&#34;)
        origTf = f&#34;{repoDir}/tf/{versionPre}&#34;
        newTf = f&#34;{repoDir}/tf/{version}&#34;
        initTree(newTf, fresh=True, gentle=False)

        modify(
            origTf,
            newTf,
            addTypes=dict(
                token=dict(
                    nodeFrom=1,
                    nodeTo=lastNode[tokenType],
                    nodeSlots=slotLinks[tokenType],
                    nodeFeatures={
                        feat: features[feat]
                        for feat in tokenFeatures
                        if feat is not None
                    },
                ),
                sentence=dict(
                    nodeFrom=1,
                    nodeTo=lastNode[sentenceType],
                    nodeSlots=slotLinks[sentenceType],
                    nodeFeatures={
                        feat: features[feat]
                        for feat in sentenceFeatures
                        if feat is not None
                    },
                ),
            ),
            deleteTypes=(&#34;word&#34;,),
            featureMeta=dict(
                nsent=dict(
                    valueType=&#34;int&#34;,
                    description=&#34;number of sentence in corpus&#34;,
                ),
                otext={
                    &#34;fmt:text-orig-full&#34;: &#34;{&#34;
                    + tokenFeatures[0]
                    + &#34;}{&#34;
                    + tokenFeatures[1]
                    + &#34;}&#34;
                },
            ),
            replaceSlotType=(tokenType, *removeSlotFeatures),
            silent=silent,
        )
        info(&#34;Done&#34;, force=verbose &gt;= 0)
        indent(level=False)
        info(f&#34;Enriched data is available in version {version}&#34;, force=verbose &gt;= 0)
        info(
            &#34;You may need to adapt this TF app and its documentation:&#34;,
            tm=False,
            force=verbose &gt;= 0,
        )
        info(&#34;please run: python tfFromTei.py apptoken&#34;, tm=False, force=verbose &gt;= 0)
        return version

    def task(
        self,
        plaintext=False,
        lingo=False,
        ingest=False,
        write=False,
        verbose=-1,
        **kwargs,
    ):
        &#34;&#34;&#34;Carry out any task, possibly modified by any flag.

        This is a higher level function that can execute a selection of tasks.

        The tasks will be executed in a fixed order: plaintext, lingo, ingest.
        But you can select which one(s) must be executed.

        If multiple tasks must be executed and one fails, the subsequent tasks
        will not be executed.

        Parameters
        ----------
        plaintext: boolean, optional False
            Whether to generate the plain text and position files.
        lingo: boolean, optional False
            Whether to carry out NLP pipeline (Spacy).
        ingest: boolean, optional False
            Whether to ingest the NLP results into the dataset..
        verbose: integer, optional -1
            Produce no (-1), some (0) or many (1) orprogress and reporting messages
        write: boolean, optional False
            Whether to write the generated plain text and position files to disk.
        kwargs: dict
            remaining arguments that can serve as input for the task

        Returns
        -------
        boolean
            Whether all tasks have executed successfully.
        &#34;&#34;&#34;

        self.write = write
        self.verbose = verbose
        silent = TERSE if verbose == 1 else DEEP

        self.loadApp()
        if not self.good:
            return False

        app = self.app
        app.setSilent(silent)

        txtDir = self.txtDir
        textPath = self.textPath
        tokenFile = self.tokenFile
        sentenceFile = self.sentenceFile

        app.indent(reset=True)

        text = kwargs.get(&#34;text&#34;, None)
        positions = kwargs.get(&#34;positions&#34;, None)
        tokens = kwargs.get(&#34;tokens&#34;, None)
        sentences = kwargs.get(&#34;sentences&#34;, None)

        result = False

        if plaintext and self.good:
            (text, positions) = self.generatePlain()
            result = (text, positions) if self.good else False

        if lingo and self.good:
            app.info(&#34;Using NLP pipeline Spacy (may take a while)...&#34;, force=True)
            if text is None or positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                text = rec.text()
                positions = rec.positions()

            (tokens, sentences) = self.lingo(text)
            if write:
                if not dirExists(txtDir):
                    dirMake(txtDir)
                writeList(tokens, tokenFile, intCols=(True, True, False, False))
                writeList(sentences, sentenceFile, intCols=(True, True, False))
            app.info(&#34;NLP done&#34;, force=True)

            result = (tokens, sentences) if self.good else False

        if ingest and self.good:
            if positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                positions = rec.positions(simple=True)

            if tokens is None or sentences is None:
                tokens = readList(tokenFile)
                sentences = readList(sentenceFile)
            newVersion = self.ingestTokensAndSentences(positions, tokens, sentences)

            result = newVersion if self.good else False

        return result

    def run(self):
        &#34;&#34;&#34;Carry out tasks specified by arguments on the command line.

        This function inspects arguments, and runs the specified tasks,
        with the specified flags enabled.

        Returns
        -------
        data | boolean
            Data or True if the command was successful, False if not.
        &#34;&#34;&#34;
        possibleTasks = {&#34;all&#34;, &#34;plaintext&#34;, &#34;lingo&#34;, &#34;ingest&#34;}
        possibleFlags = {
            &#34;-write&#34;: False,
            &#34;+write&#34;: True,
            &#34;-verbose&#34;: -1,
            &#34;+verbose&#34;: 0,
            &#34;++verbose&#34;: 1,
        }
        possibleArgs = possibleTasks | set(possibleFlags)

        args = set(sys.argv[1:])

        if not len(args) or &#34;--help&#34; in args or &#34;-h&#34; in args:
            self.help()
            console(&#34;No task specified&#34;)
            return True

        illegalArgs = {arg for arg in args if arg not in possibleArgs}

        if len(illegalArgs):
            self.help()
            for arg in illegalArgs:
                console(f&#34;Illegal argument `{arg}`&#34;)
            return False

        tasks = {arg: True for arg in possibleTasks if arg in args}
        flags = {
            arg.lstrip(&#34;+-&#34;): val for (arg, val) in possibleFlags.items() if arg in args
        }

        if &#34;all&#34; in tasks:
            if tasks[&#34;all&#34;]:
                tasks = {arg: True for arg in possibleTasks if arg != &#34;all&#34;}
            else:
                del tasks[&#34;all&#34;]

        result = self.task(**tasks, **flags)

        if type(result) is bool:
            if not result:
                return False
            else:
                return True

        return result


def main():
    NLP = NLPipeline()
    NLP.run()


if __name__ == &#34;__main__&#34;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tf.convert.addnlp.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L1085-L1087" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def main():
    NLP = NLPipeline()
    NLP.run()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tf.convert.addnlp.NLPipeline"><code class="flex name class">
<span>class <span class="ident">NLPipeline</span></span>
<span>(</span><span>app=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Enrich a TF dataset with annotations generated by an NLP pipeline.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L70-L1082" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class NLPipeline:
    def __init__(self, app=None):
        &#34;&#34;&#34;Enrich a TF dataset with annotations generated by an NLP pipeline.&#34;&#34;&#34;
        self.good = True
        self.verbose = -1

    def loadApp(self, app=None, verbose=None):
        &#34;&#34;&#34;Loads a given TF app or loads the TF app based on the working directory.

        Parameters
        ----------
        app: object, optional None
            The handle to the original TF dataset, already loaded.
            We assume that the original data resides in the current
            version, which has the string `pre` appended to it,
            e.g. in version `1.3pre`.
            We create a new version of the dataset, with the same number,
            but without the `pre`.

            If not given, we load the TF app that is nearby in the file system.
        verbose: boolean, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        if app is None:
            (backend, org, repo, relative) = getLocation()
            if any(s is None for s in (backend, org, repo, relative)):
                console(
                    &#34;Not working in a repo: &#34;
                    f&#34;backend={backend} org={org} repo={repo} relative={relative}&#34;
                )
                self.good = False
                return

            app = use(
                f&#34;{org}/{repo}{relative}:clone&#34;,
                checkout=&#34;clone&#34;,
                backend=backend,
                silent=DEEP,
            )

        self.app = app
        repoDir = app.repoLocation
        txtDir = f&#34;{repoDir}/_temp/txt&#34;
        self.txtDir = txtDir
        self.tokenFile = f&#34;{txtDir}/tokens.tsv&#34;
        self.sentenceFile = f&#34;{txtDir}/sentences.tsv&#34;
        self.textPath = f&#34;{repoDir}/_temp/txt/plain.txt&#34;

    @staticmethod
    def help():
        &#34;&#34;&#34;Print a help text to the console.

        In order to give help on the command line, here is a pre-baked help text.
        Only the name of the conversion script needs to be merged in.
        &#34;&#34;&#34;

        console(
            &#34;&#34;&#34;

        Add NLP-generated features to a TF dataset.

        There are also commands to perform this operation step by step.

        addnlp [tasks/flags] [--help]

        --help: show this text and exit

        tasks:
            a sequence of tasks:
            plaintext:
                make a plain text for the NLP tools
            lingo:
                run the NLP tool on the plain text
            ingest:
                ingest the results of the NLP tool in the dataset
            all:
                all tasks

        flags:
            -write, +write:
                whether to write the generated files with plain text and node
                positions to disk
            +verbose, ++verbose:
                Produce more progress and reporting messages
        &#34;&#34;&#34;
        )

    def getElementInfo(self, verbose=None):
        &#34;&#34;&#34;Analyse the schema.

        The XML schema has useful information about the XML elements that
        occur in the source. Here we extract that information and make it
        fast-accessible.

        Parameters
        ----------
        verbose: boolean, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.

        Returns
        -------
        dict
            Keyed by element name (without namespaces), where the value
            for each name is a tuple of booleans: whether the element is simple
            or complex; whether the element allows mixed content or only pure content.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        self.elementDefs = {}

        A = Analysis(verbose=verbose)
        A.configure()
        A.interpret()
        if not A.good:
            console(&#34;Could not get TEI element definitions&#34;)
            return

        elementDefs = {name: (typ, mixed) for (name, typ, mixed) in A.getDefs()}
        self.mixedTypes = {x for (x, (typ, mixed)) in elementDefs.items() if mixed}

    def generatePlain(self, takeAways={&#34;note&#34;, &#34;orig&#34;, &#34;del&#34;}, empty=&#34;empty&#34;):
        &#34;&#34;&#34;Generates a plain text out of a data source.

        The text is generatad in such a way that out of flow elements are collected
        and put at the end. Examples of such elements are notes.
        Leaving them at their original positions will interfere with sentence detection.

        We separate the flows clearly in the output, so that they are discernible
        in the output of the NLP pipeline.

        Parameters
        ----------
        takeAways: set, optional `{&#34;note&#34;, &#34;reg&#34;}`
            A set of node types whose content will be put in separate text flows at
            the end of the document.
        empty: string, optional empty
            Name of feature that identifies the empty slots.

        Returns
        -------
        tuple
            The result is a tuple consisting of

            *   *text*: the generated text
            *   *positions*: a list of nodes such that list item *i* contains
                the original slot that corresponds to the character *i* in the
                generated text (counting from zero).
        &#34;&#34;&#34;
        verbose = self.verbose
        write = self.write
        app = self.app
        info = app.info
        indent = app.indent
        api = app.api
        F = api.F
        Fs = api.Fs
        N = api.N

        sectionTypes = {&#34;file&#34;, &#34;folder&#34;, &#34;chapter&#34;, &#34;chunk&#34;, &#34;word&#34;}
        ignoreTypes = {&#34;word&#34;}
        sentenceBreakRe = re.compile(r&#34;[.!?]&#34;)

        info(&#34;Generating a plain text with positions ...&#34;, force=verbose &gt;= 0)
        self.getElementInfo()
        mixedTypes = self.mixedTypes

        flows = {elem: [] for elem in takeAways}
        flows[&#34;&#34;] = []
        flowStack = [&#34;&#34;]

        nTypeStack = []

        def finishSentence(flowContent):
            nContent = len(flowContent)
            lnw = None  # last non white position
            for i in range(nContent - 1, -1, -1):
                item = flowContent[i]
                if type(item) is not str or item.strip() == &#34;&#34;:
                    continue
                else:
                    lnw = i
                    break

            if lnw is None:
                return

            # note that every slot appears in the sequence preceded by a neg int
            # and followed by a pos int
            # Material outside slots may be followed and preceded by other strings
            # We have to make sure that what we add, falls outside any slot.
            # We do that by inspecting the following item:
            # if that is a positive int, we are in a slot so we have to insert material
            # after that int
            # If the following item is a string or a negative int,
            # so we can insert right after the point where we are.
            if not sentenceBreakRe.match(flowContent[lnw]):
                offset = 1
                if lnw &lt; nContent - 1:
                    following = flowContent[lnw + 1]
                    if type(following) is int and following &gt; 0:
                        offset = 2
                flowContent.insert(lnw + offset, &#34;.&#34;)
                lnw += 1

            if not any(ch == &#34;\n&#34; for ch in flowContent[lnw + 1 :]):
                flowContent.append(&#34;\n&#34;)

        emptySlots = 0

        for (n, kind) in N.walk(events=True):
            nType = F.otype.v(n)

            if nType in ignoreTypes:
                continue

            isSeparateType = nType in takeAways

            if kind is None:  # slot type
                if Fs(empty).v(n):
                    emptySlots += 1
                    ch = &#34;￮&#34;
                else:
                    ch = F.ch.v(n)
                flows[flowStack[-1]].extend([-n, ch, n])

            elif kind:  # end node
                if isSeparateType:
                    flow = flowStack.pop()
                else:
                    flow = flowStack[-1]
                flowContent = flows[flow]

                if flow:
                    finishSentence(flowContent)
                else:
                    if nType == &#34;teiHeader&#34;:
                        finishSentence(flowContent)
                        flowContent.append(&#34; xxx.\nEND META.\n\n&#34;)
                    elif nType in sectionTypes:
                        flowContent.append(f&#34; xxx.\nEND {nType}.\n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            finishSentence(flowContent)
                nTypeStack.pop()

            else:  # start node
                nTypeStack.append(nType)

                if isSeparateType:
                    flowStack.append(nType)
                flow = flowStack[-1]
                flowContent = flows[flow]

                if isSeparateType:
                    flowContent.append(f&#34;\nITEM {flow}.\n&#34;)
                else:
                    if nType == &#34;teiHeader&#34;:
                        flowContent.append(&#34;BEGIN META.\n\n&#34;)
                    elif nType in sectionTypes:
                        flowContent.append(f&#34;BEGIN {nType}.\n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            flowContent.append(f&#34;{nType}. &#34;)

        indent(level=True)
        info(f&#34;Found {emptySlots} empty slots&#34;, tm=False, force=verbose &gt;= 0)

        rec = Recorder(app.api)

        for flow in sorted(flows):
            items = flows[flow]

            if len(items) == 0:
                continue

            rec.add(f&#34;BEGIN FLOW {flow if flow else &#39;MAIN&#39;}.\n\n&#34;)

            for item in items:
                if type(item) is int:
                    if item &lt; 0:
                        rec.start(-item)
                    else:
                        rec.end(item)
                else:
                    rec.add(item)

            rec.add(f&#34; xxx.\nEND FLOW {flow if flow else &#39;MAIN&#39;}.\n\n&#34;)

            info(
                (
                    f&#34;recorded flow {flow if flow else &#39;MAIN&#39;:&lt;10} &#34;
                    f&#34;with {len(items):&gt;6} items&#34;
                ),
                tm=False,
                force=verbose &gt;= 0,
            )

        indent(level=False)

        if write:
            textPath = self.textPath
            rec.write(textPath)
            info(
                f&#34;Done. Generated text and positions written to {textPath}&#34;,
                force=verbose &gt;= 0,
            )
        else:
            info(&#34;Done&#34;, force=verbose &gt;= 0)

        return (rec.text(), rec.positions(simple=True))

    @staticmethod
    def lingo(*args, **kwargs):
        return tokensAndSentences(*args, **kwargs)

    def ingest(
        self,
        isToken,
        positions,
        stream,
        slotFeature,
        tp,
        features,
        nFeature=None,
        skipBlanks=False,
        skipFlows=None,
        empty=None,
    ):
        &#34;&#34;&#34;Ingests a stream of NLP data and transforms it into nodes and features.

        The data is a stream of values associated with a spans of text.

        For each span a node will be created of the given type, and a feature
        of the given name will assign a value to that span.
        The value assigned is by default the value that is present in the data stream,
        but it is possible to specify a method to change the value.

        !!! caution
            The plain text on which the NLP pipeline has run may not correspond
            exactly with the text as defined by the corpus.
            When the plain text was generated, some extra convenience material
            may have been inserted.
            Items in the stream that refer to these pieces of text will be ignored.

            When items refer partly to proper corpus text and partly to convenience text,
            they will be narrowed down to the proper text.

        !!! caution
            The plain text may exhibit another order of material than the proper corpus
            text. For example, notes may have been collected and moved out of the
            main text flow to the end of the text.

            That means that if an item specifies a span in the plain text, it may
            not refer to a single span in the proper text, but to various spans.

            We take care to map all spans in the generated plain text back to *sets*
            of slots in the proper text.

        Parameters
        ----------
        isToken: boolean
            Whether the data specifies tokens or something else.
            Tokens are special because they are intended to become the new slot type.
        positions: list
            which slot node corresponds to which position in the plain text.

        stream: list of tuple
            The tuples should consist of

            *   *start*: a start number (char pos in the plain text, starting at `0`)
            *   *end*: an end number (char pos in tghe plain text plus one)
            *   *value*: a value for feature assignment

        tp: string
            The type of the nodes that will be generated.

        features: tuple
            The names of the features that will be generated.

        nFeature: string, optional None
            If not None, the name of a feature that will hold the sequence number of
            the element in the data stream, starting at 1.

        slotFeature: string, optional None
            The feature by which we can retrieve the character value of a slot

        skipBlanks: boolean, optional False
            If True, rows whose text component is only white space will be skipped.

        skipFlows: set
            set of elements whose resulting data in the stream should be ignored

        empty: string, optional empty
            Name of feature that identifies the empty slots.

        Returns
        -------
        tuple
            We deliver the following pieces of information in a tuple:

            * the last node
            * the mapping of the new nodes to the slots they occupy;
            * the data of the new feature.
        &#34;&#34;&#34;
        verbose = self.verbose
        app = self.app
        info = app.info
        indent = app.indent
        F = app.api.F
        Fs = app.api.Fs
        Fotypev = F.otype.v
        slotType = F.otype.slotType
        Fslotv = Fs(slotFeature).v
        if empty is not None:
            Femptyv = Fs(empty).v

        doN = nFeature is not None
        slotLinks = {}
        featuresData = {feat: {} for feat in features}
        if nFeature is not None:
            featuresData[nFeature] = {}
        if empty is not None:
            featuresData[empty] = {}

        if isToken:
            featToken = featuresData[features[0]]
            featAfter = featuresData[features[1]]

        whiteMultipleRe = re.compile(r&#34;^[ \n]{2,}$&#34;, re.S)

        node = 0
        itemsOutside = []
        itemsEmpty = []

        info(
            f&#34;generating {tp}-nodes with features {&#39;, &#39;.join(featuresData)}&#34;,
            force=verbose &gt;= 0,
        )
        indent(level=True)

        def addToken():
            nonlocal node
            nonlocal curSlots
            nonlocal curValue

            node += 1
            slotLinks[node] = curSlots
            featToken[node] = curValue
            for (feat, val) in zip(features[2:], vals[2:]):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

            curSlots = []
            curValue = &#34;&#34;

        def addSlot(slot):
            nonlocal node

            node += 1
            slotLinks[node] = [slot]
            featToken[node] = Fslotv(slot)
            if Femptyv(slot):
                featuresData[empty][node] = 1

        def addItem():
            nonlocal node

            node += 1
            slotLinks[node] = mySlots
            for (feat, val) in zip(features, vals):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

        # First add all empty slots, provided we are doing tokens

        if isToken:
            emptySlots = (
                {s for s in Fs(empty).s(1) if Fotypev(s) == slotType}
                if empty
                else set()
            )
            emptyWithinToken = 0
            spaceWithinToken = 0

            for slot in sorted(emptySlots):
                addSlot(slot)

        # now the data from the NLP pipeline

        flowBeginRe = re.compile(r&#34;BEGIN FLOW (\w+)&#34;)
        flowEndRe = re.compile(r&#34; xxx.\nEND FLOW (\w+)&#34;)

        skipping = False
        flow = None

        for (i, (b, e, *vals)) in enumerate(stream):
            if skipFlows is not None:
                text = vals[0]
                if skipping:
                    match = flowEndRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = False
                        flow = None
                        continue
                else:
                    match = flowBeginRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = flow in skipFlows
                        continue

            if skipping:
                continue

            mySlots = set()

            for j in range(b, e):
                s = positions[j]
                if s is not None:
                    mySlots.add(s)

            if len(mySlots) == 0:
                if doN:
                    vals.append(i + 1)
                itemsOutside.append((i, b, e, *vals))
                continue

            if skipBlanks and len(vals):
                slotsOrdered = sorted(mySlots)
                nSlots = len(slotsOrdered)

                start = min(
                    (
                        i
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=nSlots,
                )
                end = max(
                    (
                        i + 1
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=0,
                )

                if end &lt;= start:
                    itemsEmpty.append((i, b, e, *vals))
                    continue

                mySlots = slotsOrdered[start:end]
            else:
                mySlots = sorted(mySlots)

            curValue = &#34;&#34;
            curSlots = []

            nMySlots = len(mySlots)

            if isToken:
                # we might need to split tokens:
                # at points that correspond to empty slots
                # at spaces or newlines within the token
                # decompose it into individual characters

                tokenText = &#34;&#34;.join(Fslotv(s) for s in mySlots)

                if whiteMultipleRe.match(tokenText):
                    spaceWithinToken += 1
                    for slot in mySlots:
                        addSlot(slot)
                        spaceWithinToken += 1

                else:
                    for (i, slot) in enumerate(mySlots):
                        last = i == nMySlots - 1
                        if slot in emptySlots:
                            emptyWithinToken += 1
                            if curValue:
                                addToken()
                            if last:
                                featAfter[node] = vals[1]
                        else:
                            curValue += Fslotv(slot)
                            curSlots.append(slot)
                    if curValue:
                        addToken()
                        featAfter[node] = vals[1]
            else:
                addItem()

        repFeatures = &#34;, &#34;.join(features + ((nFeature,) if doN else ()))
        info(
            f&#34;{node} {tp} nodes have values assigned for {repFeatures}&#34;,
            force=verbose &gt;= 0,
        )
        if isToken:
            info(
                f&#34;{emptyWithinToken} empty slots have split surrounding tokens&#34;,
                force=verbose &gt;= 0,
            )
            info(
                f&#34;{spaceWithinToken} space slots have split into {slotType}s&#34;,
                force=verbose &gt;= 0,
            )

        tasks = [(&#34;Items contained in extra generated text&#34;, itemsOutside)]
        if skipBlanks:
            tasks.append((&#34;Items with empty final text&#34;, itemsEmpty))

        for (label, items) in tasks:
            nItems = len(items)
            info(f&#34;{nItems:&gt;5}x {label}&#34;, force=verbose &gt;= 0)
            indent(level=True)
            for (i, b, e, *vals) in items[0:5]:
                info(
                    f&#34;\t{i} span {b}-{e}: {&#39;, &#39;.join(str(v) for v in vals)}&#34;,
                    force=verbose == 1,
                )
            indent(level=False)

        indent(level=False)
        return (node, slotLinks, featuresData)

    def ingestTokensAndSentences(
        self,
        positions,
        tokenStream,
        sentenceStream,
        empty=&#34;empty&#34;,
        tokenType=&#34;token&#34;,
        tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None),
        removeSlotFeatures=(&#34;ch&#34;,),
        sentenceType=&#34;sentence&#34;,
        sentenceFeatures=(&#34;nsent&#34;,),
        sentenceSkipFlows={&#34;orig&#34;, &#34;del&#34;},
    ):
        &#34;&#34;&#34;Ingests a tokens and sentences in a dataset and turn the tokens into slots.

        By default:

        * tokens become nodes of a new type `token`;
        * the texts of a token ends up in the feature `str`;
        * if there is a space after a token, it ends up in the feature `after`;
        * sentences become nodes of a new type `sentence`;
        * the sentence number ends up in the feature `nsent`.
        * tokens become the new slots.

        But this function can also be adapted to token and sentence streams that
        have additional names and values, see below.

        The streams of tokens and sentences may contain more fields.
        In the parameters `tokenFeatures` and `sentenceFeatures` you may pass the
        feature names for the data in those fields.

        When the streams are read, for each feature name in the `tokenFeatures`
        (resp. `sentenceFeatures`) the corresponding field in the stream will be
        read, and the value found there will be assigned to that feature.

        If there are more fields in the stream than there are declared in the
        `tokenFeatures` (resp. `sentenceFeatures`) parameter, these extra fields will
        be ignored.

        The last feature name in these parameters is special.
        If it is None, it will be ignored.
        Otherwise, an extra feature with that name will be created, and it will be
        filled with the node numbers of the newly generated nodes.

        !!! hint &#34;Look at the defaults&#34;
            The default `tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None)` specifies that two
            fields from the tokenstream will be read, and those values will be assigned
            to features `str` and `after`.
            There will be no field with the node itself in it.

            The default `sentenceFeatures=(&#34;nsent&#34;,)` specifies that no field from the
            tokenstream will be read, but that there will be a feature `nsent` that
            has the node of each sentence as value.

        We have to ignore the sentence boundaries in some flows,
        e.g. the material coming from `&lt;orig&gt;` and `&lt;del&gt;` elements.
        However, in the flow coming from the `&lt;note&gt;` elements, we want to retain the
        sentence boundaries.

        Parameters
        ----------
        tokenStream: list
            The list of tokens as delivered by the NLP pipe.
        sentenceStream: list
            The list of sentences as delivered by the NLP pipe.
        empty: string, optional empty
            Names of features that identify the empty slots.
            Name of feature that identifies the empty slots.
        tokenType: string, optional str
            The node type for the tokens
        tokenFeatures: tuple, optional (&#34;str&#34;, &#34;after&#34;, &#34;&#34;)
            The names of the features that the token stream contains.
            There must be at least two features:
            the first one should give the token content, the second one the non-token
            material until the next token.
            The rest are additional features that the
            pipeline might supply.
        removeSlotFeatures: tuple, optional (&#34;ch&#34;,)
            The names of features defined on original slots that do not have to be
            carried over to the new slots of type token.
            There should be at least one feature: the character content of the slot.
        sentenceType: string, optional str
            The node type for the sentences
        sentenceFeatures: tuple, optional (&#34;nsent&#34;,)
            The names of the features that the sentence stream contains.
        sentenceSkipFlows: set
            The elements whose flows in the sentence stream should be ignored

        Returns
        -------
        string
            The new version number of the data that contains the tokens and sentences.
        &#34;&#34;&#34;
        app = self.app
        info = app.info
        indent = app.indent
        verbose = self.verbose
        silent = &#34;auto&#34; if verbose == 1 else TERSE if verbose == 0 else DEEP

        info(&#34;Ingesting tokens and sentences into the dataset ...&#34;, force=verbose &gt;= 0)
        indent(level=True)
        info(&#34;Mapping NLP data to nodes and features ...&#34;, force=verbose &gt;= 0)
        indent(level=True)

        slotFeature = removeSlotFeatures[0]

        slotLinks = {tokenType: {}, sentenceType: {}}
        features = {}
        for feat in tokenFeatures:
            if feat is not None:
                features[feat] = {}
        for feat in sentenceFeatures:
            if feat is not None:
                features[feat] = {}
        lastNode = {tokenType: 0, sentenceType: 0}

        i = 0
        for (isToken, data, skipFlows, tp, feats, skipBlanks, thisEmpty) in (
            (True, tokenStream, None, tokenType, tokenFeatures, False, empty),
            (
                False,
                sentenceStream,
                sentenceSkipFlows,
                sentenceType,
                sentenceFeatures,
                True,
                None,
            ),
        ):
            i += 1
            realFeats = feats[0:-1]
            nFeat = feats[-1]
            (node, theseSlotLinks, featuresData) = self.ingest(
                isToken,
                positions,
                data,
                slotFeature,
                tp,
                realFeats,
                nFeature=nFeat,
                skipBlanks=skipBlanks,
                skipFlows=skipFlows,
                empty=thisEmpty,
            )
            lastNode[tp] = node
            slotLinks[tp] = theseSlotLinks
            for (feat, featData) in featuresData.items():
                features[feat] = featData
            info(f&#34;{lastNode[tp]} {tp}s&#34;, force=verbose &gt;= 0)

        indent(level=False)

        info(&#34;Make a modified dataset ...&#34;, force=verbose &gt;= 0)

        repoDir = app.repoLocation
        versionPre = app.version
        version = versionPre.removesuffix(&#34;pre&#34;)
        origTf = f&#34;{repoDir}/tf/{versionPre}&#34;
        newTf = f&#34;{repoDir}/tf/{version}&#34;
        initTree(newTf, fresh=True, gentle=False)

        modify(
            origTf,
            newTf,
            addTypes=dict(
                token=dict(
                    nodeFrom=1,
                    nodeTo=lastNode[tokenType],
                    nodeSlots=slotLinks[tokenType],
                    nodeFeatures={
                        feat: features[feat]
                        for feat in tokenFeatures
                        if feat is not None
                    },
                ),
                sentence=dict(
                    nodeFrom=1,
                    nodeTo=lastNode[sentenceType],
                    nodeSlots=slotLinks[sentenceType],
                    nodeFeatures={
                        feat: features[feat]
                        for feat in sentenceFeatures
                        if feat is not None
                    },
                ),
            ),
            deleteTypes=(&#34;word&#34;,),
            featureMeta=dict(
                nsent=dict(
                    valueType=&#34;int&#34;,
                    description=&#34;number of sentence in corpus&#34;,
                ),
                otext={
                    &#34;fmt:text-orig-full&#34;: &#34;{&#34;
                    + tokenFeatures[0]
                    + &#34;}{&#34;
                    + tokenFeatures[1]
                    + &#34;}&#34;
                },
            ),
            replaceSlotType=(tokenType, *removeSlotFeatures),
            silent=silent,
        )
        info(&#34;Done&#34;, force=verbose &gt;= 0)
        indent(level=False)
        info(f&#34;Enriched data is available in version {version}&#34;, force=verbose &gt;= 0)
        info(
            &#34;You may need to adapt this TF app and its documentation:&#34;,
            tm=False,
            force=verbose &gt;= 0,
        )
        info(&#34;please run: python tfFromTei.py apptoken&#34;, tm=False, force=verbose &gt;= 0)
        return version

    def task(
        self,
        plaintext=False,
        lingo=False,
        ingest=False,
        write=False,
        verbose=-1,
        **kwargs,
    ):
        &#34;&#34;&#34;Carry out any task, possibly modified by any flag.

        This is a higher level function that can execute a selection of tasks.

        The tasks will be executed in a fixed order: plaintext, lingo, ingest.
        But you can select which one(s) must be executed.

        If multiple tasks must be executed and one fails, the subsequent tasks
        will not be executed.

        Parameters
        ----------
        plaintext: boolean, optional False
            Whether to generate the plain text and position files.
        lingo: boolean, optional False
            Whether to carry out NLP pipeline (Spacy).
        ingest: boolean, optional False
            Whether to ingest the NLP results into the dataset..
        verbose: integer, optional -1
            Produce no (-1), some (0) or many (1) orprogress and reporting messages
        write: boolean, optional False
            Whether to write the generated plain text and position files to disk.
        kwargs: dict
            remaining arguments that can serve as input for the task

        Returns
        -------
        boolean
            Whether all tasks have executed successfully.
        &#34;&#34;&#34;

        self.write = write
        self.verbose = verbose
        silent = TERSE if verbose == 1 else DEEP

        self.loadApp()
        if not self.good:
            return False

        app = self.app
        app.setSilent(silent)

        txtDir = self.txtDir
        textPath = self.textPath
        tokenFile = self.tokenFile
        sentenceFile = self.sentenceFile

        app.indent(reset=True)

        text = kwargs.get(&#34;text&#34;, None)
        positions = kwargs.get(&#34;positions&#34;, None)
        tokens = kwargs.get(&#34;tokens&#34;, None)
        sentences = kwargs.get(&#34;sentences&#34;, None)

        result = False

        if plaintext and self.good:
            (text, positions) = self.generatePlain()
            result = (text, positions) if self.good else False

        if lingo and self.good:
            app.info(&#34;Using NLP pipeline Spacy (may take a while)...&#34;, force=True)
            if text is None or positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                text = rec.text()
                positions = rec.positions()

            (tokens, sentences) = self.lingo(text)
            if write:
                if not dirExists(txtDir):
                    dirMake(txtDir)
                writeList(tokens, tokenFile, intCols=(True, True, False, False))
                writeList(sentences, sentenceFile, intCols=(True, True, False))
            app.info(&#34;NLP done&#34;, force=True)

            result = (tokens, sentences) if self.good else False

        if ingest and self.good:
            if positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                positions = rec.positions(simple=True)

            if tokens is None or sentences is None:
                tokens = readList(tokenFile)
                sentences = readList(sentenceFile)
            newVersion = self.ingestTokensAndSentences(positions, tokens, sentences)

            result = newVersion if self.good else False

        return result

    def run(self):
        &#34;&#34;&#34;Carry out tasks specified by arguments on the command line.

        This function inspects arguments, and runs the specified tasks,
        with the specified flags enabled.

        Returns
        -------
        data | boolean
            Data or True if the command was successful, False if not.
        &#34;&#34;&#34;
        possibleTasks = {&#34;all&#34;, &#34;plaintext&#34;, &#34;lingo&#34;, &#34;ingest&#34;}
        possibleFlags = {
            &#34;-write&#34;: False,
            &#34;+write&#34;: True,
            &#34;-verbose&#34;: -1,
            &#34;+verbose&#34;: 0,
            &#34;++verbose&#34;: 1,
        }
        possibleArgs = possibleTasks | set(possibleFlags)

        args = set(sys.argv[1:])

        if not len(args) or &#34;--help&#34; in args or &#34;-h&#34; in args:
            self.help()
            console(&#34;No task specified&#34;)
            return True

        illegalArgs = {arg for arg in args if arg not in possibleArgs}

        if len(illegalArgs):
            self.help()
            for arg in illegalArgs:
                console(f&#34;Illegal argument `{arg}`&#34;)
            return False

        tasks = {arg: True for arg in possibleTasks if arg in args}
        flags = {
            arg.lstrip(&#34;+-&#34;): val for (arg, val) in possibleFlags.items() if arg in args
        }

        if &#34;all&#34; in tasks:
            if tasks[&#34;all&#34;]:
                tasks = {arg: True for arg in possibleTasks if arg != &#34;all&#34;}
            else:
                del tasks[&#34;all&#34;]

        result = self.task(**tasks, **flags)

        if type(result) is bool:
            if not result:
                return False
            else:
                return True

        return result</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="tf.convert.addnlp.NLPipeline.help"><code class="name flex">
<span>def <span class="ident">help</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a help text to the console.</p>
<p>In order to give help on the command line, here is a pre-baked help text.
Only the name of the conversion script needs to be merged in.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L123-L160" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def help():
    &#34;&#34;&#34;Print a help text to the console.

    In order to give help on the command line, here is a pre-baked help text.
    Only the name of the conversion script needs to be merged in.
    &#34;&#34;&#34;

    console(
        &#34;&#34;&#34;

    Add NLP-generated features to a TF dataset.

    There are also commands to perform this operation step by step.

    addnlp [tasks/flags] [--help]

    --help: show this text and exit

    tasks:
        a sequence of tasks:
        plaintext:
            make a plain text for the NLP tools
        lingo:
            run the NLP tool on the plain text
        ingest:
            ingest the results of the NLP tool in the dataset
        all:
            all tasks

    flags:
        -write, +write:
            whether to write the generated files with plain text and node
            positions to disk
        +verbose, ++verbose:
            Produce more progress and reporting messages
    &#34;&#34;&#34;
    )</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.lingo"><code class="name flex">
<span>def <span class="ident">lingo</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L393-L395" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def lingo(*args, **kwargs):
    return tokensAndSentences(*args, **kwargs)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="tf.convert.addnlp.NLPipeline.generatePlain"><code class="name flex">
<span>def <span class="ident">generatePlain</span></span>(<span>self, takeAways={'del', 'orig', 'note'}, empty='empty')</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a plain text out of a data source.</p>
<p>The text is generatad in such a way that out of flow elements are collected
and put at the end. Examples of such elements are notes.
Leaving them at their original positions will interfere with sentence detection.</p>
<p>We separate the flows clearly in the output, so that they are discernible
in the output of the NLP pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>takeAways</code></strong> :&ensp;<code>set</code>, optional <code>{"note", "reg"}</code></dt>
<dd>A set of node types whose content will be put in separate text flows at
the end of the document.</dd>
<dt><strong><code>empty</code></strong> :&ensp;<code>string</code>, optional <code>empty</code></dt>
<dd>Name of feature that identifies the empty slots.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>
<p>The result is a tuple consisting of</p>
<ul>
<li><em>text</em>: the generated text</li>
<li><em>positions</em>: a list of nodes such that list item <em>i</em> contains
the original slot that corresponds to the character <em>i</em> in the
generated text (counting from zero).</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L198-L391" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def generatePlain(self, takeAways={&#34;note&#34;, &#34;orig&#34;, &#34;del&#34;}, empty=&#34;empty&#34;):
    &#34;&#34;&#34;Generates a plain text out of a data source.

    The text is generatad in such a way that out of flow elements are collected
    and put at the end. Examples of such elements are notes.
    Leaving them at their original positions will interfere with sentence detection.

    We separate the flows clearly in the output, so that they are discernible
    in the output of the NLP pipeline.

    Parameters
    ----------
    takeAways: set, optional `{&#34;note&#34;, &#34;reg&#34;}`
        A set of node types whose content will be put in separate text flows at
        the end of the document.
    empty: string, optional empty
        Name of feature that identifies the empty slots.

    Returns
    -------
    tuple
        The result is a tuple consisting of

        *   *text*: the generated text
        *   *positions*: a list of nodes such that list item *i* contains
            the original slot that corresponds to the character *i* in the
            generated text (counting from zero).
    &#34;&#34;&#34;
    verbose = self.verbose
    write = self.write
    app = self.app
    info = app.info
    indent = app.indent
    api = app.api
    F = api.F
    Fs = api.Fs
    N = api.N

    sectionTypes = {&#34;file&#34;, &#34;folder&#34;, &#34;chapter&#34;, &#34;chunk&#34;, &#34;word&#34;}
    ignoreTypes = {&#34;word&#34;}
    sentenceBreakRe = re.compile(r&#34;[.!?]&#34;)

    info(&#34;Generating a plain text with positions ...&#34;, force=verbose &gt;= 0)
    self.getElementInfo()
    mixedTypes = self.mixedTypes

    flows = {elem: [] for elem in takeAways}
    flows[&#34;&#34;] = []
    flowStack = [&#34;&#34;]

    nTypeStack = []

    def finishSentence(flowContent):
        nContent = len(flowContent)
        lnw = None  # last non white position
        for i in range(nContent - 1, -1, -1):
            item = flowContent[i]
            if type(item) is not str or item.strip() == &#34;&#34;:
                continue
            else:
                lnw = i
                break

        if lnw is None:
            return

        # note that every slot appears in the sequence preceded by a neg int
        # and followed by a pos int
        # Material outside slots may be followed and preceded by other strings
        # We have to make sure that what we add, falls outside any slot.
        # We do that by inspecting the following item:
        # if that is a positive int, we are in a slot so we have to insert material
        # after that int
        # If the following item is a string or a negative int,
        # so we can insert right after the point where we are.
        if not sentenceBreakRe.match(flowContent[lnw]):
            offset = 1
            if lnw &lt; nContent - 1:
                following = flowContent[lnw + 1]
                if type(following) is int and following &gt; 0:
                    offset = 2
            flowContent.insert(lnw + offset, &#34;.&#34;)
            lnw += 1

        if not any(ch == &#34;\n&#34; for ch in flowContent[lnw + 1 :]):
            flowContent.append(&#34;\n&#34;)

    emptySlots = 0

    for (n, kind) in N.walk(events=True):
        nType = F.otype.v(n)

        if nType in ignoreTypes:
            continue

        isSeparateType = nType in takeAways

        if kind is None:  # slot type
            if Fs(empty).v(n):
                emptySlots += 1
                ch = &#34;￮&#34;
            else:
                ch = F.ch.v(n)
            flows[flowStack[-1]].extend([-n, ch, n])

        elif kind:  # end node
            if isSeparateType:
                flow = flowStack.pop()
            else:
                flow = flowStack[-1]
            flowContent = flows[flow]

            if flow:
                finishSentence(flowContent)
            else:
                if nType == &#34;teiHeader&#34;:
                    finishSentence(flowContent)
                    flowContent.append(&#34; xxx.\nEND META.\n\n&#34;)
                elif nType in sectionTypes:
                    flowContent.append(f&#34; xxx.\nEND {nType}.\n\n&#34;)
                else:
                    if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                        nTp in mixedTypes for nTp in nTypeStack[0:-1]
                    ):
                        finishSentence(flowContent)
            nTypeStack.pop()

        else:  # start node
            nTypeStack.append(nType)

            if isSeparateType:
                flowStack.append(nType)
            flow = flowStack[-1]
            flowContent = flows[flow]

            if isSeparateType:
                flowContent.append(f&#34;\nITEM {flow}.\n&#34;)
            else:
                if nType == &#34;teiHeader&#34;:
                    flowContent.append(&#34;BEGIN META.\n\n&#34;)
                elif nType in sectionTypes:
                    flowContent.append(f&#34;BEGIN {nType}.\n\n&#34;)
                else:
                    if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                        nTp in mixedTypes for nTp in nTypeStack[0:-1]
                    ):
                        flowContent.append(f&#34;{nType}. &#34;)

    indent(level=True)
    info(f&#34;Found {emptySlots} empty slots&#34;, tm=False, force=verbose &gt;= 0)

    rec = Recorder(app.api)

    for flow in sorted(flows):
        items = flows[flow]

        if len(items) == 0:
            continue

        rec.add(f&#34;BEGIN FLOW {flow if flow else &#39;MAIN&#39;}.\n\n&#34;)

        for item in items:
            if type(item) is int:
                if item &lt; 0:
                    rec.start(-item)
                else:
                    rec.end(item)
            else:
                rec.add(item)

        rec.add(f&#34; xxx.\nEND FLOW {flow if flow else &#39;MAIN&#39;}.\n\n&#34;)

        info(
            (
                f&#34;recorded flow {flow if flow else &#39;MAIN&#39;:&lt;10} &#34;
                f&#34;with {len(items):&gt;6} items&#34;
            ),
            tm=False,
            force=verbose &gt;= 0,
        )

    indent(level=False)

    if write:
        textPath = self.textPath
        rec.write(textPath)
        info(
            f&#34;Done. Generated text and positions written to {textPath}&#34;,
            force=verbose &gt;= 0,
        )
    else:
        info(&#34;Done&#34;, force=verbose &gt;= 0)

    return (rec.text(), rec.positions(simple=True))</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.getElementInfo"><code class="name flex">
<span>def <span class="ident">getElementInfo</span></span>(<span>self, verbose=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyse the schema.</p>
<p>The XML schema has useful information about the XML elements that
occur in the source. Here we extract that information and make it
fast-accessible.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code>, optional <code>None</code></dt>
<dd>Produce more progress and reporting messages
If not passed, take the verbose member of this object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Keyed by element name (without namespaces), where the value
for each name is a tuple of booleans: whether the element is simple
or complex; whether the element allows mixed content or only pure content.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L162-L196" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def getElementInfo(self, verbose=None):
    &#34;&#34;&#34;Analyse the schema.

    The XML schema has useful information about the XML elements that
    occur in the source. Here we extract that information and make it
    fast-accessible.

    Parameters
    ----------
    verbose: boolean, optional None
        Produce more progress and reporting messages
        If not passed, take the verbose member of this object.

    Returns
    -------
    dict
        Keyed by element name (without namespaces), where the value
        for each name is a tuple of booleans: whether the element is simple
        or complex; whether the element allows mixed content or only pure content.
    &#34;&#34;&#34;
    if verbose is not None:
        self.verbose = verbose
    verbose = self.verbose

    self.elementDefs = {}

    A = Analysis(verbose=verbose)
    A.configure()
    A.interpret()
    if not A.good:
        console(&#34;Could not get TEI element definitions&#34;)
        return

    elementDefs = {name: (typ, mixed) for (name, typ, mixed) in A.getDefs()}
    self.mixedTypes = {x for (x, (typ, mixed)) in elementDefs.items() if mixed}</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.ingest"><code class="name flex">
<span>def <span class="ident">ingest</span></span>(<span>self, isToken, positions, stream, slotFeature, tp, features, nFeature=None, skipBlanks=False, skipFlows=None, empty=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Ingests a stream of NLP data and transforms it into nodes and features.</p>
<p>The data is a stream of values associated with a spans of text.</p>
<p>For each span a node will be created of the given type, and a feature
of the given name will assign a value to that span.
The value assigned is by default the value that is present in the data stream,
but it is possible to specify a method to change the value.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The plain text on which the NLP pipeline has run may not correspond
exactly with the text as defined by the corpus.
When the plain text was generated, some extra convenience material
may have been inserted.
Items in the stream that refer to these pieces of text will be ignored.</p>
<p>When items refer partly to proper corpus text and partly to convenience text,
they will be narrowed down to the proper text.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The plain text may exhibit another order of material than the proper corpus
text. For example, notes may have been collected and moved out of the
main text flow to the end of the text.</p>
<p>That means that if an item specifies a span in the plain text, it may
not refer to a single span in the proper text, but to various spans.</p>
<p>We take care to map all spans in the generated plain text back to <em>sets</em>
of slots in the proper text.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>isToken</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Whether the data specifies tokens or something else.
Tokens are special because they are intended to become the new slot type.</dd>
<dt><strong><code>positions</code></strong> :&ensp;<code>list</code></dt>
<dd>which slot node corresponds to which position in the plain text.</dd>
<dt><strong><code>stream</code></strong> :&ensp;<code>list</code> of <code>tuple</code></dt>
<dd>
<p>The tuples should consist of</p>
<ul>
<li><em>start</em>: a start number (char pos in the plain text, starting at <code>0</code>)</li>
<li><em>end</em>: an end number (char pos in tghe plain text plus one)</li>
<li><em>value</em>: a value for feature assignment</li>
</ul>
</dd>
<dt><strong><code>tp</code></strong> :&ensp;<code>string</code></dt>
<dd>The type of the nodes that will be generated.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The names of the features that will be generated.</dd>
<dt><strong><code>nFeature</code></strong> :&ensp;<code>string</code>, optional <code>None</code></dt>
<dd>If not None, the name of a feature that will hold the sequence number of
the element in the data stream, starting at 1.</dd>
<dt><strong><code>slotFeature</code></strong> :&ensp;<code>string</code>, optional <code>None</code></dt>
<dd>The feature by which we can retrieve the character value of a slot</dd>
<dt><strong><code>skipBlanks</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>If True, rows whose text component is only white space will be skipped.</dd>
<dt><strong><code>skipFlows</code></strong> :&ensp;<code>set</code></dt>
<dd>set of elements whose resulting data in the stream should be ignored</dd>
<dt><strong><code>empty</code></strong> :&ensp;<code>string</code>, optional <code>empty</code></dt>
<dd>Name of feature that identifies the empty slots.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>
<p>We deliver the following pieces of information in a tuple:</p>
<ul>
<li>the last node</li>
<li>the mapping of the new nodes to the slots they occupy;</li>
<li>the data of the new feature.</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L397-L709" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def ingest(
    self,
    isToken,
    positions,
    stream,
    slotFeature,
    tp,
    features,
    nFeature=None,
    skipBlanks=False,
    skipFlows=None,
    empty=None,
):
    &#34;&#34;&#34;Ingests a stream of NLP data and transforms it into nodes and features.

    The data is a stream of values associated with a spans of text.

    For each span a node will be created of the given type, and a feature
    of the given name will assign a value to that span.
    The value assigned is by default the value that is present in the data stream,
    but it is possible to specify a method to change the value.

    !!! caution
        The plain text on which the NLP pipeline has run may not correspond
        exactly with the text as defined by the corpus.
        When the plain text was generated, some extra convenience material
        may have been inserted.
        Items in the stream that refer to these pieces of text will be ignored.

        When items refer partly to proper corpus text and partly to convenience text,
        they will be narrowed down to the proper text.

    !!! caution
        The plain text may exhibit another order of material than the proper corpus
        text. For example, notes may have been collected and moved out of the
        main text flow to the end of the text.

        That means that if an item specifies a span in the plain text, it may
        not refer to a single span in the proper text, but to various spans.

        We take care to map all spans in the generated plain text back to *sets*
        of slots in the proper text.

    Parameters
    ----------
    isToken: boolean
        Whether the data specifies tokens or something else.
        Tokens are special because they are intended to become the new slot type.
    positions: list
        which slot node corresponds to which position in the plain text.

    stream: list of tuple
        The tuples should consist of

        *   *start*: a start number (char pos in the plain text, starting at `0`)
        *   *end*: an end number (char pos in tghe plain text plus one)
        *   *value*: a value for feature assignment

    tp: string
        The type of the nodes that will be generated.

    features: tuple
        The names of the features that will be generated.

    nFeature: string, optional None
        If not None, the name of a feature that will hold the sequence number of
        the element in the data stream, starting at 1.

    slotFeature: string, optional None
        The feature by which we can retrieve the character value of a slot

    skipBlanks: boolean, optional False
        If True, rows whose text component is only white space will be skipped.

    skipFlows: set
        set of elements whose resulting data in the stream should be ignored

    empty: string, optional empty
        Name of feature that identifies the empty slots.

    Returns
    -------
    tuple
        We deliver the following pieces of information in a tuple:

        * the last node
        * the mapping of the new nodes to the slots they occupy;
        * the data of the new feature.
    &#34;&#34;&#34;
    verbose = self.verbose
    app = self.app
    info = app.info
    indent = app.indent
    F = app.api.F
    Fs = app.api.Fs
    Fotypev = F.otype.v
    slotType = F.otype.slotType
    Fslotv = Fs(slotFeature).v
    if empty is not None:
        Femptyv = Fs(empty).v

    doN = nFeature is not None
    slotLinks = {}
    featuresData = {feat: {} for feat in features}
    if nFeature is not None:
        featuresData[nFeature] = {}
    if empty is not None:
        featuresData[empty] = {}

    if isToken:
        featToken = featuresData[features[0]]
        featAfter = featuresData[features[1]]

    whiteMultipleRe = re.compile(r&#34;^[ \n]{2,}$&#34;, re.S)

    node = 0
    itemsOutside = []
    itemsEmpty = []

    info(
        f&#34;generating {tp}-nodes with features {&#39;, &#39;.join(featuresData)}&#34;,
        force=verbose &gt;= 0,
    )
    indent(level=True)

    def addToken():
        nonlocal node
        nonlocal curSlots
        nonlocal curValue

        node += 1
        slotLinks[node] = curSlots
        featToken[node] = curValue
        for (feat, val) in zip(features[2:], vals[2:]):
            featuresData[feat][node] = val
        if doN:
            featuresData[nFeature][node] = node

        curSlots = []
        curValue = &#34;&#34;

    def addSlot(slot):
        nonlocal node

        node += 1
        slotLinks[node] = [slot]
        featToken[node] = Fslotv(slot)
        if Femptyv(slot):
            featuresData[empty][node] = 1

    def addItem():
        nonlocal node

        node += 1
        slotLinks[node] = mySlots
        for (feat, val) in zip(features, vals):
            featuresData[feat][node] = val
        if doN:
            featuresData[nFeature][node] = node

    # First add all empty slots, provided we are doing tokens

    if isToken:
        emptySlots = (
            {s for s in Fs(empty).s(1) if Fotypev(s) == slotType}
            if empty
            else set()
        )
        emptyWithinToken = 0
        spaceWithinToken = 0

        for slot in sorted(emptySlots):
            addSlot(slot)

    # now the data from the NLP pipeline

    flowBeginRe = re.compile(r&#34;BEGIN FLOW (\w+)&#34;)
    flowEndRe = re.compile(r&#34; xxx.\nEND FLOW (\w+)&#34;)

    skipping = False
    flow = None

    for (i, (b, e, *vals)) in enumerate(stream):
        if skipFlows is not None:
            text = vals[0]
            if skipping:
                match = flowEndRe.match(text)
                if match:
                    flow = match.group(1)
                    skipping = False
                    flow = None
                    continue
            else:
                match = flowBeginRe.match(text)
                if match:
                    flow = match.group(1)
                    skipping = flow in skipFlows
                    continue

        if skipping:
            continue

        mySlots = set()

        for j in range(b, e):
            s = positions[j]
            if s is not None:
                mySlots.add(s)

        if len(mySlots) == 0:
            if doN:
                vals.append(i + 1)
            itemsOutside.append((i, b, e, *vals))
            continue

        if skipBlanks and len(vals):
            slotsOrdered = sorted(mySlots)
            nSlots = len(slotsOrdered)

            start = min(
                (
                    i
                    for (i, s) in enumerate(slotsOrdered)
                    if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                ),
                default=nSlots,
            )
            end = max(
                (
                    i + 1
                    for (i, s) in enumerate(slotsOrdered)
                    if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                ),
                default=0,
            )

            if end &lt;= start:
                itemsEmpty.append((i, b, e, *vals))
                continue

            mySlots = slotsOrdered[start:end]
        else:
            mySlots = sorted(mySlots)

        curValue = &#34;&#34;
        curSlots = []

        nMySlots = len(mySlots)

        if isToken:
            # we might need to split tokens:
            # at points that correspond to empty slots
            # at spaces or newlines within the token
            # decompose it into individual characters

            tokenText = &#34;&#34;.join(Fslotv(s) for s in mySlots)

            if whiteMultipleRe.match(tokenText):
                spaceWithinToken += 1
                for slot in mySlots:
                    addSlot(slot)
                    spaceWithinToken += 1

            else:
                for (i, slot) in enumerate(mySlots):
                    last = i == nMySlots - 1
                    if slot in emptySlots:
                        emptyWithinToken += 1
                        if curValue:
                            addToken()
                        if last:
                            featAfter[node] = vals[1]
                    else:
                        curValue += Fslotv(slot)
                        curSlots.append(slot)
                if curValue:
                    addToken()
                    featAfter[node] = vals[1]
        else:
            addItem()

    repFeatures = &#34;, &#34;.join(features + ((nFeature,) if doN else ()))
    info(
        f&#34;{node} {tp} nodes have values assigned for {repFeatures}&#34;,
        force=verbose &gt;= 0,
    )
    if isToken:
        info(
            f&#34;{emptyWithinToken} empty slots have split surrounding tokens&#34;,
            force=verbose &gt;= 0,
        )
        info(
            f&#34;{spaceWithinToken} space slots have split into {slotType}s&#34;,
            force=verbose &gt;= 0,
        )

    tasks = [(&#34;Items contained in extra generated text&#34;, itemsOutside)]
    if skipBlanks:
        tasks.append((&#34;Items with empty final text&#34;, itemsEmpty))

    for (label, items) in tasks:
        nItems = len(items)
        info(f&#34;{nItems:&gt;5}x {label}&#34;, force=verbose &gt;= 0)
        indent(level=True)
        for (i, b, e, *vals) in items[0:5]:
            info(
                f&#34;\t{i} span {b}-{e}: {&#39;, &#39;.join(str(v) for v in vals)}&#34;,
                force=verbose == 1,
            )
        indent(level=False)

    indent(level=False)
    return (node, slotLinks, featuresData)</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.ingestTokensAndSentences"><code class="name flex">
<span>def <span class="ident">ingestTokensAndSentences</span></span>(<span>self, positions, tokenStream, sentenceStream, empty='empty', tokenType='token', tokenFeatures=('str', 'after', None), removeSlotFeatures=('ch',), sentenceType='sentence', sentenceFeatures=('nsent',), sentenceSkipFlows={'del', 'orig'})</span>
</code></dt>
<dd>
<div class="desc"><p>Ingests a tokens and sentences in a dataset and turn the tokens into slots.</p>
<p>By default:</p>
<ul>
<li>tokens become nodes of a new type <code>token</code>;</li>
<li>the texts of a token ends up in the feature <code>str</code>;</li>
<li>if there is a space after a token, it ends up in the feature <code>after</code>;</li>
<li>sentences become nodes of a new type <code>sentence</code>;</li>
<li>the sentence number ends up in the feature <code>nsent</code>.</li>
<li>tokens become the new slots.</li>
</ul>
<p>But this function can also be adapted to token and sentence streams that
have additional names and values, see below.</p>
<p>The streams of tokens and sentences may contain more fields.
In the parameters <code>tokenFeatures</code> and <code>sentenceFeatures</code> you may pass the
feature names for the data in those fields.</p>
<p>When the streams are read, for each feature name in the <code>tokenFeatures</code>
(resp. <code>sentenceFeatures</code>) the corresponding field in the stream will be
read, and the value found there will be assigned to that feature.</p>
<p>If there are more fields in the stream than there are declared in the
<code>tokenFeatures</code> (resp. <code>sentenceFeatures</code>) parameter, these extra fields will
be ignored.</p>
<p>The last feature name in these parameters is special.
If it is None, it will be ignored.
Otherwise, an extra feature with that name will be created, and it will be
filled with the node numbers of the newly generated nodes.</p>
<div class="admonition hint">
<p class="admonition-title">Look at the defaults</p>
<p>The default <code>tokenFeatures=("str", "after", None)</code> specifies that two
fields from the tokenstream will be read, and those values will be assigned
to features <code>str</code> and <code>after</code>.
There will be no field with the node itself in it.</p>
<p>The default <code>sentenceFeatures=("nsent",)</code> specifies that no field from the
tokenstream will be read, but that there will be a feature <code>nsent</code> that
has the node of each sentence as value.</p>
</div>
<p>We have to ignore the sentence boundaries in some flows,
e.g. the material coming from <code>&lt;orig&gt;</code> and <code>&lt;del&gt;</code> elements.
However, in the flow coming from the <code>&lt;note&gt;</code> elements, we want to retain the
sentence boundaries.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tokenStream</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of tokens as delivered by the NLP pipe.</dd>
<dt><strong><code>sentenceStream</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of sentences as delivered by the NLP pipe.</dd>
<dt><strong><code>empty</code></strong> :&ensp;<code>string</code>, optional <code>empty</code></dt>
<dd>Names of features that identify the empty slots.
Name of feature that identifies the empty slots.</dd>
<dt><strong><code>tokenType</code></strong> :&ensp;<code>string</code>, optional <code>str</code></dt>
<dd>The node type for the tokens</dd>
<dt><strong><code>tokenFeatures</code></strong> :&ensp;<code>tuple</code>, optional <code>("str", "after", "")</code></dt>
<dd>The names of the features that the token stream contains.
There must be at least two features:
the first one should give the token content, the second one the non-token
material until the next token.
The rest are additional features that the
pipeline might supply.</dd>
<dt><strong><code>removeSlotFeatures</code></strong> :&ensp;<code>tuple</code>, optional <code>("ch",)</code></dt>
<dd>The names of features defined on original slots that do not have to be
carried over to the new slots of type token.
There should be at least one feature: the character content of the slot.</dd>
<dt><strong><code>sentenceType</code></strong> :&ensp;<code>string</code>, optional <code>str</code></dt>
<dd>The node type for the sentences</dd>
<dt><strong><code>sentenceFeatures</code></strong> :&ensp;<code>tuple</code>, optional <code>("nsent",)</code></dt>
<dd>The names of the features that the sentence stream contains.</dd>
<dt><strong><code>sentenceSkipFlows</code></strong> :&ensp;<code>set</code></dt>
<dd>The elements whose flows in the sentence stream should be ignored</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>string</code></dt>
<dd>The new version number of the data that contains the tokens and sentences.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L711-L923" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def ingestTokensAndSentences(
    self,
    positions,
    tokenStream,
    sentenceStream,
    empty=&#34;empty&#34;,
    tokenType=&#34;token&#34;,
    tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None),
    removeSlotFeatures=(&#34;ch&#34;,),
    sentenceType=&#34;sentence&#34;,
    sentenceFeatures=(&#34;nsent&#34;,),
    sentenceSkipFlows={&#34;orig&#34;, &#34;del&#34;},
):
    &#34;&#34;&#34;Ingests a tokens and sentences in a dataset and turn the tokens into slots.

    By default:

    * tokens become nodes of a new type `token`;
    * the texts of a token ends up in the feature `str`;
    * if there is a space after a token, it ends up in the feature `after`;
    * sentences become nodes of a new type `sentence`;
    * the sentence number ends up in the feature `nsent`.
    * tokens become the new slots.

    But this function can also be adapted to token and sentence streams that
    have additional names and values, see below.

    The streams of tokens and sentences may contain more fields.
    In the parameters `tokenFeatures` and `sentenceFeatures` you may pass the
    feature names for the data in those fields.

    When the streams are read, for each feature name in the `tokenFeatures`
    (resp. `sentenceFeatures`) the corresponding field in the stream will be
    read, and the value found there will be assigned to that feature.

    If there are more fields in the stream than there are declared in the
    `tokenFeatures` (resp. `sentenceFeatures`) parameter, these extra fields will
    be ignored.

    The last feature name in these parameters is special.
    If it is None, it will be ignored.
    Otherwise, an extra feature with that name will be created, and it will be
    filled with the node numbers of the newly generated nodes.

    !!! hint &#34;Look at the defaults&#34;
        The default `tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None)` specifies that two
        fields from the tokenstream will be read, and those values will be assigned
        to features `str` and `after`.
        There will be no field with the node itself in it.

        The default `sentenceFeatures=(&#34;nsent&#34;,)` specifies that no field from the
        tokenstream will be read, but that there will be a feature `nsent` that
        has the node of each sentence as value.

    We have to ignore the sentence boundaries in some flows,
    e.g. the material coming from `&lt;orig&gt;` and `&lt;del&gt;` elements.
    However, in the flow coming from the `&lt;note&gt;` elements, we want to retain the
    sentence boundaries.

    Parameters
    ----------
    tokenStream: list
        The list of tokens as delivered by the NLP pipe.
    sentenceStream: list
        The list of sentences as delivered by the NLP pipe.
    empty: string, optional empty
        Names of features that identify the empty slots.
        Name of feature that identifies the empty slots.
    tokenType: string, optional str
        The node type for the tokens
    tokenFeatures: tuple, optional (&#34;str&#34;, &#34;after&#34;, &#34;&#34;)
        The names of the features that the token stream contains.
        There must be at least two features:
        the first one should give the token content, the second one the non-token
        material until the next token.
        The rest are additional features that the
        pipeline might supply.
    removeSlotFeatures: tuple, optional (&#34;ch&#34;,)
        The names of features defined on original slots that do not have to be
        carried over to the new slots of type token.
        There should be at least one feature: the character content of the slot.
    sentenceType: string, optional str
        The node type for the sentences
    sentenceFeatures: tuple, optional (&#34;nsent&#34;,)
        The names of the features that the sentence stream contains.
    sentenceSkipFlows: set
        The elements whose flows in the sentence stream should be ignored

    Returns
    -------
    string
        The new version number of the data that contains the tokens and sentences.
    &#34;&#34;&#34;
    app = self.app
    info = app.info
    indent = app.indent
    verbose = self.verbose
    silent = &#34;auto&#34; if verbose == 1 else TERSE if verbose == 0 else DEEP

    info(&#34;Ingesting tokens and sentences into the dataset ...&#34;, force=verbose &gt;= 0)
    indent(level=True)
    info(&#34;Mapping NLP data to nodes and features ...&#34;, force=verbose &gt;= 0)
    indent(level=True)

    slotFeature = removeSlotFeatures[0]

    slotLinks = {tokenType: {}, sentenceType: {}}
    features = {}
    for feat in tokenFeatures:
        if feat is not None:
            features[feat] = {}
    for feat in sentenceFeatures:
        if feat is not None:
            features[feat] = {}
    lastNode = {tokenType: 0, sentenceType: 0}

    i = 0
    for (isToken, data, skipFlows, tp, feats, skipBlanks, thisEmpty) in (
        (True, tokenStream, None, tokenType, tokenFeatures, False, empty),
        (
            False,
            sentenceStream,
            sentenceSkipFlows,
            sentenceType,
            sentenceFeatures,
            True,
            None,
        ),
    ):
        i += 1
        realFeats = feats[0:-1]
        nFeat = feats[-1]
        (node, theseSlotLinks, featuresData) = self.ingest(
            isToken,
            positions,
            data,
            slotFeature,
            tp,
            realFeats,
            nFeature=nFeat,
            skipBlanks=skipBlanks,
            skipFlows=skipFlows,
            empty=thisEmpty,
        )
        lastNode[tp] = node
        slotLinks[tp] = theseSlotLinks
        for (feat, featData) in featuresData.items():
            features[feat] = featData
        info(f&#34;{lastNode[tp]} {tp}s&#34;, force=verbose &gt;= 0)

    indent(level=False)

    info(&#34;Make a modified dataset ...&#34;, force=verbose &gt;= 0)

    repoDir = app.repoLocation
    versionPre = app.version
    version = versionPre.removesuffix(&#34;pre&#34;)
    origTf = f&#34;{repoDir}/tf/{versionPre}&#34;
    newTf = f&#34;{repoDir}/tf/{version}&#34;
    initTree(newTf, fresh=True, gentle=False)

    modify(
        origTf,
        newTf,
        addTypes=dict(
            token=dict(
                nodeFrom=1,
                nodeTo=lastNode[tokenType],
                nodeSlots=slotLinks[tokenType],
                nodeFeatures={
                    feat: features[feat]
                    for feat in tokenFeatures
                    if feat is not None
                },
            ),
            sentence=dict(
                nodeFrom=1,
                nodeTo=lastNode[sentenceType],
                nodeSlots=slotLinks[sentenceType],
                nodeFeatures={
                    feat: features[feat]
                    for feat in sentenceFeatures
                    if feat is not None
                },
            ),
        ),
        deleteTypes=(&#34;word&#34;,),
        featureMeta=dict(
            nsent=dict(
                valueType=&#34;int&#34;,
                description=&#34;number of sentence in corpus&#34;,
            ),
            otext={
                &#34;fmt:text-orig-full&#34;: &#34;{&#34;
                + tokenFeatures[0]
                + &#34;}{&#34;
                + tokenFeatures[1]
                + &#34;}&#34;
            },
        ),
        replaceSlotType=(tokenType, *removeSlotFeatures),
        silent=silent,
    )
    info(&#34;Done&#34;, force=verbose &gt;= 0)
    indent(level=False)
    info(f&#34;Enriched data is available in version {version}&#34;, force=verbose &gt;= 0)
    info(
        &#34;You may need to adapt this TF app and its documentation:&#34;,
        tm=False,
        force=verbose &gt;= 0,
    )
    info(&#34;please run: python tfFromTei.py apptoken&#34;, tm=False, force=verbose &gt;= 0)
    return version</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.loadApp"><code class="name flex">
<span>def <span class="ident">loadApp</span></span>(<span>self, app=None, verbose=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a given TF app or loads the TF app based on the working directory.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>app</code></strong> :&ensp;<code>object</code>, optional <code>None</code></dt>
<dd>
<p>The handle to the original TF dataset, already loaded.
We assume that the original data resides in the current
version, which has the string <code>pre</code> appended to it,
e.g. in version <code>1.3pre</code>.
We create a new version of the dataset, with the same number,
but without the <code>pre</code>.</p>
<p>If not given, we load the TF app that is nearby in the file system.</p>
</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code>, optional <code>None</code></dt>
<dd>Produce more progress and reporting messages
If not passed, take the verbose member of this object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L76-L121" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def loadApp(self, app=None, verbose=None):
    &#34;&#34;&#34;Loads a given TF app or loads the TF app based on the working directory.

    Parameters
    ----------
    app: object, optional None
        The handle to the original TF dataset, already loaded.
        We assume that the original data resides in the current
        version, which has the string `pre` appended to it,
        e.g. in version `1.3pre`.
        We create a new version of the dataset, with the same number,
        but without the `pre`.

        If not given, we load the TF app that is nearby in the file system.
    verbose: boolean, optional None
        Produce more progress and reporting messages
        If not passed, take the verbose member of this object.
    &#34;&#34;&#34;
    if verbose is not None:
        self.verbose = verbose
    verbose = self.verbose

    if app is None:
        (backend, org, repo, relative) = getLocation()
        if any(s is None for s in (backend, org, repo, relative)):
            console(
                &#34;Not working in a repo: &#34;
                f&#34;backend={backend} org={org} repo={repo} relative={relative}&#34;
            )
            self.good = False
            return

        app = use(
            f&#34;{org}/{repo}{relative}:clone&#34;,
            checkout=&#34;clone&#34;,
            backend=backend,
            silent=DEEP,
        )

    self.app = app
    repoDir = app.repoLocation
    txtDir = f&#34;{repoDir}/_temp/txt&#34;
    self.txtDir = txtDir
    self.tokenFile = f&#34;{txtDir}/tokens.tsv&#34;
    self.sentenceFile = f&#34;{txtDir}/sentences.tsv&#34;
    self.textPath = f&#34;{repoDir}/_temp/txt/plain.txt&#34;</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Carry out tasks specified by arguments on the command line.</p>
<p>This function inspects arguments, and runs the specified tasks,
with the specified flags enabled.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>data | boolean</code></dt>
<dd>Data or True if the command was successful, False if not.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L1027-L1082" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;Carry out tasks specified by arguments on the command line.

    This function inspects arguments, and runs the specified tasks,
    with the specified flags enabled.

    Returns
    -------
    data | boolean
        Data or True if the command was successful, False if not.
    &#34;&#34;&#34;
    possibleTasks = {&#34;all&#34;, &#34;plaintext&#34;, &#34;lingo&#34;, &#34;ingest&#34;}
    possibleFlags = {
        &#34;-write&#34;: False,
        &#34;+write&#34;: True,
        &#34;-verbose&#34;: -1,
        &#34;+verbose&#34;: 0,
        &#34;++verbose&#34;: 1,
    }
    possibleArgs = possibleTasks | set(possibleFlags)

    args = set(sys.argv[1:])

    if not len(args) or &#34;--help&#34; in args or &#34;-h&#34; in args:
        self.help()
        console(&#34;No task specified&#34;)
        return True

    illegalArgs = {arg for arg in args if arg not in possibleArgs}

    if len(illegalArgs):
        self.help()
        for arg in illegalArgs:
            console(f&#34;Illegal argument `{arg}`&#34;)
        return False

    tasks = {arg: True for arg in possibleTasks if arg in args}
    flags = {
        arg.lstrip(&#34;+-&#34;): val for (arg, val) in possibleFlags.items() if arg in args
    }

    if &#34;all&#34; in tasks:
        if tasks[&#34;all&#34;]:
            tasks = {arg: True for arg in possibleTasks if arg != &#34;all&#34;}
        else:
            del tasks[&#34;all&#34;]

    result = self.task(**tasks, **flags)

    if type(result) is bool:
        if not result:
            return False
        else:
            return True

    return result</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.task"><code class="name flex">
<span>def <span class="ident">task</span></span>(<span>self, plaintext=False, lingo=False, ingest=False, write=False, verbose=-1, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Carry out any task, possibly modified by any flag.</p>
<p>This is a higher level function that can execute a selection of tasks.</p>
<p>The tasks will be executed in a fixed order: plaintext, lingo, ingest.
But you can select which one(s) must be executed.</p>
<p>If multiple tasks must be executed and one fails, the subsequent tasks
will not be executed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>plaintext</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to generate the plain text and position files.</dd>
<dt><strong><code>lingo</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to carry out NLP pipeline (Spacy).</dd>
<dt><strong><code>ingest</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to ingest the NLP results into the dataset..</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>integer</code>, optional <code>-1</code></dt>
<dd>Produce no (-1), some (0) or many (1) orprogress and reporting messages</dd>
<dt><strong><code>write</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to write the generated plain text and position files to disk.</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>remaining arguments that can serve as input for the task</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all tasks have executed successfully.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/d8e405eaff85b7d1f61b7f352eda49fd0aa50fe9/tf/convert/addnlp.py#L925-L1025" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def task(
    self,
    plaintext=False,
    lingo=False,
    ingest=False,
    write=False,
    verbose=-1,
    **kwargs,
):
    &#34;&#34;&#34;Carry out any task, possibly modified by any flag.

    This is a higher level function that can execute a selection of tasks.

    The tasks will be executed in a fixed order: plaintext, lingo, ingest.
    But you can select which one(s) must be executed.

    If multiple tasks must be executed and one fails, the subsequent tasks
    will not be executed.

    Parameters
    ----------
    plaintext: boolean, optional False
        Whether to generate the plain text and position files.
    lingo: boolean, optional False
        Whether to carry out NLP pipeline (Spacy).
    ingest: boolean, optional False
        Whether to ingest the NLP results into the dataset..
    verbose: integer, optional -1
        Produce no (-1), some (0) or many (1) orprogress and reporting messages
    write: boolean, optional False
        Whether to write the generated plain text and position files to disk.
    kwargs: dict
        remaining arguments that can serve as input for the task

    Returns
    -------
    boolean
        Whether all tasks have executed successfully.
    &#34;&#34;&#34;

    self.write = write
    self.verbose = verbose
    silent = TERSE if verbose == 1 else DEEP

    self.loadApp()
    if not self.good:
        return False

    app = self.app
    app.setSilent(silent)

    txtDir = self.txtDir
    textPath = self.textPath
    tokenFile = self.tokenFile
    sentenceFile = self.sentenceFile

    app.indent(reset=True)

    text = kwargs.get(&#34;text&#34;, None)
    positions = kwargs.get(&#34;positions&#34;, None)
    tokens = kwargs.get(&#34;tokens&#34;, None)
    sentences = kwargs.get(&#34;sentences&#34;, None)

    result = False

    if plaintext and self.good:
        (text, positions) = self.generatePlain()
        result = (text, positions) if self.good else False

    if lingo and self.good:
        app.info(&#34;Using NLP pipeline Spacy (may take a while)...&#34;, force=True)
        if text is None or positions is None:
            rec = Recorder(app.api)
            rec.read(textPath)
            text = rec.text()
            positions = rec.positions()

        (tokens, sentences) = self.lingo(text)
        if write:
            if not dirExists(txtDir):
                dirMake(txtDir)
            writeList(tokens, tokenFile, intCols=(True, True, False, False))
            writeList(sentences, sentenceFile, intCols=(True, True, False))
        app.info(&#34;NLP done&#34;, force=True)

        result = (tokens, sentences) if self.good else False

    if ingest and self.good:
        if positions is None:
            rec = Recorder(app.api)
            rec.read(textPath)
            positions = rec.positions(simple=True)

        if tokens is None or sentences is None:
            tokens = readList(tokenFile)
            sentences = readList(sentenceFile)
        newVersion = self.ingestTokensAndSentences(positions, tokens, sentences)

        result = newVersion if self.good else False

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<p><a href="https://github.com/annotation" title="annotation on GitHub"><img src="../../tf/images/tf-small.png" alt="annotation"></a></p>
<p><a href="../../tf/index.html">tf home</a> -
<a href="../../tf/cheatsheet.html">cheat sheet</a> -
<a href="https://github.com/annotation/text-fabric" title="GitHub repo"><img src="../../tf/images/GitHub_Logo.png" alt="GitHub" width="50"></a></p>
</p>
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#requirements">Requirements</a></li>
<li><a href="#effect">Effect</a></li>
<li><a href="#homework">Homework</a></li>
<li><a href="#examples">Examples</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tf.convert" href="index.html">tf.convert</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tf.convert.addnlp.main" href="#tf.convert.addnlp.main">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tf.convert.addnlp.NLPipeline" href="#tf.convert.addnlp.NLPipeline">NLPipeline</a></code></h4>
<ul class="">
<li><code><a title="tf.convert.addnlp.NLPipeline.generatePlain" href="#tf.convert.addnlp.NLPipeline.generatePlain">generatePlain</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.getElementInfo" href="#tf.convert.addnlp.NLPipeline.getElementInfo">getElementInfo</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.help" href="#tf.convert.addnlp.NLPipeline.help">help</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.ingest" href="#tf.convert.addnlp.NLPipeline.ingest">ingest</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.ingestTokensAndSentences" href="#tf.convert.addnlp.NLPipeline.ingestTokensAndSentences">ingestTokensAndSentences</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.lingo" href="#tf.convert.addnlp.NLPipeline.lingo">lingo</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.loadApp" href="#tf.convert.addnlp.NLPipeline.loadApp">loadApp</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.run" href="#tf.convert.addnlp.NLPipeline.run">run</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.task" href="#tf.convert.addnlp.NLPipeline.task">task</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://pure.knaw.nl/portal/en/persons/dirk-roorda">Dirk Roorda</a>
<a href="https://huc.knaw.nl"><img alt="HuC" src="../../tf/images/huc.png" width="200" alt="Humanities Cluster"></a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>