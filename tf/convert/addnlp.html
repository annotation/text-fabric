<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tf.convert.addnlp API documentation</title>
<meta name="description" content="Add data from an NLP pipeline …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tf.convert.addnlp</code></h1>
</header>
<section id="section-intro">
<p>Add data from an NLP pipeline.</p>
<p>When you have used <code><a title="tf.convert.tei" href="tei.html">tf.convert.tei</a></code> to convert a TEI data source into a TF dataset,
the situation with words and sentences is usually not satisfactory.
In most TEI sources, words and sentences are not explicitly marked up, and it is
really hard to build token detection and sentence boundary detection into the
conversion program.</p>
<p>There is a better way.
You can use this module to have tokens and sentences detected by NLP pipelines
(currently only Spacy is supported).
These tokens and sentences will then be transformed to nodes and attributes
and inserted in the TF dataset as a new version.</p>
<p>The original slots in the TF dataset (characters) will be discarded, because the
new tokens will be used as slots.</p>
<p><strong>This is work in progress. Details of the workflow may change rather often!</strong></p>
<h2 id="requirements">Requirements</h2>
<ul>
<li>The initial data set should be one that has characters as slots.</li>
<li>The version of the initial data should end with the string <code>pre</code>, e.g.
<code>0.8pre</code>.</li>
</ul>
<h2 id="effect">Effect</h2>
<ul>
<li>
<p>A new version of the data (whose label is the old version minus the <code>pre</code>)
will be produced:</p>
<ul>
<li>with new node types <code>sentence</code> and <code>token</code>;</li>
<li>with <code>token</code> as slot type;</li>
<li>with the old slot type removed;</li>
<li>with the feature that contains the text of the slots removed;</li>
<li>with other slot features translated to equally named features on <code>token</code>;</li>
<li>with other node and edge features translated faithfully to the new situation.</li>
</ul>
</li>
</ul>
<h2 id="homework">Homework</h2>
<ul>
<li>
<p>The new data needs a slightly different TF app than the original version.
You can generate that with the program that created the TF from the TEI,
typically</p>
<p><code>sh
python tfFromTei.py apptoken</code></p>
</li>
</ul>
<h1 id="usage">Usage</h1>
<h2 id="commandline">Commandline</h2>
<pre><code class="language-sh">tf-addnlp tasks params flags
</code></pre>
<h2 id="from-python">From Python</h2>
<pre><code class="language-python">from tf.convert.addnlp import NLPipeline
from tf.app import use

ORG = &quot;yourOrg&quot;
REPO = &quot;yourRepo&quot;

Apre = use(f&quot;{ORG}/{REPO}:clone&quot;, checkout=&quot;clone&quot;)

NLP = NLPipeline(**params, **flags)
NLP.loadApp(Apre)
NLP.task(**tasks, **flags)
</code></pre>
<p>For the tasks, parameters and flags, see
<code><a title="tf.convert.addnlp.TASKS" href="#tf.convert.addnlp.TASKS">TASKS</a></code>, <code><a title="tf.convert.addnlp.PARAMS" href="#tf.convert.addnlp.PARAMS">PARAMS</a></code>, and <code><a title="tf.convert.addnlp.FLAGS" href="#tf.convert.addnlp.FLAGS">FLAGS</a></code> and expand the code.</p>
<p>The parameters have defaults that are exactly suited to corpora that have been
converted from TEI by <code><a title="tf.convert.tei" href="tei.html">tf.convert.tei</a></code>.</p>
<h2 id="examples">Examples</h2>
<p>Exactly how you can call the methods of this module is demonstrated in the small
corpus of 14 letter by the Dutch artist Piet Mondriaan.</p>
<ul>
<li><a href="https://nbviewer.org/github/annotation/mondriaan/blob/master/programs/convertExpress.ipynb">Mondriaan</a>.</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L1-L1191" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;Add data from an NLP pipeline.

When you have used `tf.convert.tei` to convert a TEI data source into a TF dataset,
the situation with words and sentences is usually not satisfactory.
In most TEI sources, words and sentences are not explicitly marked up, and it is
really hard to build token detection and sentence boundary detection into the
conversion program.

There is a better way.
You can use this module to have tokens and sentences detected by NLP pipelines
(currently only Spacy is supported).
These tokens and sentences will then be transformed to nodes and attributes
and inserted in the TF dataset as a new version.

The original slots in the TF dataset (characters) will be discarded, because the
new tokens will be used as slots.

**This is work in progress. Details of the workflow may change rather often!**

## Requirements

*   The initial data set should be one that has characters as slots.
*   The version of the initial data should end with the string `pre`, e.g.
    `0.8pre`.

## Effect

*   A new version of the data (whose label is the old version minus the `pre`)
    will be produced:

    *   with new node types `sentence` and `token`;
    *   with `token` as slot type;
    *   with the old slot type removed;
    *   with the feature that contains the text of the slots removed;
    *   with other slot features translated to equally named features on `token`;
    *   with other node and edge features translated faithfully to the new situation.

## Homework

*   The new data needs a slightly different TF app than the original version.
    You can generate that with the program that created the TF from the TEI,
    typically

    ``` sh
    python tfFromTei.py apptoken
    ```

# Usage

## Commandline

```sh
tf-addnlp tasks params flags
```

## From Python

```python
from tf.convert.addnlp import NLPipeline
from tf.app import use

ORG = &#34;yourOrg&#34;
REPO = &#34;yourRepo&#34;

Apre = use(f&#34;{ORG}/{REPO}:clone&#34;, checkout=&#34;clone&#34;)

NLP = NLPipeline(**params, **flags)
NLP.loadApp(Apre)
NLP.task(**tasks, **flags)
```

For the tasks, parameters and flags, see
`TASKS`, `PARAMS`, and `FLAGS` and expand the code.

The parameters have defaults that are exactly suited to corpora that have been
converted from TEI by `tf.convert.tei`.

## Examples

Exactly how you can call the methods of this module is demonstrated in the small
corpus of 14 letter by the Dutch artist Piet Mondriaan.

*   [Mondriaan](https://nbviewer.org/github/annotation/mondriaan/blob/master/programs/convertExpress.ipynb).
&#34;&#34;&#34;

import sys
import re

from .recorder import Recorder
from ..advanced.app import loadApp
from ..tools.xmlschema import Analysis
from ..tools.myspacy import tokensAndSentences
from ..dataset import modify
from ..core.helpers import console
from ..core.files import initTree, dirMake, dirExists
from ..core.timestamp import DEEP, TERSE
from ..core.command import readArgs
from ..lib import writeList, readList


HELP = &#34;Add NLP-generated features to a TF dataset.&#34;

TASKS = dict(
    plaintext=&#34;make a plain text for the NLP tools&#34;,
    lingo=&#34;run the NLP tool on the plain text&#34;,
    ingest=&#34;ingest the results of the NLP tool in the dataset&#34;,
    all=None,
)
&#34;&#34;&#34;Possible tasks.&#34;&#34;&#34;

PARAMS = dict(
    lang=(&#34;Set up the NLP tool for this language&#34;, &#34;en&#34;),
    slotFeature=(
        &#34;When generating text, use this feature to obtain text from slots&#34;,
        &#34;ch&#34;,
    ),
    removeSlotFeatures=(
        &#34;Discardable slot features. Will not be translated to token features&#34;,
        &#34;ch&#34;,
    ),
    emptyFeature=(&#34;Feature to identify empty slots.&#34;, &#34;empty&#34;),
    ignoreTypes=(
        &#34;Node types that will be ignored when generating the plain text.&#34;,
        &#34;word&#34;,
    ),
    outOfFlow=(
        &#34;These node types will be put in separate text flows in the plain text.&#34;,
        &#34;note,orig,del&#34;,
    ),
    tokenType=(&#34;The node type for the tokens.&#34;, &#34;token&#34;),
    tokenFeatures=(
        (
            &#34;The features in the token output by the NLP: &#34;
            &#34;1: token content; 2: space after the token (if any), ...&#34;
        ),
        &#34;str,after&#34;,
    ),
    sentenceBarriers=(&#34;Elements that trigger a senetence boundary.&#34;, &#34;div,p&#34;),
    sentenceSkipFlow=(&#34;The flows that are not fed to sentence detection.&#34;, &#34;orig,del&#34;),
    tokenNFeature=(&#34;The feature that will hold the sequence number of the token.&#34;, &#34;&#34;),
    sentenceType=(&#34;The node type for the sentences&#34;, &#34;sentence&#34;),
    sentenceFeatures=(&#34;&#34;, &#34;&#34;),
    sentenceNFeature=(&#34;The features in the sentence output by the NLP&#34;, &#34;&#34;),
)
&#34;&#34;&#34;Possible parameters.&#34;&#34;&#34;

FLAGS = dict(
    write=(
        (
            &#34;whether to write the generated files &#34;
            &#34;with plain text and node positions to disk&#34;
        ),
        False,
        2,
    ),
    verbose=(&#34;Produce less or more progress and reporting messages&#34;, -1, 3),
)
&#34;&#34;&#34;Possible flags.&#34;&#34;&#34;


class NLPipeline:
    def __init__(
        self,
        app=None,
        lang=PARAMS[&#34;lang&#34;][1],
        slotFeature=PARAMS[&#34;slotFeature&#34;][1],
        removeSlotFeatures=PARAMS[&#34;removeSlotFeatures&#34;][1],
        emptyFeature=PARAMS[&#34;emptyFeature&#34;][1],
        ignoreTypes=PARAMS[&#34;ignoreTypes&#34;][1],
        outOfFlow=PARAMS[&#34;outOfFlow&#34;][1],
        tokenType=PARAMS[&#34;tokenType&#34;][1],
        tokenFeatures=PARAMS[&#34;tokenFeatures&#34;][1],
        tokenNFeature=PARAMS[&#34;tokenNFeature&#34;][1],
        sentenceBarriers=PARAMS[&#34;sentenceBarriers&#34;][1],
        sentenceSkipFlow=PARAMS[&#34;sentenceSkipFlow&#34;][1],
        sentenceType=PARAMS[&#34;sentenceType&#34;][1],
        sentenceFeatures=PARAMS[&#34;sentenceFeatures&#34;][1],
        sentenceNFeature=PARAMS[&#34;sentenceNFeature&#34;][1],
        verbose=FLAGS[&#34;verbose&#34;][1],
        write=FLAGS[&#34;write&#34;][1],
    ):
        &#34;&#34;&#34;Enrich a TF dataset with annotations generated by an NLP pipeline.

        Parameters
        ----------
        lang: string, optional en
            The language for which the NLP tool will be set up
        app: object, None
            A loaded TF app. If None, the TF App that is nearby in the file system
            will be loaded.
            We assume that the original data resides in the current
            version, which has the string `pre` appended to it,
            e.g. in version `1.3pre`.
            We create a new version of the dataset, with the same number,
            but without the `pre`.
        slotFeature: string, optional ch
            The  feature on slots that provides the text of a slot to be included
            in the generated text.
        removeSlotFeatures: &#34;ch&#34;
            A tuple is distilled from comma-separated values.
            The names of features defined on original slots that do not have to be
            carried over to the new slots of type token.
            There should be at least one feature: the character content of the slot.
        emptyFeature: string, optional &#34;empty&#34;
            Name of feature that identifies the empty slots.
        ignoreTypes: set, optional &#34;word&#34;
            A set is distilled from comma-separated values.
            Node types that will be ignored when generating the plain text.
        outOfFlow: string, optional &#34;note,orig,del&#34;
            A set is distilled from comma-separated values.
            A set of node types whose content will be put in separate text flows at
            the end of the document.
        sentenceSkipFlow: string, optional &#34;orig,del&#34;
            A set is distilled from comma-separated values.
            The elements whose flows in the sentence stream should be ignored
        tokenType: string, optional token
            The node type for the tokens
        tokenFeatures: tuple, optional (&#34;str&#34;, &#34;after&#34;)
            A tuple is distilled from comma-separated values.
            The names of the features that the token stream contains.
            There must be at least two features:
            the first one should give the token content, the second one the non-token
            material until the next token.
            The rest are additional features that the
            pipeline might supply.
        tokenNFeature: string, optional None
            If not None, the name of the token feature that will hold the
            sequence number of the token in the data stream, starting at 1.
        sentenceType: string, optional sentence
            The node type for the sentences
        sentenceFeatures: tuple, optional ()
            A tuple is distilled from comma-separated values.
            The names of the features that the sentence stream contains.
        sentenceNFeature: string, optional nsent
            If not None, the name of the sentence feature that will hold the
            sequence number of the sentence in the data stream, starting at 1.

        &#34;&#34;&#34;

        def makeString(s):
            return None if not s else s

        def makeSet(s):
            return set() if not s else set(s.split(&#34;,&#34;))

        def makeTuple(s):
            return tuple() if not s else tuple(s.split(&#34;,&#34;))

        self.good = True
        self.app = app
        self.lang = makeString(lang)
        self.slotFeature = makeString(slotFeature)
        self.removeSlotFeatures = makeTuple(removeSlotFeatures)
        self.emptyFeature = makeString(emptyFeature)
        self.ignoreTypes = makeSet(ignoreTypes)
        self.outOfFlow = makeSet(outOfFlow)
        self.tokenType = makeString(tokenType)
        self.tokenFeatures = makeTuple(tokenFeatures)
        self.tokenNFeature = makeString(tokenNFeature)
        self.sentenceBarriers = makeSet(sentenceBarriers)
        self.sentenceSkipFlow = makeSet(sentenceSkipFlow)
        self.sentenceType = makeString(sentenceType)
        self.sentenceFeatures = makeTuple(sentenceFeatures)
        self.sentenceNFeature = makeString(sentenceNFeature)
        self.verbose = verbose
        self.write = write

    def loadApp(self, app=None, verbose=None):
        &#34;&#34;&#34;Loads a given TF app or loads the TF app based on the working directory.

        Parameters
        ----------
        app: object, optional None
            The handle to the original TF dataset, already loaded.

            If not given, we load the TF app that is nearby in the file system.

        verbose: integer, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        if app is None:
            if self.app is None:
                app = loadApp(silent=DEEP)
                self.app = app
            else:
                app = self.app
        else:
            self.app = app

        self.app = app
        version = app.version
        if verbose &gt;= 0:
            console(f&#34;Input data has version {version}&#34;)

        repoDir = app.repoLocation
        txtDir = f&#34;{repoDir}/_temp/txt&#34;
        self.txtDir = txtDir
        self.tokenFile = f&#34;{txtDir}/tokens.tsv&#34;
        self.sentenceFile = f&#34;{txtDir}/sentences.tsv&#34;
        self.textPath = f&#34;{repoDir}/_temp/txt/plain.txt&#34;

    def getElementInfo(self, verbose=None):
        &#34;&#34;&#34;Analyse the schema.

        The XML schema has useful information about the XML elements that
        occur in the source. Here we extract that information and make it
        fast-accessible.

        Parameters
        ----------
        verbose: integer, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.

        Returns
        -------
        dict
            Keyed by element name (without namespaces), where the value
            for each name is a tuple of booleans: whether the element is simple
            or complex; whether the element allows mixed content or only pure content.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        self.elementDefs = {}

        A = Analysis(verbose=verbose)
        A.configure()
        A.interpret()
        if not A.good:
            console(&#34;Could not get TEI element definitions&#34;)
            return

        elementDefs = {name: (typ, mixed) for (name, typ, mixed) in A.getDefs()}
        self.mixedTypes = {x for (x, (typ, mixed)) in elementDefs.items() if mixed}

    def generatePlain(self):
        &#34;&#34;&#34;Generates a plain text out of a data source.

        The text is generatad in such a way that out of flow elements are collected
        and put at the end. Examples of such elements are notes.
        Leaving them at their original positions will interfere with sentence detection.

        We separate the flows clearly in the output, so that they are discernible
        in the output of the NLP pipeline.

        Returns
        -------
        tuple
            The result is a tuple consisting of

            *   *text*: the generated text
            *   *positions*: a list of nodes such that list item *i* contains
                the original slot that corresponds to the character *i* in the
                generated text (counting from zero).
        &#34;&#34;&#34;
        slotFeature = self.slotFeature
        emptyFeature = self.emptyFeature
        ignoreTypes = self.ignoreTypes
        outOfFlow = self.outOfFlow
        sentenceBarriers = self.sentenceBarriers
        verbose = self.verbose
        write = self.write
        app = self.app
        info = app.info
        indent = app.indent
        api = app.api
        F = api.F
        Fs = api.Fs
        N = api.N
        T = api.T

        sentenceBreakRe = re.compile(r&#34;[.!?]&#34;)

        info(&#34;Generating a plain text with positions ...&#34;, force=verbose &gt;= 0)
        self.getElementInfo()
        mixedTypes = self.mixedTypes

        flows = {elem: [] for elem in outOfFlow}
        flows[&#34;&#34;] = []
        flowStack = [&#34;&#34;]

        nTypeStack = []

        def finishSentence(flowContent):
            nContent = len(flowContent)
            lnw = None  # last non white position
            for i in range(nContent - 1, -1, -1):
                item = flowContent[i]
                if type(item) is not str or item.strip() == &#34;&#34;:
                    continue
                else:
                    lnw = i
                    break

            if lnw is None:
                return

            # note that every slot appears in the sequence preceded by a neg int
            # and followed by a pos int
            # Material outside slots may be followed and preceded by other strings
            # We have to make sure that what we add, falls outside any slot.
            # We do that by inspecting the following item:
            # if that is a positive int, we are in a slot so we have to insert material
            # after that int
            # If the following item is a string or a negative int,
            # so we can insert right after the point where we are.
            if not sentenceBreakRe.match(flowContent[lnw]):
                offset = 1
                if lnw &lt; nContent - 1:
                    following = flowContent[lnw + 1]
                    if type(following) is int and following &gt; 0:
                        offset = 2
                flowContent.insert(lnw + offset, &#34;.&#34;)
                lnw += 1

            if not any(ch == &#34;\n&#34; for ch in flowContent[lnw + 1 :]):
                flowContent.append(&#34;\n&#34;)

        emptySlots = 0

        Femptyv = Fs(emptyFeature).v
        Fchv = Fs(slotFeature).v
        sectionTypes = T.sectionTypes

        for (n, kind) in N.walk(events=True):
            nType = F.otype.v(n)

            if nType in ignoreTypes:
                continue

            isOutFlow = nType in outOfFlow

            if kind is None:  # slot type
                if Femptyv(n):
                    emptySlots += 1
                    ch = &#34;￮&#34;
                else:
                    ch = Fchv(n)
                flows[flowStack[-1]].extend([-n, ch, n])

            elif kind:  # end node
                if isOutFlow:
                    flow = flowStack.pop()
                else:
                    flow = flowStack[-1]
                flowContent = flows[flow]

                if flow:
                    finishSentence(flowContent)
                else:
                    if nType == &#34;teiHeader&#34;:
                        finishSentence(flowContent)
                        flowContent.append(&#34; \n xxx. \nAa bb. \nEnd meta. \n\n&#34;)
                    elif nType in sectionTypes or nType in sentenceBarriers:
                        flowContent.append(f&#34; \n xxx. \nAa bb. \nEnd {nType}. \n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            finishSentence(flowContent)
                nTypeStack.pop()

            else:  # start node
                nTypeStack.append(nType)

                if isOutFlow:
                    flowStack.append(nType)
                flow = flowStack[-1]
                flowContent = flows[flow]

                if isOutFlow:
                    flowContent.append(f&#34; \nAa bb. \nitem {flow}. \n&#34;)
                else:
                    if nType == &#34;teiHeader&#34;:
                        flowContent.append(&#34; \nAa bb. \nBegin meta. \n\n&#34;)
                    elif nType in sectionTypes:
                        flowContent.append(f&#34; \nAa bb. \nBegin {nType}. \n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            flowContent.append(f&#34;{nType}. &#34;)

        indent(level=True)
        info(f&#34;Found {emptySlots} empty slots&#34;, tm=False, force=verbose &gt;= 0)

        rec = Recorder(app.api)

        for flow in sorted(flows):
            items = flows[flow]

            if len(items) == 0:
                continue

            rec.add(f&#34; \nAa bb. \nBegin flow {flow if flow else &#39;main&#39;}. \n\n&#34;)

            for item in items:
                if type(item) is int:
                    if item &lt; 0:
                        rec.start(-item)
                    else:
                        rec.end(item)
                else:
                    rec.add(item)

            rec.add(f&#34; \n xxx. \nAa bb. \nEnd flow {flow if flow else &#39;main&#39;}. \n\n&#34;)

            info(
                (
                    f&#34;recorded flow {flow if flow else &#39;main&#39;:&lt;10} &#34;
                    f&#34;with {len(items):&gt;6} items&#34;
                ),
                tm=False,
                force=verbose &gt;= 0,
            )

        indent(level=False)

        if write:
            textPath = self.textPath
            rec.write(textPath)
            info(
                f&#34;Done. Generated text and positions written to {textPath}&#34;,
                force=verbose &gt;= 0,
            )
        else:
            info(&#34;Done&#34;, force=verbose &gt;= 0)

        return (rec.text(), rec.positions(simple=True))

    @staticmethod
    def lingo(*args, **kwargs):
        return tokensAndSentences(*args, **kwargs)

    def ingest(
        self,
        isToken,
        positions,
        stream,
        tp,
        features,
        nFeature=None,
        skipBlanks=False,
        skipFlows=None,
        emptyFeature=None,
    ):
        &#34;&#34;&#34;Ingests a stream of NLP data and transforms it into nodes and features.

        The data is a stream of values associated with a spans of text.

        For each span a node will be created of the given type, and a feature
        of the given name will assign a value to that span.
        The value assigned is by default the value that is present in the data stream,
        but it is possible to specify a method to change the value.

        !!! caution
            The plain text on which the NLP pipeline has run may not correspond
            exactly with the text as defined by the corpus.
            When the plain text was generated, some extra convenience material
            may have been inserted.
            Items in the stream that refer to these pieces of text will be ignored.

            When items refer partly to proper corpus text and partly to convenience text,
            they will be narrowed down to the proper text.

        !!! caution
            The plain text may exhibit another order of material than the proper corpus
            text. For example, notes may have been collected and moved out of the
            main text flow to the end of the text.

            That means that if an item specifies a span in the plain text, it may
            not refer to a single span in the proper text, but to various spans.

            We take care to map all spans in the generated plain text back to *sets*
            of slots in the proper text.

        Parameters
        ----------
        isToken: boolean
            Whether the data specifies tokens or something else.
            Tokens are special because they are intended to become the new slot type.
        positions: list
            which slot node corresponds to which position in the plain text.

        stream: list of tuple
            The tuples should consist of

            *   *start*: a start number (char pos in the plain text, starting at `0`)
            *   *end*: an end number (char pos in tghe plain text plus one)
            *   *value*: a value for feature assignment

        tp: string
            The type of the nodes that will be generated.

        features: tuple
            The names of the features that will be generated.

        nFeature: string, optional None
            If not None, the name of a feature that will hold the sequence number of
            the element in the data stream, starting at 1.

        emptyFeature: string, optional empty
            Name of feature that identifies the empty slots.

        skipBlanks: boolean, optional False
            If True, rows whose text component is only white space will be skipped.

        skipFlows: set
            set of elements whose resulting data in the stream should be ignored

        Returns
        -------
        tuple
            We deliver the following pieces of information in a tuple:

            * the last node
            * the mapping of the new nodes to the slots they occupy;
            * the data of the new feature.
        &#34;&#34;&#34;
        slotFeature = self.slotFeature

        verbose = self.verbose
        app = self.app
        info = app.info
        indent = app.indent
        F = app.api.F
        Fs = app.api.Fs
        Fotypev = F.otype.v
        slotType = F.otype.slotType
        Fslotv = Fs(slotFeature).v
        if emptyFeature is not None:
            Femptyv = Fs(emptyFeature).v
            Femptys = Fs(emptyFeature).s

        doN = nFeature is not None
        slotLinks = {}
        featuresData = {feat: {} for feat in features}
        if nFeature is not None:
            featuresData[nFeature] = {}
        if emptyFeature is not None:
            featuresData[emptyFeature] = {}

        if isToken:
            featToken = featuresData[features[0]]
            featAfter = featuresData[features[1]]

        whiteMultipleRe = re.compile(r&#34;^[ \n]{2,}$&#34;, re.S)

        node = 0
        itemsOutside = []
        itemsEmpty = []

        info(
            f&#34;generating {tp}-nodes with features {&#39;, &#39;.join(featuresData)}&#34;,
            force=verbose &gt;= 0,
        )
        indent(level=True)

        def addToken():
            nonlocal node
            nonlocal curSlots
            nonlocal curValue

            node += 1
            slotLinks[node] = curSlots
            featToken[node] = curValue
            for (feat, val) in zip(features[2:], vals[2:]):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

            curSlots = []
            curValue = &#34;&#34;

        def addSlot(slot):
            nonlocal node

            node += 1
            slotLinks[node] = [slot]
            featToken[node] = Fslotv(slot)
            if Femptyv(slot):
                featuresData[emptyFeature][node] = 1

        def addItem():
            nonlocal node

            node += 1
            slotLinks[node] = mySlots
            for (feat, val) in zip(features, vals):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

        # First add all empty slots, provided we are doing tokens

        if isToken:
            emptySlots = (
                {s for s in Femptys(1) if Fotypev(s) == slotType}
                if emptyFeature
                else set()
            )
            emptyWithinToken = 0
            spaceWithinToken = 0

            for slot in sorted(emptySlots):
                addSlot(slot)

        # now the data from the NLP pipeline

        flowBeginRe = re.compile(r&#34; \nAa bb\. \nBegin flow (\w+)\. &#34;)
        flowEndRe = re.compile(r&#34; \n xxx. \nAa bb\. \nEnd flow (\w+)\. &#34;)

        skipping = False
        flow = None

        for (i, (b, e, *vals)) in enumerate(stream):
            if skipFlows is not None:
                text = vals[0]
                if skipping:
                    match = flowEndRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = False
                        flow = None
                        continue
                else:
                    match = flowBeginRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = flow in skipFlows
                        continue

            if skipping:
                continue

            mySlots = set()

            for j in range(b, e):
                s = positions[j]
                if s is not None:
                    mySlots.add(s)

            if len(mySlots) == 0:
                if doN:
                    vals.append(i + 1)
                itemsOutside.append((i, b, e, *vals))
                continue

            if skipBlanks and len(vals):
                slotsOrdered = sorted(mySlots)
                nSlots = len(slotsOrdered)

                start = min(
                    (
                        i
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=nSlots,
                )
                end = max(
                    (
                        i + 1
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=0,
                )

                if end &lt;= start:
                    itemsEmpty.append((i, b, e, *vals))
                    continue

                mySlots = slotsOrdered[start:end]
            else:
                mySlots = sorted(mySlots)

            curValue = &#34;&#34;
            curSlots = []

            nMySlots = len(mySlots)

            if isToken:
                # we might need to split tokens:
                # at points that correspond to empty slots
                # at spaces or newlines within the token
                # decompose it into individual characters

                tokenText = &#34;&#34;.join(Fslotv(s) for s in mySlots)

                if whiteMultipleRe.match(tokenText):
                    spaceWithinToken += 1
                    for slot in mySlots:
                        addSlot(slot)
                        spaceWithinToken += 1

                else:
                    for (i, slot) in enumerate(mySlots):
                        last = i == nMySlots - 1
                        if slot in emptySlots:
                            emptyWithinToken += 1
                            if curValue:
                                addToken()
                            if last:
                                featAfter[node] = vals[1]
                        else:
                            curValue += Fslotv(slot)
                            curSlots.append(slot)
                    if curValue:
                        addToken()
                        featAfter[node] = vals[1]
            else:
                addItem()

        repFeatures = &#34;, &#34;.join(features + ((nFeature,) if doN else ()))
        info(
            f&#34;{node} {tp} nodes have values assigned for {repFeatures}&#34;,
            force=verbose &gt;= 0,
        )
        if isToken:
            info(
                f&#34;{emptyWithinToken} empty slots have split surrounding tokens&#34;,
                force=verbose &gt;= 0,
            )
            info(
                f&#34;{spaceWithinToken} space slots have split into {slotType}s&#34;,
                force=verbose &gt;= 0,
            )

        tasks = [(&#34;Items contained in extra generated text&#34;, itemsOutside)]
        if skipBlanks:
            tasks.append((&#34;Items with empty final text&#34;, itemsEmpty))

        for (label, items) in tasks:
            nItems = len(items)
            info(f&#34;{nItems:&gt;5}x {label}&#34;, force=verbose &gt;= 0)
            indent(level=True)
            for (i, b, e, *vals) in items[0:5]:
                info(
                    f&#34;\t{i} span {b}-{e}: {&#39;, &#39;.join(str(v) for v in vals)}&#34;,
                    force=verbose == 1,
                )
            indent(level=False)

        indent(level=False)
        return (node, slotLinks, featuresData)

    def ingestTokensAndSentences(self, positions, tokenStream, sentenceStream):
        &#34;&#34;&#34;Ingests a tokens and sentences in a dataset and turn the tokens into slots.

        By default:

        * tokens become nodes of a new type `token`;
        * the texts of a token ends up in the feature `str`;
        * if there is a space after a token, it ends up in the feature `after`;
        * sentences become nodes of a new type `sentence`;
        * the sentence number ends up in the feature `nsent`.
        * tokens become the new slots.

        But this function can also be adapted to token and sentence streams that
        have additional names and values, see below.

        The streams of tokens and sentences may contain more fields.
        In the parameters `tokenFeatures` and `sentenceFeatures` you may pass the
        feature names for the data in those fields.

        When the streams are read, for each feature name in the `tokenFeatures`
        (resp. `sentenceFeatures`) the corresponding field in the stream will be
        read, and the value found there will be assigned to that feature.

        If there are more fields in the stream than there are declared in the
        `tokenFeatures` (resp. `sentenceFeatures`) parameter, these extra fields will
        be ignored.

        The last feature name in these parameters is special.
        If it is None, it will be ignored.
        Otherwise, an extra feature with that name will be created, and it will be
        filled with the node numbers of the newly generated nodes.

        !!! hint &#34;Look at the defaults&#34;
            The default `tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None)` specifies that two
            fields from the tokenstream will be read, and those values will be assigned
            to features `str` and `after`.
            There will be no field with the node itself in it.

            The default `sentenceFeatures=(&#34;nsent&#34;,)` specifies that no field from the
            tokenstream will be read, but that there will be a feature `nsent` that
            has the node of each sentence as value.

        We have to ignore the sentence boundaries in some flows,
        e.g. the material coming from `&lt;orig&gt;` and `&lt;del&gt;` elements.
        However, in the flow coming from the `&lt;note&gt;` elements, we want to retain the
        sentence boundaries.

        Parameters
        ----------
        positions: list
            which slot node corresponds to which position in the plain text.
        tokenStream: list
            The list of tokens as delivered by the NLP pipe.
        sentenceStream: list
            The list of sentences as delivered by the NLP pipe.

        Returns
        -------
        string
            The new version number of the data that contains the tokens and sentences.
        &#34;&#34;&#34;
        emptyFeature = self.emptyFeature
        removeSlotFeatures = self.removeSlotFeatures
        tokenType = self.tokenType
        tokenFeatures = self.tokenFeatures
        tokenNFeature = self.tokenNFeature
        sentenceType = self.sentenceType
        sentenceFeatures = self.sentenceFeatures
        sentenceNFeature = self.sentenceNFeature
        sentenceSkipFlow = self.sentenceSkipFlow

        app = self.app
        info = app.info
        indent = app.indent
        verbose = self.verbose
        silent = &#34;auto&#34; if verbose == 1 else TERSE if verbose == 0 else DEEP

        info(&#34;Ingesting tokens and sentences into the dataset ...&#34;, force=verbose &gt;= 0)
        indent(level=True)
        info(&#34;Mapping NLP data to nodes and features ...&#34;, force=verbose &gt;= 0)
        indent(level=True)

        slotLinks = {tokenType: {}, sentenceType: {}}
        features = {}
        lastNode = {}

        for feat in (*tokenFeatures, tokenNFeature):
            if feat is not None:
                features[feat] = {}
        lastNode[tokenType] = 0

        canSentences = len(sentenceStream) != 0

        if canSentences:
            for feat in sentenceFeatures:
                if feat is not None:
                    features[feat] = {}
            lastNode[sentenceType] = 0

        for (isToken, data, skipFlows, tp, feats, nFeat, skipBlanks, thisEmpty) in (
            (
                True,
                tokenStream,
                None,
                tokenType,
                tokenFeatures,
                tokenNFeature,
                False,
                emptyFeature,
            ),
            (
                False,
                sentenceStream,
                sentenceSkipFlow,
                sentenceType,
                sentenceFeatures,
                sentenceNFeature,
                True,
                None,
            ),
        ):
            if len(data) == 0:
                continue
            (node, theseSlotLinks, featuresData) = self.ingest(
                isToken,
                positions,
                data,
                tp,
                feats,
                nFeature=nFeat,
                emptyFeature=thisEmpty,
                skipBlanks=skipBlanks,
                skipFlows=skipFlows,
            )
            lastNode[tp] = node
            slotLinks[tp] = theseSlotLinks
            for (feat, featData) in featuresData.items():
                features[feat] = featData
            info(f&#34;{lastNode[tp]} {tp}s&#34;, force=verbose &gt;= 0)

        indent(level=False)

        info(&#34;Make a modified dataset ...&#34;, force=verbose &gt;= 0)

        repoDir = app.repoLocation
        versionPre = app.version
        version = versionPre.removesuffix(&#34;pre&#34;)
        origTf = f&#34;{repoDir}/tf/{versionPre}&#34;
        newTf = f&#34;{repoDir}/tf/{version}&#34;
        initTree(newTf, fresh=True, gentle=False)

        allTokenFeatures = list(tokenFeatures)
        if tokenNFeature is not None:
            allTokenFeatures.append(tokenNFeature)

        allSentenceFeatures = list(sentenceFeatures)
        if sentenceNFeature is not None:
            allSentenceFeatures.append(sentenceNFeature)

        addTypes = dict(
            token=dict(
                nodeFrom=1,
                nodeTo=lastNode[tokenType],
                nodeSlots=slotLinks[tokenType],
                nodeFeatures={feat: features[feat] for feat in allTokenFeatures},
            )
        )
        if canSentences:
            addTypes[&#34;sentence&#34;] = dict(
                nodeFrom=1,
                nodeTo=lastNode[sentenceType],
                nodeSlots=slotLinks[sentenceType],
                nodeFeatures={feat: features[feat] for feat in allSentenceFeatures},
            )

        featureMeta = dict(
            nsent=dict(
                valueType=&#34;int&#34;,
                description=&#34;number of sentence in corpus&#34;,
            ),
            otext={
                &#34;fmt:text-orig-full&#34;: &#34;{&#34;
                + tokenFeatures[0]
                + &#34;}{&#34;
                + tokenFeatures[1]
                + &#34;}&#34;
            },
        )

        modify(
            origTf,
            newTf,
            targetVersion=version,
            addTypes=addTypes,
            deleteTypes=(&#34;word&#34;,),
            featureMeta=featureMeta,
            replaceSlotType=(tokenType, *removeSlotFeatures),
            silent=silent,
        )
        info(&#34;Done&#34;, force=verbose &gt;= 0)
        indent(level=False)
        info(f&#34;Enriched data is available in version {version}&#34;, force=verbose &gt;= 0)
        info(
            &#34;You may need to adapt this TF app and its documentation:&#34;,
            tm=False,
            force=verbose &gt;= 0,
        )
        info(&#34;please run: python tfFromTei.py apptoken&#34;, tm=False, force=verbose &gt;= 0)
        return version

    def task(
        self,
        plaintext=False,
        lingo=False,
        ingest=False,
        write=None,
        verbose=None,
        **kwargs,
    ):
        &#34;&#34;&#34;Carry out tasks, possibly modified by flags.

        This is a higher level function that can execute a selection of tasks.

        The tasks will be executed in a fixed order: plaintext, lingo, ingest.
        But you can select which one(s) must be executed.

        If multiple tasks must be executed and one fails, the subsequent tasks
        will not be executed.

        Parameters
        ----------
        plaintext: boolean, optional False
            Whether to generate the plain text and position files.
        lingo: boolean, optional False
            Whether to carry out NLP pipeline (Spacy).
        ingest: boolean, optional False
            Whether to ingest the NLP results into the dataset..
        verbose: integer, optional -1
            Produce no (-1), some (0) or many (1) orprogress and reporting messages
        write: boolean, optional False
            Whether to write the generated plain text and position files to disk.
        kwargs: dict
            remaining arguments that can serve as input for the task

        Returns
        -------
        boolean
            Whether all tasks have executed successfully.
        &#34;&#34;&#34;

        if write is not None:
            self.write = write
        if verbose is not None:
            self.verbose = verbose

        lang = self.lang

        silent = TERSE if verbose == 1 else DEEP

        self.loadApp()
        if not self.good:
            return False

        app = self.app
        app.setSilent(silent)

        txtDir = self.txtDir
        textPath = self.textPath
        tokenFile = self.tokenFile
        sentenceFile = self.sentenceFile

        app.indent(reset=True)

        text = kwargs.get(&#34;text&#34;, None)
        positions = kwargs.get(&#34;positions&#34;, None)
        tokens = kwargs.get(&#34;tokens&#34;, None)
        sentences = kwargs.get(&#34;sentences&#34;, None)

        result = False

        if plaintext and self.good:
            (text, positions) = self.generatePlain()
            result = (text, positions) if self.good else False

        if lingo and self.good:
            app.info(f&#34;Using NLP pipeline Spacy ({lang}) ...&#34;, force=True)
            if text is None or positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                text = rec.text()
                positions = rec.positions()

            (tokens, sentences) = self.lingo(text, lang=lang)
            if write:
                if not dirExists(txtDir):
                    dirMake(txtDir)
                writeList(tokens, tokenFile, intCols=(True, True, False, False))
                writeList(sentences, sentenceFile, intCols=(True, True, False))
            app.info(&#34;NLP done&#34;, force=True)

            result = (tokens, sentences) if self.good else False

        if ingest and self.good:
            if positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                positions = rec.positions(simple=True)

            if tokens is None or sentences is None:
                tokens = readList(tokenFile)
                sentences = readList(sentenceFile)
            newVersion = self.ingestTokensAndSentences(positions, tokens, sentences)

            result = newVersion if self.good else False

        if type(result) is bool:
            if not result:
                return False
            else:
                return True

        return result


def main():
    (good, tasks, params, flags) = readArgs(&#34;tf-addnlp&#34;, HELP, TASKS, PARAMS, FLAGS)
    if not good:
        return False

    NLP = NLPipeline(**params, **flags)
    NLP.task(**tasks, **flags)

    return NLP.good


if __name__ == &#34;__main__&#34;:
    sys.exit(0 if main() else 1)</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="tf.convert.addnlp.FLAGS"><code class="name">var <span class="ident">FLAGS</span></code></dt>
<dd>
<div class="desc"><p>Possible flags.</p></div>
</dd>
<dt id="tf.convert.addnlp.PARAMS"><code class="name">var <span class="ident">PARAMS</span></code></dt>
<dd>
<div class="desc"><p>Possible parameters.</p></div>
</dd>
<dt id="tf.convert.addnlp.TASKS"><code class="name">var <span class="ident">TASKS</span></code></dt>
<dd>
<div class="desc"><p>Possible tasks.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tf.convert.addnlp.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L1179-L1187" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def main():
    (good, tasks, params, flags) = readArgs(&#34;tf-addnlp&#34;, HELP, TASKS, PARAMS, FLAGS)
    if not good:
        return False

    NLP = NLPipeline(**params, **flags)
    NLP.task(**tasks, **flags)

    return NLP.good</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tf.convert.addnlp.NLPipeline"><code class="flex name class">
<span>class <span class="ident">NLPipeline</span></span>
<span>(</span><span>app=None, lang='en', slotFeature='ch', removeSlotFeatures='ch', emptyFeature='empty', ignoreTypes='word', outOfFlow='note,orig,del', tokenType='token', tokenFeatures='str,after', tokenNFeature='', sentenceBarriers='div,p', sentenceSkipFlow='orig,del', sentenceType='sentence', sentenceFeatures='', sentenceNFeature='', verbose=-1, write=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Enrich a TF dataset with annotations generated by an NLP pipeline.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lang</code></strong> :&ensp;<code>string</code>, optional <code>en</code></dt>
<dd>The language for which the NLP tool will be set up</dd>
<dt><strong><code>app</code></strong> :&ensp;<code>object, None</code></dt>
<dd>A loaded TF app. If None, the TF App that is nearby in the file system
will be loaded.
We assume that the original data resides in the current
version, which has the string <code>pre</code> appended to it,
e.g. in version <code>1.3pre</code>.
We create a new version of the dataset, with the same number,
but without the <code>pre</code>.</dd>
<dt><strong><code>slotFeature</code></strong> :&ensp;<code>string</code>, optional <code>ch</code></dt>
<dd>The
feature on slots that provides the text of a slot to be included
in the generated text.</dd>
<dt><strong><code>removeSlotFeatures</code></strong> :&ensp;<code>"ch"</code></dt>
<dd>A tuple is distilled from comma-separated values.
The names of features defined on original slots that do not have to be
carried over to the new slots of type token.
There should be at least one feature: the character content of the slot.</dd>
<dt><strong><code>emptyFeature</code></strong> :&ensp;<code>string</code>, optional <code>"empty"</code></dt>
<dd>Name of feature that identifies the empty slots.</dd>
<dt><strong><code>ignoreTypes</code></strong> :&ensp;<code>set</code>, optional <code>"word"</code></dt>
<dd>A set is distilled from comma-separated values.
Node types that will be ignored when generating the plain text.</dd>
<dt><strong><code>outOfFlow</code></strong> :&ensp;<code>string</code>, optional <code>"note,orig,del"</code></dt>
<dd>A set is distilled from comma-separated values.
A set of node types whose content will be put in separate text flows at
the end of the document.</dd>
<dt><strong><code>sentenceSkipFlow</code></strong> :&ensp;<code>string</code>, optional <code>"orig,del"</code></dt>
<dd>A set is distilled from comma-separated values.
The elements whose flows in the sentence stream should be ignored</dd>
<dt><strong><code>tokenType</code></strong> :&ensp;<code>string</code>, optional <code>token</code></dt>
<dd>The node type for the tokens</dd>
<dt><strong><code>tokenFeatures</code></strong> :&ensp;<code>tuple</code>, optional <code>("str", "after")</code></dt>
<dd>A tuple is distilled from comma-separated values.
The names of the features that the token stream contains.
There must be at least two features:
the first one should give the token content, the second one the non-token
material until the next token.
The rest are additional features that the
pipeline might supply.</dd>
<dt><strong><code>tokenNFeature</code></strong> :&ensp;<code>string</code>, optional <code>None</code></dt>
<dd>If not None, the name of the token feature that will hold the
sequence number of the token in the data stream, starting at 1.</dd>
<dt><strong><code>sentenceType</code></strong> :&ensp;<code>string</code>, optional <code>sentence</code></dt>
<dd>The node type for the sentences</dd>
<dt><strong><code>sentenceFeatures</code></strong> :&ensp;<code>tuple</code>, optional <code>()</code></dt>
<dd>A tuple is distilled from comma-separated values.
The names of the features that the sentence stream contains.</dd>
<dt><strong><code>sentenceNFeature</code></strong> :&ensp;<code>string</code>, optional <code>nsent</code></dt>
<dd>If not None, the name of the sentence feature that will hold the
sequence number of the sentence in the data stream, starting at 1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L161-L1176" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class NLPipeline:
    def __init__(
        self,
        app=None,
        lang=PARAMS[&#34;lang&#34;][1],
        slotFeature=PARAMS[&#34;slotFeature&#34;][1],
        removeSlotFeatures=PARAMS[&#34;removeSlotFeatures&#34;][1],
        emptyFeature=PARAMS[&#34;emptyFeature&#34;][1],
        ignoreTypes=PARAMS[&#34;ignoreTypes&#34;][1],
        outOfFlow=PARAMS[&#34;outOfFlow&#34;][1],
        tokenType=PARAMS[&#34;tokenType&#34;][1],
        tokenFeatures=PARAMS[&#34;tokenFeatures&#34;][1],
        tokenNFeature=PARAMS[&#34;tokenNFeature&#34;][1],
        sentenceBarriers=PARAMS[&#34;sentenceBarriers&#34;][1],
        sentenceSkipFlow=PARAMS[&#34;sentenceSkipFlow&#34;][1],
        sentenceType=PARAMS[&#34;sentenceType&#34;][1],
        sentenceFeatures=PARAMS[&#34;sentenceFeatures&#34;][1],
        sentenceNFeature=PARAMS[&#34;sentenceNFeature&#34;][1],
        verbose=FLAGS[&#34;verbose&#34;][1],
        write=FLAGS[&#34;write&#34;][1],
    ):
        &#34;&#34;&#34;Enrich a TF dataset with annotations generated by an NLP pipeline.

        Parameters
        ----------
        lang: string, optional en
            The language for which the NLP tool will be set up
        app: object, None
            A loaded TF app. If None, the TF App that is nearby in the file system
            will be loaded.
            We assume that the original data resides in the current
            version, which has the string `pre` appended to it,
            e.g. in version `1.3pre`.
            We create a new version of the dataset, with the same number,
            but without the `pre`.
        slotFeature: string, optional ch
            The  feature on slots that provides the text of a slot to be included
            in the generated text.
        removeSlotFeatures: &#34;ch&#34;
            A tuple is distilled from comma-separated values.
            The names of features defined on original slots that do not have to be
            carried over to the new slots of type token.
            There should be at least one feature: the character content of the slot.
        emptyFeature: string, optional &#34;empty&#34;
            Name of feature that identifies the empty slots.
        ignoreTypes: set, optional &#34;word&#34;
            A set is distilled from comma-separated values.
            Node types that will be ignored when generating the plain text.
        outOfFlow: string, optional &#34;note,orig,del&#34;
            A set is distilled from comma-separated values.
            A set of node types whose content will be put in separate text flows at
            the end of the document.
        sentenceSkipFlow: string, optional &#34;orig,del&#34;
            A set is distilled from comma-separated values.
            The elements whose flows in the sentence stream should be ignored
        tokenType: string, optional token
            The node type for the tokens
        tokenFeatures: tuple, optional (&#34;str&#34;, &#34;after&#34;)
            A tuple is distilled from comma-separated values.
            The names of the features that the token stream contains.
            There must be at least two features:
            the first one should give the token content, the second one the non-token
            material until the next token.
            The rest are additional features that the
            pipeline might supply.
        tokenNFeature: string, optional None
            If not None, the name of the token feature that will hold the
            sequence number of the token in the data stream, starting at 1.
        sentenceType: string, optional sentence
            The node type for the sentences
        sentenceFeatures: tuple, optional ()
            A tuple is distilled from comma-separated values.
            The names of the features that the sentence stream contains.
        sentenceNFeature: string, optional nsent
            If not None, the name of the sentence feature that will hold the
            sequence number of the sentence in the data stream, starting at 1.

        &#34;&#34;&#34;

        def makeString(s):
            return None if not s else s

        def makeSet(s):
            return set() if not s else set(s.split(&#34;,&#34;))

        def makeTuple(s):
            return tuple() if not s else tuple(s.split(&#34;,&#34;))

        self.good = True
        self.app = app
        self.lang = makeString(lang)
        self.slotFeature = makeString(slotFeature)
        self.removeSlotFeatures = makeTuple(removeSlotFeatures)
        self.emptyFeature = makeString(emptyFeature)
        self.ignoreTypes = makeSet(ignoreTypes)
        self.outOfFlow = makeSet(outOfFlow)
        self.tokenType = makeString(tokenType)
        self.tokenFeatures = makeTuple(tokenFeatures)
        self.tokenNFeature = makeString(tokenNFeature)
        self.sentenceBarriers = makeSet(sentenceBarriers)
        self.sentenceSkipFlow = makeSet(sentenceSkipFlow)
        self.sentenceType = makeString(sentenceType)
        self.sentenceFeatures = makeTuple(sentenceFeatures)
        self.sentenceNFeature = makeString(sentenceNFeature)
        self.verbose = verbose
        self.write = write

    def loadApp(self, app=None, verbose=None):
        &#34;&#34;&#34;Loads a given TF app or loads the TF app based on the working directory.

        Parameters
        ----------
        app: object, optional None
            The handle to the original TF dataset, already loaded.

            If not given, we load the TF app that is nearby in the file system.

        verbose: integer, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        if app is None:
            if self.app is None:
                app = loadApp(silent=DEEP)
                self.app = app
            else:
                app = self.app
        else:
            self.app = app

        self.app = app
        version = app.version
        if verbose &gt;= 0:
            console(f&#34;Input data has version {version}&#34;)

        repoDir = app.repoLocation
        txtDir = f&#34;{repoDir}/_temp/txt&#34;
        self.txtDir = txtDir
        self.tokenFile = f&#34;{txtDir}/tokens.tsv&#34;
        self.sentenceFile = f&#34;{txtDir}/sentences.tsv&#34;
        self.textPath = f&#34;{repoDir}/_temp/txt/plain.txt&#34;

    def getElementInfo(self, verbose=None):
        &#34;&#34;&#34;Analyse the schema.

        The XML schema has useful information about the XML elements that
        occur in the source. Here we extract that information and make it
        fast-accessible.

        Parameters
        ----------
        verbose: integer, optional None
            Produce more progress and reporting messages
            If not passed, take the verbose member of this object.

        Returns
        -------
        dict
            Keyed by element name (without namespaces), where the value
            for each name is a tuple of booleans: whether the element is simple
            or complex; whether the element allows mixed content or only pure content.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose
        verbose = self.verbose

        self.elementDefs = {}

        A = Analysis(verbose=verbose)
        A.configure()
        A.interpret()
        if not A.good:
            console(&#34;Could not get TEI element definitions&#34;)
            return

        elementDefs = {name: (typ, mixed) for (name, typ, mixed) in A.getDefs()}
        self.mixedTypes = {x for (x, (typ, mixed)) in elementDefs.items() if mixed}

    def generatePlain(self):
        &#34;&#34;&#34;Generates a plain text out of a data source.

        The text is generatad in such a way that out of flow elements are collected
        and put at the end. Examples of such elements are notes.
        Leaving them at their original positions will interfere with sentence detection.

        We separate the flows clearly in the output, so that they are discernible
        in the output of the NLP pipeline.

        Returns
        -------
        tuple
            The result is a tuple consisting of

            *   *text*: the generated text
            *   *positions*: a list of nodes such that list item *i* contains
                the original slot that corresponds to the character *i* in the
                generated text (counting from zero).
        &#34;&#34;&#34;
        slotFeature = self.slotFeature
        emptyFeature = self.emptyFeature
        ignoreTypes = self.ignoreTypes
        outOfFlow = self.outOfFlow
        sentenceBarriers = self.sentenceBarriers
        verbose = self.verbose
        write = self.write
        app = self.app
        info = app.info
        indent = app.indent
        api = app.api
        F = api.F
        Fs = api.Fs
        N = api.N
        T = api.T

        sentenceBreakRe = re.compile(r&#34;[.!?]&#34;)

        info(&#34;Generating a plain text with positions ...&#34;, force=verbose &gt;= 0)
        self.getElementInfo()
        mixedTypes = self.mixedTypes

        flows = {elem: [] for elem in outOfFlow}
        flows[&#34;&#34;] = []
        flowStack = [&#34;&#34;]

        nTypeStack = []

        def finishSentence(flowContent):
            nContent = len(flowContent)
            lnw = None  # last non white position
            for i in range(nContent - 1, -1, -1):
                item = flowContent[i]
                if type(item) is not str or item.strip() == &#34;&#34;:
                    continue
                else:
                    lnw = i
                    break

            if lnw is None:
                return

            # note that every slot appears in the sequence preceded by a neg int
            # and followed by a pos int
            # Material outside slots may be followed and preceded by other strings
            # We have to make sure that what we add, falls outside any slot.
            # We do that by inspecting the following item:
            # if that is a positive int, we are in a slot so we have to insert material
            # after that int
            # If the following item is a string or a negative int,
            # so we can insert right after the point where we are.
            if not sentenceBreakRe.match(flowContent[lnw]):
                offset = 1
                if lnw &lt; nContent - 1:
                    following = flowContent[lnw + 1]
                    if type(following) is int and following &gt; 0:
                        offset = 2
                flowContent.insert(lnw + offset, &#34;.&#34;)
                lnw += 1

            if not any(ch == &#34;\n&#34; for ch in flowContent[lnw + 1 :]):
                flowContent.append(&#34;\n&#34;)

        emptySlots = 0

        Femptyv = Fs(emptyFeature).v
        Fchv = Fs(slotFeature).v
        sectionTypes = T.sectionTypes

        for (n, kind) in N.walk(events=True):
            nType = F.otype.v(n)

            if nType in ignoreTypes:
                continue

            isOutFlow = nType in outOfFlow

            if kind is None:  # slot type
                if Femptyv(n):
                    emptySlots += 1
                    ch = &#34;￮&#34;
                else:
                    ch = Fchv(n)
                flows[flowStack[-1]].extend([-n, ch, n])

            elif kind:  # end node
                if isOutFlow:
                    flow = flowStack.pop()
                else:
                    flow = flowStack[-1]
                flowContent = flows[flow]

                if flow:
                    finishSentence(flowContent)
                else:
                    if nType == &#34;teiHeader&#34;:
                        finishSentence(flowContent)
                        flowContent.append(&#34; \n xxx. \nAa bb. \nEnd meta. \n\n&#34;)
                    elif nType in sectionTypes or nType in sentenceBarriers:
                        flowContent.append(f&#34; \n xxx. \nAa bb. \nEnd {nType}. \n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            finishSentence(flowContent)
                nTypeStack.pop()

            else:  # start node
                nTypeStack.append(nType)

                if isOutFlow:
                    flowStack.append(nType)
                flow = flowStack[-1]
                flowContent = flows[flow]

                if isOutFlow:
                    flowContent.append(f&#34; \nAa bb. \nitem {flow}. \n&#34;)
                else:
                    if nType == &#34;teiHeader&#34;:
                        flowContent.append(&#34; \nAa bb. \nBegin meta. \n\n&#34;)
                    elif nType in sectionTypes:
                        flowContent.append(f&#34; \nAa bb. \nBegin {nType}. \n\n&#34;)
                    else:
                        if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                            nTp in mixedTypes for nTp in nTypeStack[0:-1]
                        ):
                            flowContent.append(f&#34;{nType}. &#34;)

        indent(level=True)
        info(f&#34;Found {emptySlots} empty slots&#34;, tm=False, force=verbose &gt;= 0)

        rec = Recorder(app.api)

        for flow in sorted(flows):
            items = flows[flow]

            if len(items) == 0:
                continue

            rec.add(f&#34; \nAa bb. \nBegin flow {flow if flow else &#39;main&#39;}. \n\n&#34;)

            for item in items:
                if type(item) is int:
                    if item &lt; 0:
                        rec.start(-item)
                    else:
                        rec.end(item)
                else:
                    rec.add(item)

            rec.add(f&#34; \n xxx. \nAa bb. \nEnd flow {flow if flow else &#39;main&#39;}. \n\n&#34;)

            info(
                (
                    f&#34;recorded flow {flow if flow else &#39;main&#39;:&lt;10} &#34;
                    f&#34;with {len(items):&gt;6} items&#34;
                ),
                tm=False,
                force=verbose &gt;= 0,
            )

        indent(level=False)

        if write:
            textPath = self.textPath
            rec.write(textPath)
            info(
                f&#34;Done. Generated text and positions written to {textPath}&#34;,
                force=verbose &gt;= 0,
            )
        else:
            info(&#34;Done&#34;, force=verbose &gt;= 0)

        return (rec.text(), rec.positions(simple=True))

    @staticmethod
    def lingo(*args, **kwargs):
        return tokensAndSentences(*args, **kwargs)

    def ingest(
        self,
        isToken,
        positions,
        stream,
        tp,
        features,
        nFeature=None,
        skipBlanks=False,
        skipFlows=None,
        emptyFeature=None,
    ):
        &#34;&#34;&#34;Ingests a stream of NLP data and transforms it into nodes and features.

        The data is a stream of values associated with a spans of text.

        For each span a node will be created of the given type, and a feature
        of the given name will assign a value to that span.
        The value assigned is by default the value that is present in the data stream,
        but it is possible to specify a method to change the value.

        !!! caution
            The plain text on which the NLP pipeline has run may not correspond
            exactly with the text as defined by the corpus.
            When the plain text was generated, some extra convenience material
            may have been inserted.
            Items in the stream that refer to these pieces of text will be ignored.

            When items refer partly to proper corpus text and partly to convenience text,
            they will be narrowed down to the proper text.

        !!! caution
            The plain text may exhibit another order of material than the proper corpus
            text. For example, notes may have been collected and moved out of the
            main text flow to the end of the text.

            That means that if an item specifies a span in the plain text, it may
            not refer to a single span in the proper text, but to various spans.

            We take care to map all spans in the generated plain text back to *sets*
            of slots in the proper text.

        Parameters
        ----------
        isToken: boolean
            Whether the data specifies tokens or something else.
            Tokens are special because they are intended to become the new slot type.
        positions: list
            which slot node corresponds to which position in the plain text.

        stream: list of tuple
            The tuples should consist of

            *   *start*: a start number (char pos in the plain text, starting at `0`)
            *   *end*: an end number (char pos in tghe plain text plus one)
            *   *value*: a value for feature assignment

        tp: string
            The type of the nodes that will be generated.

        features: tuple
            The names of the features that will be generated.

        nFeature: string, optional None
            If not None, the name of a feature that will hold the sequence number of
            the element in the data stream, starting at 1.

        emptyFeature: string, optional empty
            Name of feature that identifies the empty slots.

        skipBlanks: boolean, optional False
            If True, rows whose text component is only white space will be skipped.

        skipFlows: set
            set of elements whose resulting data in the stream should be ignored

        Returns
        -------
        tuple
            We deliver the following pieces of information in a tuple:

            * the last node
            * the mapping of the new nodes to the slots they occupy;
            * the data of the new feature.
        &#34;&#34;&#34;
        slotFeature = self.slotFeature

        verbose = self.verbose
        app = self.app
        info = app.info
        indent = app.indent
        F = app.api.F
        Fs = app.api.Fs
        Fotypev = F.otype.v
        slotType = F.otype.slotType
        Fslotv = Fs(slotFeature).v
        if emptyFeature is not None:
            Femptyv = Fs(emptyFeature).v
            Femptys = Fs(emptyFeature).s

        doN = nFeature is not None
        slotLinks = {}
        featuresData = {feat: {} for feat in features}
        if nFeature is not None:
            featuresData[nFeature] = {}
        if emptyFeature is not None:
            featuresData[emptyFeature] = {}

        if isToken:
            featToken = featuresData[features[0]]
            featAfter = featuresData[features[1]]

        whiteMultipleRe = re.compile(r&#34;^[ \n]{2,}$&#34;, re.S)

        node = 0
        itemsOutside = []
        itemsEmpty = []

        info(
            f&#34;generating {tp}-nodes with features {&#39;, &#39;.join(featuresData)}&#34;,
            force=verbose &gt;= 0,
        )
        indent(level=True)

        def addToken():
            nonlocal node
            nonlocal curSlots
            nonlocal curValue

            node += 1
            slotLinks[node] = curSlots
            featToken[node] = curValue
            for (feat, val) in zip(features[2:], vals[2:]):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

            curSlots = []
            curValue = &#34;&#34;

        def addSlot(slot):
            nonlocal node

            node += 1
            slotLinks[node] = [slot]
            featToken[node] = Fslotv(slot)
            if Femptyv(slot):
                featuresData[emptyFeature][node] = 1

        def addItem():
            nonlocal node

            node += 1
            slotLinks[node] = mySlots
            for (feat, val) in zip(features, vals):
                featuresData[feat][node] = val
            if doN:
                featuresData[nFeature][node] = node

        # First add all empty slots, provided we are doing tokens

        if isToken:
            emptySlots = (
                {s for s in Femptys(1) if Fotypev(s) == slotType}
                if emptyFeature
                else set()
            )
            emptyWithinToken = 0
            spaceWithinToken = 0

            for slot in sorted(emptySlots):
                addSlot(slot)

        # now the data from the NLP pipeline

        flowBeginRe = re.compile(r&#34; \nAa bb\. \nBegin flow (\w+)\. &#34;)
        flowEndRe = re.compile(r&#34; \n xxx. \nAa bb\. \nEnd flow (\w+)\. &#34;)

        skipping = False
        flow = None

        for (i, (b, e, *vals)) in enumerate(stream):
            if skipFlows is not None:
                text = vals[0]
                if skipping:
                    match = flowEndRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = False
                        flow = None
                        continue
                else:
                    match = flowBeginRe.match(text)
                    if match:
                        flow = match.group(1)
                        skipping = flow in skipFlows
                        continue

            if skipping:
                continue

            mySlots = set()

            for j in range(b, e):
                s = positions[j]
                if s is not None:
                    mySlots.add(s)

            if len(mySlots) == 0:
                if doN:
                    vals.append(i + 1)
                itemsOutside.append((i, b, e, *vals))
                continue

            if skipBlanks and len(vals):
                slotsOrdered = sorted(mySlots)
                nSlots = len(slotsOrdered)

                start = min(
                    (
                        i
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=nSlots,
                )
                end = max(
                    (
                        i + 1
                        for (i, s) in enumerate(slotsOrdered)
                        if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                    ),
                    default=0,
                )

                if end &lt;= start:
                    itemsEmpty.append((i, b, e, *vals))
                    continue

                mySlots = slotsOrdered[start:end]
            else:
                mySlots = sorted(mySlots)

            curValue = &#34;&#34;
            curSlots = []

            nMySlots = len(mySlots)

            if isToken:
                # we might need to split tokens:
                # at points that correspond to empty slots
                # at spaces or newlines within the token
                # decompose it into individual characters

                tokenText = &#34;&#34;.join(Fslotv(s) for s in mySlots)

                if whiteMultipleRe.match(tokenText):
                    spaceWithinToken += 1
                    for slot in mySlots:
                        addSlot(slot)
                        spaceWithinToken += 1

                else:
                    for (i, slot) in enumerate(mySlots):
                        last = i == nMySlots - 1
                        if slot in emptySlots:
                            emptyWithinToken += 1
                            if curValue:
                                addToken()
                            if last:
                                featAfter[node] = vals[1]
                        else:
                            curValue += Fslotv(slot)
                            curSlots.append(slot)
                    if curValue:
                        addToken()
                        featAfter[node] = vals[1]
            else:
                addItem()

        repFeatures = &#34;, &#34;.join(features + ((nFeature,) if doN else ()))
        info(
            f&#34;{node} {tp} nodes have values assigned for {repFeatures}&#34;,
            force=verbose &gt;= 0,
        )
        if isToken:
            info(
                f&#34;{emptyWithinToken} empty slots have split surrounding tokens&#34;,
                force=verbose &gt;= 0,
            )
            info(
                f&#34;{spaceWithinToken} space slots have split into {slotType}s&#34;,
                force=verbose &gt;= 0,
            )

        tasks = [(&#34;Items contained in extra generated text&#34;, itemsOutside)]
        if skipBlanks:
            tasks.append((&#34;Items with empty final text&#34;, itemsEmpty))

        for (label, items) in tasks:
            nItems = len(items)
            info(f&#34;{nItems:&gt;5}x {label}&#34;, force=verbose &gt;= 0)
            indent(level=True)
            for (i, b, e, *vals) in items[0:5]:
                info(
                    f&#34;\t{i} span {b}-{e}: {&#39;, &#39;.join(str(v) for v in vals)}&#34;,
                    force=verbose == 1,
                )
            indent(level=False)

        indent(level=False)
        return (node, slotLinks, featuresData)

    def ingestTokensAndSentences(self, positions, tokenStream, sentenceStream):
        &#34;&#34;&#34;Ingests a tokens and sentences in a dataset and turn the tokens into slots.

        By default:

        * tokens become nodes of a new type `token`;
        * the texts of a token ends up in the feature `str`;
        * if there is a space after a token, it ends up in the feature `after`;
        * sentences become nodes of a new type `sentence`;
        * the sentence number ends up in the feature `nsent`.
        * tokens become the new slots.

        But this function can also be adapted to token and sentence streams that
        have additional names and values, see below.

        The streams of tokens and sentences may contain more fields.
        In the parameters `tokenFeatures` and `sentenceFeatures` you may pass the
        feature names for the data in those fields.

        When the streams are read, for each feature name in the `tokenFeatures`
        (resp. `sentenceFeatures`) the corresponding field in the stream will be
        read, and the value found there will be assigned to that feature.

        If there are more fields in the stream than there are declared in the
        `tokenFeatures` (resp. `sentenceFeatures`) parameter, these extra fields will
        be ignored.

        The last feature name in these parameters is special.
        If it is None, it will be ignored.
        Otherwise, an extra feature with that name will be created, and it will be
        filled with the node numbers of the newly generated nodes.

        !!! hint &#34;Look at the defaults&#34;
            The default `tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None)` specifies that two
            fields from the tokenstream will be read, and those values will be assigned
            to features `str` and `after`.
            There will be no field with the node itself in it.

            The default `sentenceFeatures=(&#34;nsent&#34;,)` specifies that no field from the
            tokenstream will be read, but that there will be a feature `nsent` that
            has the node of each sentence as value.

        We have to ignore the sentence boundaries in some flows,
        e.g. the material coming from `&lt;orig&gt;` and `&lt;del&gt;` elements.
        However, in the flow coming from the `&lt;note&gt;` elements, we want to retain the
        sentence boundaries.

        Parameters
        ----------
        positions: list
            which slot node corresponds to which position in the plain text.
        tokenStream: list
            The list of tokens as delivered by the NLP pipe.
        sentenceStream: list
            The list of sentences as delivered by the NLP pipe.

        Returns
        -------
        string
            The new version number of the data that contains the tokens and sentences.
        &#34;&#34;&#34;
        emptyFeature = self.emptyFeature
        removeSlotFeatures = self.removeSlotFeatures
        tokenType = self.tokenType
        tokenFeatures = self.tokenFeatures
        tokenNFeature = self.tokenNFeature
        sentenceType = self.sentenceType
        sentenceFeatures = self.sentenceFeatures
        sentenceNFeature = self.sentenceNFeature
        sentenceSkipFlow = self.sentenceSkipFlow

        app = self.app
        info = app.info
        indent = app.indent
        verbose = self.verbose
        silent = &#34;auto&#34; if verbose == 1 else TERSE if verbose == 0 else DEEP

        info(&#34;Ingesting tokens and sentences into the dataset ...&#34;, force=verbose &gt;= 0)
        indent(level=True)
        info(&#34;Mapping NLP data to nodes and features ...&#34;, force=verbose &gt;= 0)
        indent(level=True)

        slotLinks = {tokenType: {}, sentenceType: {}}
        features = {}
        lastNode = {}

        for feat in (*tokenFeatures, tokenNFeature):
            if feat is not None:
                features[feat] = {}
        lastNode[tokenType] = 0

        canSentences = len(sentenceStream) != 0

        if canSentences:
            for feat in sentenceFeatures:
                if feat is not None:
                    features[feat] = {}
            lastNode[sentenceType] = 0

        for (isToken, data, skipFlows, tp, feats, nFeat, skipBlanks, thisEmpty) in (
            (
                True,
                tokenStream,
                None,
                tokenType,
                tokenFeatures,
                tokenNFeature,
                False,
                emptyFeature,
            ),
            (
                False,
                sentenceStream,
                sentenceSkipFlow,
                sentenceType,
                sentenceFeatures,
                sentenceNFeature,
                True,
                None,
            ),
        ):
            if len(data) == 0:
                continue
            (node, theseSlotLinks, featuresData) = self.ingest(
                isToken,
                positions,
                data,
                tp,
                feats,
                nFeature=nFeat,
                emptyFeature=thisEmpty,
                skipBlanks=skipBlanks,
                skipFlows=skipFlows,
            )
            lastNode[tp] = node
            slotLinks[tp] = theseSlotLinks
            for (feat, featData) in featuresData.items():
                features[feat] = featData
            info(f&#34;{lastNode[tp]} {tp}s&#34;, force=verbose &gt;= 0)

        indent(level=False)

        info(&#34;Make a modified dataset ...&#34;, force=verbose &gt;= 0)

        repoDir = app.repoLocation
        versionPre = app.version
        version = versionPre.removesuffix(&#34;pre&#34;)
        origTf = f&#34;{repoDir}/tf/{versionPre}&#34;
        newTf = f&#34;{repoDir}/tf/{version}&#34;
        initTree(newTf, fresh=True, gentle=False)

        allTokenFeatures = list(tokenFeatures)
        if tokenNFeature is not None:
            allTokenFeatures.append(tokenNFeature)

        allSentenceFeatures = list(sentenceFeatures)
        if sentenceNFeature is not None:
            allSentenceFeatures.append(sentenceNFeature)

        addTypes = dict(
            token=dict(
                nodeFrom=1,
                nodeTo=lastNode[tokenType],
                nodeSlots=slotLinks[tokenType],
                nodeFeatures={feat: features[feat] for feat in allTokenFeatures},
            )
        )
        if canSentences:
            addTypes[&#34;sentence&#34;] = dict(
                nodeFrom=1,
                nodeTo=lastNode[sentenceType],
                nodeSlots=slotLinks[sentenceType],
                nodeFeatures={feat: features[feat] for feat in allSentenceFeatures},
            )

        featureMeta = dict(
            nsent=dict(
                valueType=&#34;int&#34;,
                description=&#34;number of sentence in corpus&#34;,
            ),
            otext={
                &#34;fmt:text-orig-full&#34;: &#34;{&#34;
                + tokenFeatures[0]
                + &#34;}{&#34;
                + tokenFeatures[1]
                + &#34;}&#34;
            },
        )

        modify(
            origTf,
            newTf,
            targetVersion=version,
            addTypes=addTypes,
            deleteTypes=(&#34;word&#34;,),
            featureMeta=featureMeta,
            replaceSlotType=(tokenType, *removeSlotFeatures),
            silent=silent,
        )
        info(&#34;Done&#34;, force=verbose &gt;= 0)
        indent(level=False)
        info(f&#34;Enriched data is available in version {version}&#34;, force=verbose &gt;= 0)
        info(
            &#34;You may need to adapt this TF app and its documentation:&#34;,
            tm=False,
            force=verbose &gt;= 0,
        )
        info(&#34;please run: python tfFromTei.py apptoken&#34;, tm=False, force=verbose &gt;= 0)
        return version

    def task(
        self,
        plaintext=False,
        lingo=False,
        ingest=False,
        write=None,
        verbose=None,
        **kwargs,
    ):
        &#34;&#34;&#34;Carry out tasks, possibly modified by flags.

        This is a higher level function that can execute a selection of tasks.

        The tasks will be executed in a fixed order: plaintext, lingo, ingest.
        But you can select which one(s) must be executed.

        If multiple tasks must be executed and one fails, the subsequent tasks
        will not be executed.

        Parameters
        ----------
        plaintext: boolean, optional False
            Whether to generate the plain text and position files.
        lingo: boolean, optional False
            Whether to carry out NLP pipeline (Spacy).
        ingest: boolean, optional False
            Whether to ingest the NLP results into the dataset..
        verbose: integer, optional -1
            Produce no (-1), some (0) or many (1) orprogress and reporting messages
        write: boolean, optional False
            Whether to write the generated plain text and position files to disk.
        kwargs: dict
            remaining arguments that can serve as input for the task

        Returns
        -------
        boolean
            Whether all tasks have executed successfully.
        &#34;&#34;&#34;

        if write is not None:
            self.write = write
        if verbose is not None:
            self.verbose = verbose

        lang = self.lang

        silent = TERSE if verbose == 1 else DEEP

        self.loadApp()
        if not self.good:
            return False

        app = self.app
        app.setSilent(silent)

        txtDir = self.txtDir
        textPath = self.textPath
        tokenFile = self.tokenFile
        sentenceFile = self.sentenceFile

        app.indent(reset=True)

        text = kwargs.get(&#34;text&#34;, None)
        positions = kwargs.get(&#34;positions&#34;, None)
        tokens = kwargs.get(&#34;tokens&#34;, None)
        sentences = kwargs.get(&#34;sentences&#34;, None)

        result = False

        if plaintext and self.good:
            (text, positions) = self.generatePlain()
            result = (text, positions) if self.good else False

        if lingo and self.good:
            app.info(f&#34;Using NLP pipeline Spacy ({lang}) ...&#34;, force=True)
            if text is None or positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                text = rec.text()
                positions = rec.positions()

            (tokens, sentences) = self.lingo(text, lang=lang)
            if write:
                if not dirExists(txtDir):
                    dirMake(txtDir)
                writeList(tokens, tokenFile, intCols=(True, True, False, False))
                writeList(sentences, sentenceFile, intCols=(True, True, False))
            app.info(&#34;NLP done&#34;, force=True)

            result = (tokens, sentences) if self.good else False

        if ingest and self.good:
            if positions is None:
                rec = Recorder(app.api)
                rec.read(textPath)
                positions = rec.positions(simple=True)

            if tokens is None or sentences is None:
                tokens = readList(tokenFile)
                sentences = readList(sentenceFile)
            newVersion = self.ingestTokensAndSentences(positions, tokens, sentences)

            result = newVersion if self.good else False

        if type(result) is bool:
            if not result:
                return False
            else:
                return True

        return result</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="tf.convert.addnlp.NLPipeline.lingo"><code class="name flex">
<span>def <span class="ident">lingo</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L538-L540" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@staticmethod
def lingo(*args, **kwargs):
    return tokensAndSentences(*args, **kwargs)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="tf.convert.addnlp.NLPipeline.generatePlain"><code class="name flex">
<span>def <span class="ident">generatePlain</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a plain text out of a data source.</p>
<p>The text is generatad in such a way that out of flow elements are collected
and put at the end. Examples of such elements are notes.
Leaving them at their original positions will interfere with sentence detection.</p>
<p>We separate the flows clearly in the output, so that they are discernible
in the output of the NLP pipeline.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>
<p>The result is a tuple consisting of</p>
<ul>
<li><em>text</em>: the generated text</li>
<li><em>positions</em>: a list of nodes such that list item <em>i</em> contains
the original slot that corresponds to the character <em>i</em> in the
generated text (counting from zero).</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L343-L536" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def generatePlain(self):
    &#34;&#34;&#34;Generates a plain text out of a data source.

    The text is generatad in such a way that out of flow elements are collected
    and put at the end. Examples of such elements are notes.
    Leaving them at their original positions will interfere with sentence detection.

    We separate the flows clearly in the output, so that they are discernible
    in the output of the NLP pipeline.

    Returns
    -------
    tuple
        The result is a tuple consisting of

        *   *text*: the generated text
        *   *positions*: a list of nodes such that list item *i* contains
            the original slot that corresponds to the character *i* in the
            generated text (counting from zero).
    &#34;&#34;&#34;
    slotFeature = self.slotFeature
    emptyFeature = self.emptyFeature
    ignoreTypes = self.ignoreTypes
    outOfFlow = self.outOfFlow
    sentenceBarriers = self.sentenceBarriers
    verbose = self.verbose
    write = self.write
    app = self.app
    info = app.info
    indent = app.indent
    api = app.api
    F = api.F
    Fs = api.Fs
    N = api.N
    T = api.T

    sentenceBreakRe = re.compile(r&#34;[.!?]&#34;)

    info(&#34;Generating a plain text with positions ...&#34;, force=verbose &gt;= 0)
    self.getElementInfo()
    mixedTypes = self.mixedTypes

    flows = {elem: [] for elem in outOfFlow}
    flows[&#34;&#34;] = []
    flowStack = [&#34;&#34;]

    nTypeStack = []

    def finishSentence(flowContent):
        nContent = len(flowContent)
        lnw = None  # last non white position
        for i in range(nContent - 1, -1, -1):
            item = flowContent[i]
            if type(item) is not str or item.strip() == &#34;&#34;:
                continue
            else:
                lnw = i
                break

        if lnw is None:
            return

        # note that every slot appears in the sequence preceded by a neg int
        # and followed by a pos int
        # Material outside slots may be followed and preceded by other strings
        # We have to make sure that what we add, falls outside any slot.
        # We do that by inspecting the following item:
        # if that is a positive int, we are in a slot so we have to insert material
        # after that int
        # If the following item is a string or a negative int,
        # so we can insert right after the point where we are.
        if not sentenceBreakRe.match(flowContent[lnw]):
            offset = 1
            if lnw &lt; nContent - 1:
                following = flowContent[lnw + 1]
                if type(following) is int and following &gt; 0:
                    offset = 2
            flowContent.insert(lnw + offset, &#34;.&#34;)
            lnw += 1

        if not any(ch == &#34;\n&#34; for ch in flowContent[lnw + 1 :]):
            flowContent.append(&#34;\n&#34;)

    emptySlots = 0

    Femptyv = Fs(emptyFeature).v
    Fchv = Fs(slotFeature).v
    sectionTypes = T.sectionTypes

    for (n, kind) in N.walk(events=True):
        nType = F.otype.v(n)

        if nType in ignoreTypes:
            continue

        isOutFlow = nType in outOfFlow

        if kind is None:  # slot type
            if Femptyv(n):
                emptySlots += 1
                ch = &#34;￮&#34;
            else:
                ch = Fchv(n)
            flows[flowStack[-1]].extend([-n, ch, n])

        elif kind:  # end node
            if isOutFlow:
                flow = flowStack.pop()
            else:
                flow = flowStack[-1]
            flowContent = flows[flow]

            if flow:
                finishSentence(flowContent)
            else:
                if nType == &#34;teiHeader&#34;:
                    finishSentence(flowContent)
                    flowContent.append(&#34; \n xxx. \nAa bb. \nEnd meta. \n\n&#34;)
                elif nType in sectionTypes or nType in sentenceBarriers:
                    flowContent.append(f&#34; \n xxx. \nAa bb. \nEnd {nType}. \n\n&#34;)
                else:
                    if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                        nTp in mixedTypes for nTp in nTypeStack[0:-1]
                    ):
                        finishSentence(flowContent)
            nTypeStack.pop()

        else:  # start node
            nTypeStack.append(nType)

            if isOutFlow:
                flowStack.append(nType)
            flow = flowStack[-1]
            flowContent = flows[flow]

            if isOutFlow:
                flowContent.append(f&#34; \nAa bb. \nitem {flow}. \n&#34;)
            else:
                if nType == &#34;teiHeader&#34;:
                    flowContent.append(&#34; \nAa bb. \nBegin meta. \n\n&#34;)
                elif nType in sectionTypes:
                    flowContent.append(f&#34; \nAa bb. \nBegin {nType}. \n\n&#34;)
                else:
                    if any(nTp == &#34;teiHeader&#34; for nTp in nTypeStack) and not any(
                        nTp in mixedTypes for nTp in nTypeStack[0:-1]
                    ):
                        flowContent.append(f&#34;{nType}. &#34;)

    indent(level=True)
    info(f&#34;Found {emptySlots} empty slots&#34;, tm=False, force=verbose &gt;= 0)

    rec = Recorder(app.api)

    for flow in sorted(flows):
        items = flows[flow]

        if len(items) == 0:
            continue

        rec.add(f&#34; \nAa bb. \nBegin flow {flow if flow else &#39;main&#39;}. \n\n&#34;)

        for item in items:
            if type(item) is int:
                if item &lt; 0:
                    rec.start(-item)
                else:
                    rec.end(item)
            else:
                rec.add(item)

        rec.add(f&#34; \n xxx. \nAa bb. \nEnd flow {flow if flow else &#39;main&#39;}. \n\n&#34;)

        info(
            (
                f&#34;recorded flow {flow if flow else &#39;main&#39;:&lt;10} &#34;
                f&#34;with {len(items):&gt;6} items&#34;
            ),
            tm=False,
            force=verbose &gt;= 0,
        )

    indent(level=False)

    if write:
        textPath = self.textPath
        rec.write(textPath)
        info(
            f&#34;Done. Generated text and positions written to {textPath}&#34;,
            force=verbose &gt;= 0,
        )
    else:
        info(&#34;Done&#34;, force=verbose &gt;= 0)

    return (rec.text(), rec.positions(simple=True))</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.getElementInfo"><code class="name flex">
<span>def <span class="ident">getElementInfo</span></span>(<span>self, verbose=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Analyse the schema.</p>
<p>The XML schema has useful information about the XML elements that
occur in the source. Here we extract that information and make it
fast-accessible.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>verbose</code></strong> :&ensp;<code>integer</code>, optional <code>None</code></dt>
<dd>Produce more progress and reporting messages
If not passed, take the verbose member of this object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Keyed by element name (without namespaces), where the value
for each name is a tuple of booleans: whether the element is simple
or complex; whether the element allows mixed content or only pure content.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L307-L341" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def getElementInfo(self, verbose=None):
    &#34;&#34;&#34;Analyse the schema.

    The XML schema has useful information about the XML elements that
    occur in the source. Here we extract that information and make it
    fast-accessible.

    Parameters
    ----------
    verbose: integer, optional None
        Produce more progress and reporting messages
        If not passed, take the verbose member of this object.

    Returns
    -------
    dict
        Keyed by element name (without namespaces), where the value
        for each name is a tuple of booleans: whether the element is simple
        or complex; whether the element allows mixed content or only pure content.
    &#34;&#34;&#34;
    if verbose is not None:
        self.verbose = verbose
    verbose = self.verbose

    self.elementDefs = {}

    A = Analysis(verbose=verbose)
    A.configure()
    A.interpret()
    if not A.good:
        console(&#34;Could not get TEI element definitions&#34;)
        return

    elementDefs = {name: (typ, mixed) for (name, typ, mixed) in A.getDefs()}
    self.mixedTypes = {x for (x, (typ, mixed)) in elementDefs.items() if mixed}</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.ingest"><code class="name flex">
<span>def <span class="ident">ingest</span></span>(<span>self, isToken, positions, stream, tp, features, nFeature=None, skipBlanks=False, skipFlows=None, emptyFeature=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Ingests a stream of NLP data and transforms it into nodes and features.</p>
<p>The data is a stream of values associated with a spans of text.</p>
<p>For each span a node will be created of the given type, and a feature
of the given name will assign a value to that span.
The value assigned is by default the value that is present in the data stream,
but it is possible to specify a method to change the value.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The plain text on which the NLP pipeline has run may not correspond
exactly with the text as defined by the corpus.
When the plain text was generated, some extra convenience material
may have been inserted.
Items in the stream that refer to these pieces of text will be ignored.</p>
<p>When items refer partly to proper corpus text and partly to convenience text,
they will be narrowed down to the proper text.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The plain text may exhibit another order of material than the proper corpus
text. For example, notes may have been collected and moved out of the
main text flow to the end of the text.</p>
<p>That means that if an item specifies a span in the plain text, it may
not refer to a single span in the proper text, but to various spans.</p>
<p>We take care to map all spans in the generated plain text back to <em>sets</em>
of slots in the proper text.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>isToken</code></strong> :&ensp;<code>boolean</code></dt>
<dd>Whether the data specifies tokens or something else.
Tokens are special because they are intended to become the new slot type.</dd>
<dt><strong><code>positions</code></strong> :&ensp;<code>list</code></dt>
<dd>which slot node corresponds to which position in the plain text.</dd>
<dt><strong><code>stream</code></strong> :&ensp;<code>list</code> of <code>tuple</code></dt>
<dd>
<p>The tuples should consist of</p>
<ul>
<li><em>start</em>: a start number (char pos in the plain text, starting at <code>0</code>)</li>
<li><em>end</em>: an end number (char pos in tghe plain text plus one)</li>
<li><em>value</em>: a value for feature assignment</li>
</ul>
</dd>
<dt><strong><code>tp</code></strong> :&ensp;<code>string</code></dt>
<dd>The type of the nodes that will be generated.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The names of the features that will be generated.</dd>
<dt><strong><code>nFeature</code></strong> :&ensp;<code>string</code>, optional <code>None</code></dt>
<dd>If not None, the name of a feature that will hold the sequence number of
the element in the data stream, starting at 1.</dd>
<dt><strong><code>emptyFeature</code></strong> :&ensp;<code>string</code>, optional <code>empty</code></dt>
<dd>Name of feature that identifies the empty slots.</dd>
<dt><strong><code>skipBlanks</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>If True, rows whose text component is only white space will be skipped.</dd>
<dt><strong><code>skipFlows</code></strong> :&ensp;<code>set</code></dt>
<dd>set of elements whose resulting data in the stream should be ignored</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>
<p>We deliver the following pieces of information in a tuple:</p>
<ul>
<li>the last node</li>
<li>the mapping of the new nodes to the slots they occupy;</li>
<li>the data of the new feature.</li>
</ul>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L542-L853" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def ingest(
    self,
    isToken,
    positions,
    stream,
    tp,
    features,
    nFeature=None,
    skipBlanks=False,
    skipFlows=None,
    emptyFeature=None,
):
    &#34;&#34;&#34;Ingests a stream of NLP data and transforms it into nodes and features.

    The data is a stream of values associated with a spans of text.

    For each span a node will be created of the given type, and a feature
    of the given name will assign a value to that span.
    The value assigned is by default the value that is present in the data stream,
    but it is possible to specify a method to change the value.

    !!! caution
        The plain text on which the NLP pipeline has run may not correspond
        exactly with the text as defined by the corpus.
        When the plain text was generated, some extra convenience material
        may have been inserted.
        Items in the stream that refer to these pieces of text will be ignored.

        When items refer partly to proper corpus text and partly to convenience text,
        they will be narrowed down to the proper text.

    !!! caution
        The plain text may exhibit another order of material than the proper corpus
        text. For example, notes may have been collected and moved out of the
        main text flow to the end of the text.

        That means that if an item specifies a span in the plain text, it may
        not refer to a single span in the proper text, but to various spans.

        We take care to map all spans in the generated plain text back to *sets*
        of slots in the proper text.

    Parameters
    ----------
    isToken: boolean
        Whether the data specifies tokens or something else.
        Tokens are special because they are intended to become the new slot type.
    positions: list
        which slot node corresponds to which position in the plain text.

    stream: list of tuple
        The tuples should consist of

        *   *start*: a start number (char pos in the plain text, starting at `0`)
        *   *end*: an end number (char pos in tghe plain text plus one)
        *   *value*: a value for feature assignment

    tp: string
        The type of the nodes that will be generated.

    features: tuple
        The names of the features that will be generated.

    nFeature: string, optional None
        If not None, the name of a feature that will hold the sequence number of
        the element in the data stream, starting at 1.

    emptyFeature: string, optional empty
        Name of feature that identifies the empty slots.

    skipBlanks: boolean, optional False
        If True, rows whose text component is only white space will be skipped.

    skipFlows: set
        set of elements whose resulting data in the stream should be ignored

    Returns
    -------
    tuple
        We deliver the following pieces of information in a tuple:

        * the last node
        * the mapping of the new nodes to the slots they occupy;
        * the data of the new feature.
    &#34;&#34;&#34;
    slotFeature = self.slotFeature

    verbose = self.verbose
    app = self.app
    info = app.info
    indent = app.indent
    F = app.api.F
    Fs = app.api.Fs
    Fotypev = F.otype.v
    slotType = F.otype.slotType
    Fslotv = Fs(slotFeature).v
    if emptyFeature is not None:
        Femptyv = Fs(emptyFeature).v
        Femptys = Fs(emptyFeature).s

    doN = nFeature is not None
    slotLinks = {}
    featuresData = {feat: {} for feat in features}
    if nFeature is not None:
        featuresData[nFeature] = {}
    if emptyFeature is not None:
        featuresData[emptyFeature] = {}

    if isToken:
        featToken = featuresData[features[0]]
        featAfter = featuresData[features[1]]

    whiteMultipleRe = re.compile(r&#34;^[ \n]{2,}$&#34;, re.S)

    node = 0
    itemsOutside = []
    itemsEmpty = []

    info(
        f&#34;generating {tp}-nodes with features {&#39;, &#39;.join(featuresData)}&#34;,
        force=verbose &gt;= 0,
    )
    indent(level=True)

    def addToken():
        nonlocal node
        nonlocal curSlots
        nonlocal curValue

        node += 1
        slotLinks[node] = curSlots
        featToken[node] = curValue
        for (feat, val) in zip(features[2:], vals[2:]):
            featuresData[feat][node] = val
        if doN:
            featuresData[nFeature][node] = node

        curSlots = []
        curValue = &#34;&#34;

    def addSlot(slot):
        nonlocal node

        node += 1
        slotLinks[node] = [slot]
        featToken[node] = Fslotv(slot)
        if Femptyv(slot):
            featuresData[emptyFeature][node] = 1

    def addItem():
        nonlocal node

        node += 1
        slotLinks[node] = mySlots
        for (feat, val) in zip(features, vals):
            featuresData[feat][node] = val
        if doN:
            featuresData[nFeature][node] = node

    # First add all empty slots, provided we are doing tokens

    if isToken:
        emptySlots = (
            {s for s in Femptys(1) if Fotypev(s) == slotType}
            if emptyFeature
            else set()
        )
        emptyWithinToken = 0
        spaceWithinToken = 0

        for slot in sorted(emptySlots):
            addSlot(slot)

    # now the data from the NLP pipeline

    flowBeginRe = re.compile(r&#34; \nAa bb\. \nBegin flow (\w+)\. &#34;)
    flowEndRe = re.compile(r&#34; \n xxx. \nAa bb\. \nEnd flow (\w+)\. &#34;)

    skipping = False
    flow = None

    for (i, (b, e, *vals)) in enumerate(stream):
        if skipFlows is not None:
            text = vals[0]
            if skipping:
                match = flowEndRe.match(text)
                if match:
                    flow = match.group(1)
                    skipping = False
                    flow = None
                    continue
            else:
                match = flowBeginRe.match(text)
                if match:
                    flow = match.group(1)
                    skipping = flow in skipFlows
                    continue

        if skipping:
            continue

        mySlots = set()

        for j in range(b, e):
            s = positions[j]
            if s is not None:
                mySlots.add(s)

        if len(mySlots) == 0:
            if doN:
                vals.append(i + 1)
            itemsOutside.append((i, b, e, *vals))
            continue

        if skipBlanks and len(vals):
            slotsOrdered = sorted(mySlots)
            nSlots = len(slotsOrdered)

            start = min(
                (
                    i
                    for (i, s) in enumerate(slotsOrdered)
                    if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                ),
                default=nSlots,
            )
            end = max(
                (
                    i + 1
                    for (i, s) in enumerate(slotsOrdered)
                    if Fslotv(s) not in {&#34; &#34;, &#34;\t&#34;, &#34;\n&#34;}
                ),
                default=0,
            )

            if end &lt;= start:
                itemsEmpty.append((i, b, e, *vals))
                continue

            mySlots = slotsOrdered[start:end]
        else:
            mySlots = sorted(mySlots)

        curValue = &#34;&#34;
        curSlots = []

        nMySlots = len(mySlots)

        if isToken:
            # we might need to split tokens:
            # at points that correspond to empty slots
            # at spaces or newlines within the token
            # decompose it into individual characters

            tokenText = &#34;&#34;.join(Fslotv(s) for s in mySlots)

            if whiteMultipleRe.match(tokenText):
                spaceWithinToken += 1
                for slot in mySlots:
                    addSlot(slot)
                    spaceWithinToken += 1

            else:
                for (i, slot) in enumerate(mySlots):
                    last = i == nMySlots - 1
                    if slot in emptySlots:
                        emptyWithinToken += 1
                        if curValue:
                            addToken()
                        if last:
                            featAfter[node] = vals[1]
                    else:
                        curValue += Fslotv(slot)
                        curSlots.append(slot)
                if curValue:
                    addToken()
                    featAfter[node] = vals[1]
        else:
            addItem()

    repFeatures = &#34;, &#34;.join(features + ((nFeature,) if doN else ()))
    info(
        f&#34;{node} {tp} nodes have values assigned for {repFeatures}&#34;,
        force=verbose &gt;= 0,
    )
    if isToken:
        info(
            f&#34;{emptyWithinToken} empty slots have split surrounding tokens&#34;,
            force=verbose &gt;= 0,
        )
        info(
            f&#34;{spaceWithinToken} space slots have split into {slotType}s&#34;,
            force=verbose &gt;= 0,
        )

    tasks = [(&#34;Items contained in extra generated text&#34;, itemsOutside)]
    if skipBlanks:
        tasks.append((&#34;Items with empty final text&#34;, itemsEmpty))

    for (label, items) in tasks:
        nItems = len(items)
        info(f&#34;{nItems:&gt;5}x {label}&#34;, force=verbose &gt;= 0)
        indent(level=True)
        for (i, b, e, *vals) in items[0:5]:
            info(
                f&#34;\t{i} span {b}-{e}: {&#39;, &#39;.join(str(v) for v in vals)}&#34;,
                force=verbose == 1,
            )
        indent(level=False)

    indent(level=False)
    return (node, slotLinks, featuresData)</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.ingestTokensAndSentences"><code class="name flex">
<span>def <span class="ident">ingestTokensAndSentences</span></span>(<span>self, positions, tokenStream, sentenceStream)</span>
</code></dt>
<dd>
<div class="desc"><p>Ingests a tokens and sentences in a dataset and turn the tokens into slots.</p>
<p>By default:</p>
<ul>
<li>tokens become nodes of a new type <code>token</code>;</li>
<li>the texts of a token ends up in the feature <code>str</code>;</li>
<li>if there is a space after a token, it ends up in the feature <code>after</code>;</li>
<li>sentences become nodes of a new type <code>sentence</code>;</li>
<li>the sentence number ends up in the feature <code>nsent</code>.</li>
<li>tokens become the new slots.</li>
</ul>
<p>But this function can also be adapted to token and sentence streams that
have additional names and values, see below.</p>
<p>The streams of tokens and sentences may contain more fields.
In the parameters <code>tokenFeatures</code> and <code>sentenceFeatures</code> you may pass the
feature names for the data in those fields.</p>
<p>When the streams are read, for each feature name in the <code>tokenFeatures</code>
(resp. <code>sentenceFeatures</code>) the corresponding field in the stream will be
read, and the value found there will be assigned to that feature.</p>
<p>If there are more fields in the stream than there are declared in the
<code>tokenFeatures</code> (resp. <code>sentenceFeatures</code>) parameter, these extra fields will
be ignored.</p>
<p>The last feature name in these parameters is special.
If it is None, it will be ignored.
Otherwise, an extra feature with that name will be created, and it will be
filled with the node numbers of the newly generated nodes.</p>
<div class="admonition hint">
<p class="admonition-title">Look at the defaults</p>
<p>The default <code>tokenFeatures=("str", "after", None)</code> specifies that two
fields from the tokenstream will be read, and those values will be assigned
to features <code>str</code> and <code>after</code>.
There will be no field with the node itself in it.</p>
<p>The default <code>sentenceFeatures=("nsent",)</code> specifies that no field from the
tokenstream will be read, but that there will be a feature <code>nsent</code> that
has the node of each sentence as value.</p>
</div>
<p>We have to ignore the sentence boundaries in some flows,
e.g. the material coming from <code>&lt;orig&gt;</code> and <code>&lt;del&gt;</code> elements.
However, in the flow coming from the <code>&lt;note&gt;</code> elements, we want to retain the
sentence boundaries.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>positions</code></strong> :&ensp;<code>list</code></dt>
<dd>which slot node corresponds to which position in the plain text.</dd>
<dt><strong><code>tokenStream</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of tokens as delivered by the NLP pipe.</dd>
<dt><strong><code>sentenceStream</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of sentences as delivered by the NLP pipe.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>string</code></dt>
<dd>The new version number of the data that contains the tokens and sentences.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L855-L1063" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def ingestTokensAndSentences(self, positions, tokenStream, sentenceStream):
    &#34;&#34;&#34;Ingests a tokens and sentences in a dataset and turn the tokens into slots.

    By default:

    * tokens become nodes of a new type `token`;
    * the texts of a token ends up in the feature `str`;
    * if there is a space after a token, it ends up in the feature `after`;
    * sentences become nodes of a new type `sentence`;
    * the sentence number ends up in the feature `nsent`.
    * tokens become the new slots.

    But this function can also be adapted to token and sentence streams that
    have additional names and values, see below.

    The streams of tokens and sentences may contain more fields.
    In the parameters `tokenFeatures` and `sentenceFeatures` you may pass the
    feature names for the data in those fields.

    When the streams are read, for each feature name in the `tokenFeatures`
    (resp. `sentenceFeatures`) the corresponding field in the stream will be
    read, and the value found there will be assigned to that feature.

    If there are more fields in the stream than there are declared in the
    `tokenFeatures` (resp. `sentenceFeatures`) parameter, these extra fields will
    be ignored.

    The last feature name in these parameters is special.
    If it is None, it will be ignored.
    Otherwise, an extra feature with that name will be created, and it will be
    filled with the node numbers of the newly generated nodes.

    !!! hint &#34;Look at the defaults&#34;
        The default `tokenFeatures=(&#34;str&#34;, &#34;after&#34;, None)` specifies that two
        fields from the tokenstream will be read, and those values will be assigned
        to features `str` and `after`.
        There will be no field with the node itself in it.

        The default `sentenceFeatures=(&#34;nsent&#34;,)` specifies that no field from the
        tokenstream will be read, but that there will be a feature `nsent` that
        has the node of each sentence as value.

    We have to ignore the sentence boundaries in some flows,
    e.g. the material coming from `&lt;orig&gt;` and `&lt;del&gt;` elements.
    However, in the flow coming from the `&lt;note&gt;` elements, we want to retain the
    sentence boundaries.

    Parameters
    ----------
    positions: list
        which slot node corresponds to which position in the plain text.
    tokenStream: list
        The list of tokens as delivered by the NLP pipe.
    sentenceStream: list
        The list of sentences as delivered by the NLP pipe.

    Returns
    -------
    string
        The new version number of the data that contains the tokens and sentences.
    &#34;&#34;&#34;
    emptyFeature = self.emptyFeature
    removeSlotFeatures = self.removeSlotFeatures
    tokenType = self.tokenType
    tokenFeatures = self.tokenFeatures
    tokenNFeature = self.tokenNFeature
    sentenceType = self.sentenceType
    sentenceFeatures = self.sentenceFeatures
    sentenceNFeature = self.sentenceNFeature
    sentenceSkipFlow = self.sentenceSkipFlow

    app = self.app
    info = app.info
    indent = app.indent
    verbose = self.verbose
    silent = &#34;auto&#34; if verbose == 1 else TERSE if verbose == 0 else DEEP

    info(&#34;Ingesting tokens and sentences into the dataset ...&#34;, force=verbose &gt;= 0)
    indent(level=True)
    info(&#34;Mapping NLP data to nodes and features ...&#34;, force=verbose &gt;= 0)
    indent(level=True)

    slotLinks = {tokenType: {}, sentenceType: {}}
    features = {}
    lastNode = {}

    for feat in (*tokenFeatures, tokenNFeature):
        if feat is not None:
            features[feat] = {}
    lastNode[tokenType] = 0

    canSentences = len(sentenceStream) != 0

    if canSentences:
        for feat in sentenceFeatures:
            if feat is not None:
                features[feat] = {}
        lastNode[sentenceType] = 0

    for (isToken, data, skipFlows, tp, feats, nFeat, skipBlanks, thisEmpty) in (
        (
            True,
            tokenStream,
            None,
            tokenType,
            tokenFeatures,
            tokenNFeature,
            False,
            emptyFeature,
        ),
        (
            False,
            sentenceStream,
            sentenceSkipFlow,
            sentenceType,
            sentenceFeatures,
            sentenceNFeature,
            True,
            None,
        ),
    ):
        if len(data) == 0:
            continue
        (node, theseSlotLinks, featuresData) = self.ingest(
            isToken,
            positions,
            data,
            tp,
            feats,
            nFeature=nFeat,
            emptyFeature=thisEmpty,
            skipBlanks=skipBlanks,
            skipFlows=skipFlows,
        )
        lastNode[tp] = node
        slotLinks[tp] = theseSlotLinks
        for (feat, featData) in featuresData.items():
            features[feat] = featData
        info(f&#34;{lastNode[tp]} {tp}s&#34;, force=verbose &gt;= 0)

    indent(level=False)

    info(&#34;Make a modified dataset ...&#34;, force=verbose &gt;= 0)

    repoDir = app.repoLocation
    versionPre = app.version
    version = versionPre.removesuffix(&#34;pre&#34;)
    origTf = f&#34;{repoDir}/tf/{versionPre}&#34;
    newTf = f&#34;{repoDir}/tf/{version}&#34;
    initTree(newTf, fresh=True, gentle=False)

    allTokenFeatures = list(tokenFeatures)
    if tokenNFeature is not None:
        allTokenFeatures.append(tokenNFeature)

    allSentenceFeatures = list(sentenceFeatures)
    if sentenceNFeature is not None:
        allSentenceFeatures.append(sentenceNFeature)

    addTypes = dict(
        token=dict(
            nodeFrom=1,
            nodeTo=lastNode[tokenType],
            nodeSlots=slotLinks[tokenType],
            nodeFeatures={feat: features[feat] for feat in allTokenFeatures},
        )
    )
    if canSentences:
        addTypes[&#34;sentence&#34;] = dict(
            nodeFrom=1,
            nodeTo=lastNode[sentenceType],
            nodeSlots=slotLinks[sentenceType],
            nodeFeatures={feat: features[feat] for feat in allSentenceFeatures},
        )

    featureMeta = dict(
        nsent=dict(
            valueType=&#34;int&#34;,
            description=&#34;number of sentence in corpus&#34;,
        ),
        otext={
            &#34;fmt:text-orig-full&#34;: &#34;{&#34;
            + tokenFeatures[0]
            + &#34;}{&#34;
            + tokenFeatures[1]
            + &#34;}&#34;
        },
    )

    modify(
        origTf,
        newTf,
        targetVersion=version,
        addTypes=addTypes,
        deleteTypes=(&#34;word&#34;,),
        featureMeta=featureMeta,
        replaceSlotType=(tokenType, *removeSlotFeatures),
        silent=silent,
    )
    info(&#34;Done&#34;, force=verbose &gt;= 0)
    indent(level=False)
    info(f&#34;Enriched data is available in version {version}&#34;, force=verbose &gt;= 0)
    info(
        &#34;You may need to adapt this TF app and its documentation:&#34;,
        tm=False,
        force=verbose &gt;= 0,
    )
    info(&#34;please run: python tfFromTei.py apptoken&#34;, tm=False, force=verbose &gt;= 0)
    return version</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.loadApp"><code class="name flex">
<span>def <span class="ident">loadApp</span></span>(<span>self, app=None, verbose=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a given TF app or loads the TF app based on the working directory.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>app</code></strong> :&ensp;<code>object</code>, optional <code>None</code></dt>
<dd>
<p>The handle to the original TF dataset, already loaded.</p>
<p>If not given, we load the TF app that is nearby in the file system.</p>
</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>integer</code>, optional <code>None</code></dt>
<dd>Produce more progress and reporting messages
If not passed, take the verbose member of this object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L268-L305" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def loadApp(self, app=None, verbose=None):
    &#34;&#34;&#34;Loads a given TF app or loads the TF app based on the working directory.

    Parameters
    ----------
    app: object, optional None
        The handle to the original TF dataset, already loaded.

        If not given, we load the TF app that is nearby in the file system.

    verbose: integer, optional None
        Produce more progress and reporting messages
        If not passed, take the verbose member of this object.
    &#34;&#34;&#34;
    if verbose is not None:
        self.verbose = verbose
    verbose = self.verbose

    if app is None:
        if self.app is None:
            app = loadApp(silent=DEEP)
            self.app = app
        else:
            app = self.app
    else:
        self.app = app

    self.app = app
    version = app.version
    if verbose &gt;= 0:
        console(f&#34;Input data has version {version}&#34;)

    repoDir = app.repoLocation
    txtDir = f&#34;{repoDir}/_temp/txt&#34;
    self.txtDir = txtDir
    self.tokenFile = f&#34;{txtDir}/tokens.tsv&#34;
    self.sentenceFile = f&#34;{txtDir}/sentences.tsv&#34;
    self.textPath = f&#34;{repoDir}/_temp/txt/plain.txt&#34;</code></pre>
</details>
</dd>
<dt id="tf.convert.addnlp.NLPipeline.task"><code class="name flex">
<span>def <span class="ident">task</span></span>(<span>self, plaintext=False, lingo=False, ingest=False, write=None, verbose=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Carry out tasks, possibly modified by flags.</p>
<p>This is a higher level function that can execute a selection of tasks.</p>
<p>The tasks will be executed in a fixed order: plaintext, lingo, ingest.
But you can select which one(s) must be executed.</p>
<p>If multiple tasks must be executed and one fails, the subsequent tasks
will not be executed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>plaintext</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to generate the plain text and position files.</dd>
<dt><strong><code>lingo</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to carry out NLP pipeline (Spacy).</dd>
<dt><strong><code>ingest</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to ingest the NLP results into the dataset..</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>integer</code>, optional <code>-1</code></dt>
<dd>Produce no (-1), some (0) or many (1) orprogress and reporting messages</dd>
<dt><strong><code>write</code></strong> :&ensp;<code>boolean</code>, optional <code>False</code></dt>
<dd>Whether to write the generated plain text and position files to disk.</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>remaining arguments that can serve as input for the task</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>boolean</code></dt>
<dd>Whether all tasks have executed successfully.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/1e1cdac3f24700a57db444644d62f4472fcf5677/tf/convert/addnlp.py#L1065-L1176" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def task(
    self,
    plaintext=False,
    lingo=False,
    ingest=False,
    write=None,
    verbose=None,
    **kwargs,
):
    &#34;&#34;&#34;Carry out tasks, possibly modified by flags.

    This is a higher level function that can execute a selection of tasks.

    The tasks will be executed in a fixed order: plaintext, lingo, ingest.
    But you can select which one(s) must be executed.

    If multiple tasks must be executed and one fails, the subsequent tasks
    will not be executed.

    Parameters
    ----------
    plaintext: boolean, optional False
        Whether to generate the plain text and position files.
    lingo: boolean, optional False
        Whether to carry out NLP pipeline (Spacy).
    ingest: boolean, optional False
        Whether to ingest the NLP results into the dataset..
    verbose: integer, optional -1
        Produce no (-1), some (0) or many (1) orprogress and reporting messages
    write: boolean, optional False
        Whether to write the generated plain text and position files to disk.
    kwargs: dict
        remaining arguments that can serve as input for the task

    Returns
    -------
    boolean
        Whether all tasks have executed successfully.
    &#34;&#34;&#34;

    if write is not None:
        self.write = write
    if verbose is not None:
        self.verbose = verbose

    lang = self.lang

    silent = TERSE if verbose == 1 else DEEP

    self.loadApp()
    if not self.good:
        return False

    app = self.app
    app.setSilent(silent)

    txtDir = self.txtDir
    textPath = self.textPath
    tokenFile = self.tokenFile
    sentenceFile = self.sentenceFile

    app.indent(reset=True)

    text = kwargs.get(&#34;text&#34;, None)
    positions = kwargs.get(&#34;positions&#34;, None)
    tokens = kwargs.get(&#34;tokens&#34;, None)
    sentences = kwargs.get(&#34;sentences&#34;, None)

    result = False

    if plaintext and self.good:
        (text, positions) = self.generatePlain()
        result = (text, positions) if self.good else False

    if lingo and self.good:
        app.info(f&#34;Using NLP pipeline Spacy ({lang}) ...&#34;, force=True)
        if text is None or positions is None:
            rec = Recorder(app.api)
            rec.read(textPath)
            text = rec.text()
            positions = rec.positions()

        (tokens, sentences) = self.lingo(text, lang=lang)
        if write:
            if not dirExists(txtDir):
                dirMake(txtDir)
            writeList(tokens, tokenFile, intCols=(True, True, False, False))
            writeList(sentences, sentenceFile, intCols=(True, True, False))
        app.info(&#34;NLP done&#34;, force=True)

        result = (tokens, sentences) if self.good else False

    if ingest and self.good:
        if positions is None:
            rec = Recorder(app.api)
            rec.read(textPath)
            positions = rec.positions(simple=True)

        if tokens is None or sentences is None:
            tokens = readList(tokenFile)
            sentences = readList(sentenceFile)
        newVersion = self.ingestTokensAndSentences(positions, tokens, sentences)

        result = newVersion if self.good else False

    if type(result) is bool:
        if not result:
            return False
        else:
            return True

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<p><a href="https://github.com/annotation" title="annotation on GitHub"><img src="../../tf/images/tf-small.png" alt="annotation"></a></p>
<p><a href="../../tf/index.html">tf home</a> -
<a href="../../tf/cheatsheet.html">cheat sheet</a> -
<a href="https://github.com/annotation/text-fabric" title="GitHub repo"><img src="../../tf/images/GitHub_Logo.png" alt="GitHub" width="50"></a></p>
</p>
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#requirements">Requirements</a></li>
<li><a href="#effect">Effect</a></li>
<li><a href="#homework">Homework</a></li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#commandline">Commandline</a></li>
<li><a href="#from-python">From Python</a></li>
<li><a href="#examples">Examples</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tf.convert" href="index.html">tf.convert</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="tf.convert.addnlp.FLAGS" href="#tf.convert.addnlp.FLAGS">FLAGS</a></code></li>
<li><code><a title="tf.convert.addnlp.PARAMS" href="#tf.convert.addnlp.PARAMS">PARAMS</a></code></li>
<li><code><a title="tf.convert.addnlp.TASKS" href="#tf.convert.addnlp.TASKS">TASKS</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tf.convert.addnlp.main" href="#tf.convert.addnlp.main">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tf.convert.addnlp.NLPipeline" href="#tf.convert.addnlp.NLPipeline">NLPipeline</a></code></h4>
<ul class="">
<li><code><a title="tf.convert.addnlp.NLPipeline.generatePlain" href="#tf.convert.addnlp.NLPipeline.generatePlain">generatePlain</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.getElementInfo" href="#tf.convert.addnlp.NLPipeline.getElementInfo">getElementInfo</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.ingest" href="#tf.convert.addnlp.NLPipeline.ingest">ingest</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.ingestTokensAndSentences" href="#tf.convert.addnlp.NLPipeline.ingestTokensAndSentences">ingestTokensAndSentences</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.lingo" href="#tf.convert.addnlp.NLPipeline.lingo">lingo</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.loadApp" href="#tf.convert.addnlp.NLPipeline.loadApp">loadApp</a></code></li>
<li><code><a title="tf.convert.addnlp.NLPipeline.task" href="#tf.convert.addnlp.NLPipeline.task">task</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://pure.knaw.nl/portal/en/persons/dirk-roorda">Dirk Roorda</a>
<a href="https://huc.knaw.nl"><img alt="HuC" src="../../tf/images/huc.png" width="200" alt="Humanities Cluster"></a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>