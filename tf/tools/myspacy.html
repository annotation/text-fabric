<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tf.tools.myspacy API documentation</title>
<meta name="description" content="Get words and tokens from a plain text with the help of Spacy ‚Ä¶" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tf.tools.myspacy</code></h1>
</header>
<section id="section-intro">
<p>Get words and tokens from a plain text with the help of Spacy.</p>
<p>This module supposes that you have installed Spacy and the necessary
language modules.</p>
<p>To get <a href="https://spacy.io">Spacy</a>, do</p>
<pre><code>pip install spacy
</code></pre>
<p>The Enlish language module can then be installed by</p>
<pre><code>python -m spacy download en_core_web_sm
</code></pre>
<p>You can install many more <a href="https://spacy.io/usage/models">language models</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e4191ae3e7bbe8c426b07012cac4e759f5b02de6/tf/tools/myspacy.py#L1-L287" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;Get words and tokens from a plain text with the help of Spacy.

This module supposes that you have installed Spacy and the necessary
language modules.

To get [Spacy](https://spacy.io), do

```
pip install spacy
```

The Enlish language module can then be installed by

```
python -m spacy download en_core_web_sm
```

You can install many more [language models](https://spacy.io/usage/models).
&#34;&#34;&#34;

import re
import spacy
from spacy.cli.download import download

from ..core.helpers import console


LANG_MODELS = &#34;&#34;&#34;
ca core_news Catalan
da core_news Danish
de core_news German
el core_news Greek
en core_web English
es core_news Spanish
fi core_news Finnish
fr core_news French
hr core_news Croatian
it core_news Italian
ja core_news Japanese
ko core_news Korean
lt core_news Lithuanian
mk core_news Macedonian
nb core_news Norwegian (Bokm√•l)
nl core_news Dutch
pl core_news Polish
pt core_news Portuguese
ro core_news Romanian
ru core_news Russian
sv core_news Swedish
uk core_news Ukrainian
zh core_news Chinese
xx ent_wiki multi-language
&#34;&#34;&#34;.strip().split(&#34;\n&#34;)
&#34;&#34;&#34;Languages and their associated Spacy models.&#34;&#34;&#34;


class Spacy:
    def __init__(self, lang=None):
        &#34;&#34;&#34;Sets up an NLP (Natural Language Processing) pipeline.

        The pipeline is tied to a particular language model, which you can pass
        as a parameter, provided you have installed it.

        For now, we use Spacy in a fairly trivial way: only tokenization and sentence
        detection.
        We do not need the parser for this.

        Parameters
        ----------
        lang: string, optional xx
            The language to be used; Spacy may need to download it, if so, it will
            happen automatically.

            If the language is not supported by Spacy, we switch to the multilanguage
            called `xx`.

            See `tf.tools.myspacy.LANG_MODELS` about the language models that Spacy supports.
        &#34;&#34;&#34;
        langModels = {}
        languages = {}

        for spec in LANG_MODELS:
            (lng, model, language) = spec.split(maxsplit=2)
            langModels[lng] = f&#34;{lng}_{model}_sm&#34;
            languages[lng] = language

        self.langModels = langModels
        self.languages = languages

        prevLang = None
        targetLang = lang
        loaded = False

        i = 0
        while True:
            i += 1
            targetModel = langModels.get(targetLang, None)
            targetLanguage = languages.get(targetLang, None)

            if targetModel is None:
                (prevLang, targetLang) = (targetLang, &#34;xx&#34;)
                targetModel = langModels[targetLang]
                targetLanguage = languages[targetLang]

                if prevLang is None:
                    console(&#34;No language specified&#34;)
                else:
                    console(
                        f&#34;No language model for {prevLang} supported by Spacy.\n&#34;
                    )
                console(
                    f&#34;Switching to the {targetLanguage} model&#34;
                )
                if targetLang == prevLang:
                    break
                else:
                    continue

            try:
                nlp = spacy.load(targetModel)
                loaded = True
                break

            except Exception:
                console(f&#34;Language model {targetModel} not installed. Downloading ...&#34;)

            try:
                console(f&#34;Downloading {targetModel} ...&#34;)
                download(targetModel)
            except Exception:
                console(f&#34;Could not download {targetModel} ...&#34;)
                (prevLang, targetLang) = (targetLang, &#34;xx&#34;)
                if targetLang == prevLang:
                    break
                else:
                    continue

        if loaded:
            try:
                nlp.disable_pipe(&#34;parser&#34;)
                nlp.disable_pipe(&#34;sentencizer&#34;)
            except Exception:
                pass
            try:
                nlp.enable_pipe(&#34;senter&#34;)
                self.canSentence = True
            except Exception:
                self.canSentence = False
                console(&#34;This language does not support sentence boundary detection&#34;)
        else:
            console(&#34;Cannot load (language data) to get Spacy working&#34;)
            nlp = None

        self.nlp = nlp
        self.doc = None

    def read(self, text):
        &#34;&#34;&#34;Process a plain text.

        A text is ingested and tokenized. Sentences are detected.
        This may require quite some processing time, think of 30 seconds for 200,000
        words on a decent laptop.

        Parameters
        ----------
        text: string
            The complete, raw text.
        &#34;&#34;&#34;
        nText = len(text)
        nlp = self.nlp

        if nlp is None:
            console(&#34;The NLP pipeline is not functioning&#34;)
            return

        nlp.max_length = nText
        doc = nlp(text)
        self.doc = doc

    def getTokens(self):
        &#34;&#34;&#34;Get the resulting tokens.

        A token is represented as a tuple consisting of

        *   *start*: first character position that the token occupies in the text.
            Character positions start at 0.
        *   *end*: last character position that the token occupies in the text
            *plus one*.
        *   *text*: text of the token, **excluding any trailing whitespace**.
        *   *space*: any white space behind the token, if present, otherwise
            the empty string.


        !!! note &#34;End position and space&#34;
            If there is a space behind the token, it will not add to the end position
            of the token. So the start and end positions of the tokens reflect
            where the tokens themselves are, and spaces do nto belong to the tokens.

        Returns
        -------
        list
            All tokens as tuples.
        &#34;&#34;&#34;
        doc = self.doc
        if self.doc is None:
            console(&#34;No results available from the NLP pipeline&#34;)
            return []

        result = []
        for token in doc:
            start = token.idx
            text = token.text
            space = token.whitespace_
            end = start + len(text)
            result.append((start, end, text, space))
        return result

    def getSentences(self):
        &#34;&#34;&#34;Get the resulting sentences.

        A sentence is represented as a tuple consisting of

        *   *start*: first character position that the sentence occupies in the text.
            Character positions start at 0.
        *   *end*: last character position that the sentence occupies in the text
            *plus one*.
        *   *text*: text of the sentence.

        Returns
        -------
        list
            All sentences as tuples.
        &#34;&#34;&#34;
        doc = self.doc
        if self.doc is None:
            console(&#34;No results available from the NLP pipeline&#34;)
            return []

        if not self.canSentence:
            console(&#34;No sentence results available from the NLP pipeline&#34;)
            return []

        result = []

        whiteRe = re.compile(r&#34;^[.?!\s]*$&#34;, re.S)
        spuriousNlBefore = re.compile(r&#34;\n+(\W)&#34;)
        spuriousNlAfter = re.compile(r&#34;(\W)\n+&#34;)

        for s in doc.sents:
            text = s.text.strip(&#34;\n&#34;)
            if whiteRe.match(text):
                continue

            tokenStart = doc[s.start]
            tokenEnd = doc[s.end - 1]
            sentStart = tokenStart.idx
            sentEnd = tokenEnd.idx + len(tokenEnd.text)

            text = spuriousNlBefore.sub(r&#34;\1&#34;, text)
            text = spuriousNlAfter.sub(r&#34;\1&#34;, text)
            result.append((sentStart, sentEnd, text))
        return result


# def tokensAndSentences(text, langmodel=&#34;en_core_web_sm&#34;):
def tokensAndSentences(text, lang=&#34;en&#34;):
    &#34;&#34;&#34;Runs the Spacy NLP pipeline and delivers the results.

    Parameters
    ----------
    text: string
        The complete, raw text.
    lang: string, optional en
        The language to be used; its model should be installed; see
        `tf.tools.myspacy` for how to get language models.

    Returns
    -------
    tuple
        `tokens`: the token list as tuples
        `sentences`: the sentence list as tuples

        Both tokens and sentences are tuples (start, end, text).
    &#34;&#34;&#34;
    S = Spacy(lang=lang)
    S.read(text)
    return (S.getTokens(), S.getSentences())</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="tf.tools.myspacy.LANG_MODELS"><code class="name">var <span class="ident">LANG_MODELS</span></code></dt>
<dd>
<div class="desc"><p>Languages and their associated Spacy models.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tf.tools.myspacy.tokensAndSentences"><code class="name flex">
<span>def <span class="ident">tokensAndSentences</span></span>(<span>text, lang='en')</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the Spacy NLP pipeline and delivers the results.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>string</code></dt>
<dd>The complete, raw text.</dd>
<dt><strong><code>lang</code></strong> :&ensp;<code>string</code>, optional <code>en</code></dt>
<dd>The language to be used; its model should be installed; see
<code><a title="tf.tools.myspacy" href="#tf.tools.myspacy">tf.tools.myspacy</a></code> for how to get language models.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>
<p><code>tokens</code>: the token list as tuples
<code>sentences</code>: the sentence list as tuples</p>
<p>Both tokens and sentences are tuples (start, end, text).</p>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e4191ae3e7bbe8c426b07012cac4e759f5b02de6/tf/tools/myspacy.py#L266-L287" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def tokensAndSentences(text, lang=&#34;en&#34;):
    &#34;&#34;&#34;Runs the Spacy NLP pipeline and delivers the results.

    Parameters
    ----------
    text: string
        The complete, raw text.
    lang: string, optional en
        The language to be used; its model should be installed; see
        `tf.tools.myspacy` for how to get language models.

    Returns
    -------
    tuple
        `tokens`: the token list as tuples
        `sentences`: the sentence list as tuples

        Both tokens and sentences are tuples (start, end, text).
    &#34;&#34;&#34;
    S = Spacy(lang=lang)
    S.read(text)
    return (S.getTokens(), S.getSentences())</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tf.tools.myspacy.Spacy"><code class="flex name class">
<span>class <span class="ident">Spacy</span></span>
<span>(</span><span>lang=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up an NLP (Natural Language Processing) pipeline.</p>
<p>The pipeline is tied to a particular language model, which you can pass
as a parameter, provided you have installed it.</p>
<p>For now, we use Spacy in a fairly trivial way: only tokenization and sentence
detection.
We do not need the parser for this.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lang</code></strong> :&ensp;<code>string</code>, optional <code>xx</code></dt>
<dd>
<p>The language to be used; Spacy may need to download it, if so, it will
happen automatically.</p>
<p>If the language is not supported by Spacy, we switch to the multilanguage
called <code>xx</code>.</p>
<p>See <code><a title="tf.tools.myspacy.LANG_MODELS" href="#tf.tools.myspacy.LANG_MODELS">LANG_MODELS</a></code> about the language models that Spacy supports.</p>
</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e4191ae3e7bbe8c426b07012cac4e759f5b02de6/tf/tools/myspacy.py#L57-L262" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Spacy:
    def __init__(self, lang=None):
        &#34;&#34;&#34;Sets up an NLP (Natural Language Processing) pipeline.

        The pipeline is tied to a particular language model, which you can pass
        as a parameter, provided you have installed it.

        For now, we use Spacy in a fairly trivial way: only tokenization and sentence
        detection.
        We do not need the parser for this.

        Parameters
        ----------
        lang: string, optional xx
            The language to be used; Spacy may need to download it, if so, it will
            happen automatically.

            If the language is not supported by Spacy, we switch to the multilanguage
            called `xx`.

            See `tf.tools.myspacy.LANG_MODELS` about the language models that Spacy supports.
        &#34;&#34;&#34;
        langModels = {}
        languages = {}

        for spec in LANG_MODELS:
            (lng, model, language) = spec.split(maxsplit=2)
            langModels[lng] = f&#34;{lng}_{model}_sm&#34;
            languages[lng] = language

        self.langModels = langModels
        self.languages = languages

        prevLang = None
        targetLang = lang
        loaded = False

        i = 0
        while True:
            i += 1
            targetModel = langModels.get(targetLang, None)
            targetLanguage = languages.get(targetLang, None)

            if targetModel is None:
                (prevLang, targetLang) = (targetLang, &#34;xx&#34;)
                targetModel = langModels[targetLang]
                targetLanguage = languages[targetLang]

                if prevLang is None:
                    console(&#34;No language specified&#34;)
                else:
                    console(
                        f&#34;No language model for {prevLang} supported by Spacy.\n&#34;
                    )
                console(
                    f&#34;Switching to the {targetLanguage} model&#34;
                )
                if targetLang == prevLang:
                    break
                else:
                    continue

            try:
                nlp = spacy.load(targetModel)
                loaded = True
                break

            except Exception:
                console(f&#34;Language model {targetModel} not installed. Downloading ...&#34;)

            try:
                console(f&#34;Downloading {targetModel} ...&#34;)
                download(targetModel)
            except Exception:
                console(f&#34;Could not download {targetModel} ...&#34;)
                (prevLang, targetLang) = (targetLang, &#34;xx&#34;)
                if targetLang == prevLang:
                    break
                else:
                    continue

        if loaded:
            try:
                nlp.disable_pipe(&#34;parser&#34;)
                nlp.disable_pipe(&#34;sentencizer&#34;)
            except Exception:
                pass
            try:
                nlp.enable_pipe(&#34;senter&#34;)
                self.canSentence = True
            except Exception:
                self.canSentence = False
                console(&#34;This language does not support sentence boundary detection&#34;)
        else:
            console(&#34;Cannot load (language data) to get Spacy working&#34;)
            nlp = None

        self.nlp = nlp
        self.doc = None

    def read(self, text):
        &#34;&#34;&#34;Process a plain text.

        A text is ingested and tokenized. Sentences are detected.
        This may require quite some processing time, think of 30 seconds for 200,000
        words on a decent laptop.

        Parameters
        ----------
        text: string
            The complete, raw text.
        &#34;&#34;&#34;
        nText = len(text)
        nlp = self.nlp

        if nlp is None:
            console(&#34;The NLP pipeline is not functioning&#34;)
            return

        nlp.max_length = nText
        doc = nlp(text)
        self.doc = doc

    def getTokens(self):
        &#34;&#34;&#34;Get the resulting tokens.

        A token is represented as a tuple consisting of

        *   *start*: first character position that the token occupies in the text.
            Character positions start at 0.
        *   *end*: last character position that the token occupies in the text
            *plus one*.
        *   *text*: text of the token, **excluding any trailing whitespace**.
        *   *space*: any white space behind the token, if present, otherwise
            the empty string.


        !!! note &#34;End position and space&#34;
            If there is a space behind the token, it will not add to the end position
            of the token. So the start and end positions of the tokens reflect
            where the tokens themselves are, and spaces do nto belong to the tokens.

        Returns
        -------
        list
            All tokens as tuples.
        &#34;&#34;&#34;
        doc = self.doc
        if self.doc is None:
            console(&#34;No results available from the NLP pipeline&#34;)
            return []

        result = []
        for token in doc:
            start = token.idx
            text = token.text
            space = token.whitespace_
            end = start + len(text)
            result.append((start, end, text, space))
        return result

    def getSentences(self):
        &#34;&#34;&#34;Get the resulting sentences.

        A sentence is represented as a tuple consisting of

        *   *start*: first character position that the sentence occupies in the text.
            Character positions start at 0.
        *   *end*: last character position that the sentence occupies in the text
            *plus one*.
        *   *text*: text of the sentence.

        Returns
        -------
        list
            All sentences as tuples.
        &#34;&#34;&#34;
        doc = self.doc
        if self.doc is None:
            console(&#34;No results available from the NLP pipeline&#34;)
            return []

        if not self.canSentence:
            console(&#34;No sentence results available from the NLP pipeline&#34;)
            return []

        result = []

        whiteRe = re.compile(r&#34;^[.?!\s]*$&#34;, re.S)
        spuriousNlBefore = re.compile(r&#34;\n+(\W)&#34;)
        spuriousNlAfter = re.compile(r&#34;(\W)\n+&#34;)

        for s in doc.sents:
            text = s.text.strip(&#34;\n&#34;)
            if whiteRe.match(text):
                continue

            tokenStart = doc[s.start]
            tokenEnd = doc[s.end - 1]
            sentStart = tokenStart.idx
            sentEnd = tokenEnd.idx + len(tokenEnd.text)

            text = spuriousNlBefore.sub(r&#34;\1&#34;, text)
            text = spuriousNlAfter.sub(r&#34;\1&#34;, text)
            result.append((sentStart, sentEnd, text))
        return result</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tf.tools.myspacy.Spacy.getSentences"><code class="name flex">
<span>def <span class="ident">getSentences</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the resulting sentences.</p>
<p>A sentence is represented as a tuple consisting of</p>
<ul>
<li><em>start</em>: first character position that the sentence occupies in the text.
Character positions start at 0.</li>
<li><em>end</em>: last character position that the sentence occupies in the text
<em>plus one</em>.</li>
<li><em>text</em>: text of the sentence.</li>
</ul>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>All sentences as tuples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e4191ae3e7bbe8c426b07012cac4e759f5b02de6/tf/tools/myspacy.py#L218-L262" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def getSentences(self):
    &#34;&#34;&#34;Get the resulting sentences.

    A sentence is represented as a tuple consisting of

    *   *start*: first character position that the sentence occupies in the text.
        Character positions start at 0.
    *   *end*: last character position that the sentence occupies in the text
        *plus one*.
    *   *text*: text of the sentence.

    Returns
    -------
    list
        All sentences as tuples.
    &#34;&#34;&#34;
    doc = self.doc
    if self.doc is None:
        console(&#34;No results available from the NLP pipeline&#34;)
        return []

    if not self.canSentence:
        console(&#34;No sentence results available from the NLP pipeline&#34;)
        return []

    result = []

    whiteRe = re.compile(r&#34;^[.?!\s]*$&#34;, re.S)
    spuriousNlBefore = re.compile(r&#34;\n+(\W)&#34;)
    spuriousNlAfter = re.compile(r&#34;(\W)\n+&#34;)

    for s in doc.sents:
        text = s.text.strip(&#34;\n&#34;)
        if whiteRe.match(text):
            continue

        tokenStart = doc[s.start]
        tokenEnd = doc[s.end - 1]
        sentStart = tokenStart.idx
        sentEnd = tokenEnd.idx + len(tokenEnd.text)

        text = spuriousNlBefore.sub(r&#34;\1&#34;, text)
        text = spuriousNlAfter.sub(r&#34;\1&#34;, text)
        result.append((sentStart, sentEnd, text))
    return result</code></pre>
</details>
</dd>
<dt id="tf.tools.myspacy.Spacy.getTokens"><code class="name flex">
<span>def <span class="ident">getTokens</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the resulting tokens.</p>
<p>A token is represented as a tuple consisting of</p>
<ul>
<li><em>start</em>: first character position that the token occupies in the text.
Character positions start at 0.</li>
<li><em>end</em>: last character position that the token occupies in the text
<em>plus one</em>.</li>
<li><em>text</em>: text of the token, <strong>excluding any trailing whitespace</strong>.</li>
<li><em>space</em>: any white space behind the token, if present, otherwise
the empty string.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">End position and space</p>
<p>If there is a space behind the token, it will not add to the end position
of the token. So the start and end positions of the tokens reflect
where the tokens themselves are, and spaces do nto belong to the tokens.</p>
</div>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>All tokens as tuples.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e4191ae3e7bbe8c426b07012cac4e759f5b02de6/tf/tools/myspacy.py#L180-L216" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def getTokens(self):
    &#34;&#34;&#34;Get the resulting tokens.

    A token is represented as a tuple consisting of

    *   *start*: first character position that the token occupies in the text.
        Character positions start at 0.
    *   *end*: last character position that the token occupies in the text
        *plus one*.
    *   *text*: text of the token, **excluding any trailing whitespace**.
    *   *space*: any white space behind the token, if present, otherwise
        the empty string.


    !!! note &#34;End position and space&#34;
        If there is a space behind the token, it will not add to the end position
        of the token. So the start and end positions of the tokens reflect
        where the tokens themselves are, and spaces do nto belong to the tokens.

    Returns
    -------
    list
        All tokens as tuples.
    &#34;&#34;&#34;
    doc = self.doc
    if self.doc is None:
        console(&#34;No results available from the NLP pipeline&#34;)
        return []

    result = []
    for token in doc:
        start = token.idx
        text = token.text
        space = token.whitespace_
        end = start + len(text)
        result.append((start, end, text, space))
    return result</code></pre>
</details>
</dd>
<dt id="tf.tools.myspacy.Spacy.read"><code class="name flex">
<span>def <span class="ident">read</span></span>(<span>self, text)</span>
</code></dt>
<dd>
<div class="desc"><p>Process a plain text.</p>
<p>A text is ingested and tokenized. Sentences are detected.
This may require quite some processing time, think of 30 seconds for 200,000
words on a decent laptop.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>string</code></dt>
<dd>The complete, raw text.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/annotation/text-fabric/blob/e4191ae3e7bbe8c426b07012cac4e759f5b02de6/tf/tools/myspacy.py#L157-L178" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def read(self, text):
    &#34;&#34;&#34;Process a plain text.

    A text is ingested and tokenized. Sentences are detected.
    This may require quite some processing time, think of 30 seconds for 200,000
    words on a decent laptop.

    Parameters
    ----------
    text: string
        The complete, raw text.
    &#34;&#34;&#34;
    nText = len(text)
    nlp = self.nlp

    if nlp is None:
        console(&#34;The NLP pipeline is not functioning&#34;)
        return

    nlp.max_length = nText
    doc = nlp(text)
    self.doc = doc</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<p><a href="https://github.com/annotation" title="annotation on GitHub"><img src="../../tf/images/tf-small.png" alt="annotation"></a></p>
<p><a href="../../tf/index.html">tf home</a> -
<a href="../../tf/cheatsheet.html">cheat sheet</a> -
<a href="https://github.com/annotation/text-fabric" title="GitHub repo"><img src="../../tf/images/GitHub_Logo.png" alt="GitHub" width="50"></a></p>
</p>
<form>
<input id="lunr-search" name="q" placeholder="üîé Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tf.tools" href="index.html">tf.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="tf.tools.myspacy.LANG_MODELS" href="#tf.tools.myspacy.LANG_MODELS">LANG_MODELS</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tf.tools.myspacy.tokensAndSentences" href="#tf.tools.myspacy.tokensAndSentences">tokensAndSentences</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tf.tools.myspacy.Spacy" href="#tf.tools.myspacy.Spacy">Spacy</a></code></h4>
<ul class="">
<li><code><a title="tf.tools.myspacy.Spacy.getSentences" href="#tf.tools.myspacy.Spacy.getSentences">getSentences</a></code></li>
<li><code><a title="tf.tools.myspacy.Spacy.getTokens" href="#tf.tools.myspacy.Spacy.getTokens">getTokens</a></code></li>
<li><code><a title="tf.tools.myspacy.Spacy.read" href="#tf.tools.myspacy.Spacy.read">read</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<a href="https://pure.knaw.nl/portal/en/persons/dirk-roorda">Dirk Roorda</a>
<a href="https://huc.knaw.nl"><img alt="HuC" src="../../tf/images/huc.png" width="200" alt="Humanities Cluster"></a>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>